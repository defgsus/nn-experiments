experiment_name: textmask/convtext-QAprog-lpo2b_${matrix_slug}

matrix:
  bs: [64]
  opt: ["AdamW"]
  lr: [.0005]
  nitem:
    - 26
  len:
    - 5
  nops: [5, 6, 7, 3, 4]
  l:
    #- 4
    - 6
    #- 8
    #- 10
    #- 12
    #- 18
  d_model:
    - 32
  d_state:
    - 16
  exp:
    - 2


trainer: experiments.textmask.texttrainer.TextMaskTrainer
mask_is_arg: 1

train_set: |
  train_set, validation_set = TextQAProgramIterableDataset.create_train_and_validation_set(
    train_count=100_000,
    validation_count=10_000,
    validation_seed=23,
    with_masked=True,
    num_items=${nitem},
    input_length=${len},
    num_operations=${nops},
    operators={"+": 1., "-": 2/3},
  )
  train_set

validation_set: |
  validation_set

batch_size: ${bs}
learnrate: ${lr}
optimizer: ${opt}
scheduler: CosineAnnealingWarmupLR
loss_function: l1
max_inputs: 4_000_000
num_epochs_between_validations: 1
#num_inputs_between_validations: 50_000
freeze_validation_set: True


model: |
  from src.models.mamba.mamba import Mamba, ModelArgs
  
  Mamba(ModelArgs(
      d_model=${d_model},
      n_layer=${l},
      d_state=${d_state},
      expand=${exp},
      vocab_size=256,
  ))
