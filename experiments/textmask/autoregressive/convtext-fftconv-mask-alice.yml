experiment_name: textmask/convtext-fftconvr-mask_${matrix_slug}

matrix:
  trial: [1] #, 2, 3, 4, 5]
  bs:
    - 64
    #- 256
  opt: ["AdamW"]
  lr:
    - .0005
    #- .002
  ds:
    - "alice"
    #- "kama"
    #- "zoo"
  size:
    #- 40
    #- [15, 500]
    - [30, 200]
  mask:
    - 1
  maskt:
    #- "block"
    - "end"
    #- "single"
  l:
    - 3
    #- 4
    #- 6
  ch:
#    - 32
    - 64
#    - 128
#    - 256
#    - 512
  ks:
    - 13
    #- 11
    #- 13
  dil:
    - [3, 5, 1]
    - [3, 5, 7, 1]
    - [1, 2, 3, 4, 5, 1]
  $filter: |
    (isinstance(${dil}, int) or len(${dil}) == ${l}) 
    and (isinstance(${attn}, int) or len(${attn}) == ${l})
  fftc:
    - [True, 0, 0]
    - [0, True, 0]
    - [0, 0, True]
    - [0, 0, 0]
  qkv:
    - "QK"
    #- "QV"
    #- "KV"
    #- "QKV"
  attn:
    - [0, 0, True]
    - [0, 0, 0, True]
    - [0, 0, 0, 0, 0, True]
  attnact:
    - "elu+1"
    # - "dpfp"
  act:
    - "gelu"
  norm:
    #- "None"
    - "'bn1d'"
  cheap:
    - False
#    - True

trainer: experiments.textmask.texttrainer.TextMaskTrainer
# mask_is_arg: 1
mask_size: ${mask}
mask_type: ${maskt}

train_set: |
  FILE = Path({
    "alice": "${PATH}/datasets/text/en/alice-in-wonderland.txt",
    "kama": "${PATH}/datasets/text/en/kama-sutra.txt",
    "zoo": "${PATH}/datasets/text/en/zoology.txt",
  }["${ds}"]).read_text()
  
  TextSegmentIterableDataset(
    FILE,
    size=${size}, stride=1,
    batch_size=${bs},
  ).shuffle(100_000)

validation_set: |
  TextSegmentIterableDataset(
    FILE,
    size=${size}, stride=5,
    batch_size=${bs}, seed=42,
  ).shuffle(100_000, seed=23).limit(5000).freeze()

batch_size: ${bs}
learnrate: ${lr}
optimizer: ${opt}
scheduler: CosineAnnealingWarmupLR
#loss_function: l1
max_inputs: 1_000_000
#num_epochs_between_validations: 999999
num_inputs_between_validations: 100_000
#freeze_validation_set: True


model: |
  from experiments.textmask.textmodel import ConvTextModel
  
  ConvTextModel(
      vocab_size=256,
      num_layers=${l},
      num_channels=${ch},
      kernel_size=${ks},
      dilation=${dil},
      activation="${act}",
      norm=${norm},
      #pos_embedding=bool({posemb}),
      num_attention_heads=${attn},
      attention_invention="${qkv}",
      attention_activation="${attnact}",
      diagonal_embedding=True,
      symmetric_embedding=True,
      fft_conv=${fftc},
      cheap=${cheap},
  )
