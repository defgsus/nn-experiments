experiment_name: ae/script/unsplash_detail-unfixed_${matrix_slug}

matrix:
  script:
    #- "ch=32|down4|ch/2|ch/2|ch/2|ch*1|ch*1|ch=1"  # baseline 1:48, 3.2M params, l1-val-loss 0.046, 9.9min
    - "ch=32|down4|ch/2|ch/2|ch/2|ch*1|ch*1|ch*1|ch*1|ch*1|ch*1|ch=1"  # 3.5M params,
    #- "scl=3|ch=32|down4|ch/2|ch/2|ch/2|ch*1|ch*1|ch=1" # 15.4M,

# below comments are from 28Â² MNIST set
#    - "ch=32|down4|ch/2|ch/2|po=3|ch/2|ch*1|ch*1|ch=1"  # 3.8M, 0.054, 6.4min
#    - "ch=32|down4|ch/2|ch/2|po=4|ch/2|ch*1|ch*1|ch=1"  # 4.1M,  BAAD
#    - "ch=32|down4|po=3|ch/2|ch/2|ch/2|ch*1|ch*1|ch=1"  # BAAD
#    - "ch=32|down4|ch/2|ch*1|ch/2|ch*1|ch/2|ch*1|ch*1|ch=1"  # 4.7M params, 0.052
#    - "ch=32|down4|ch/2|ch*1|ch/2|ch*1|ch/2|ch*1|ch/2|ch*1|ch/2|ch*1|ch=1"  # 4.7M, 0.055
#    - "ch=32|down4|ch*1|ch/2|ch*1|ch*1|ch/2|ch*1|ch*1|ch/2|ch*1|ch*1|ch=1"  # 10.9M, 0.05, SLOW
#    - "ch=48|down4|ch*1|ch/2|ch*1|ch*1|ch/2|ch*1|ch*1|ch/2|ch*1|ch*1|ch=1"  # 24.5M, 0.046, 24min
#    - "scl=3|ch=32|down4|ch/2|ch/2|ch/2|ch*1|ch*1|ch=1" # 15.4M, 0.046, 4min faster than above
#    - "ch=32|down4|scl=3|ch/2|ch/2|ch/2|scl=0|ch*1|ch*1|ch=1"  # 14.8M, 0.049, SLOWWW
#    - "ch=32|down2|ch*1|ch*1|down2|ch/2|ch/2|ch/2|ch*1|ch*1|ch=1"  # 1.9M, 0.052
#    - "ch=32|down4|ch/2|ch*1|ch*1|ch*1|ch*1|ch/2|ch/2|ch/2|ch*1|ch=1"  # 7.8M, 0.052
#    - "ch=32|down4|ch/2|ch*1|ch*1|ch*1|ch*1|ch/2|ch*1|ch*1|ch*1|ch*1|ch=1"  # 8.8M, 0.05, 10min
#    - "ch=64|down4|ch/2|ch/2|ch/2|ch*1|ch*1|ch=1"  # 12.9M, 0.052, 14min
#    - "scl=2|ch=32|down2|ch*1|ch*1|down2|ch/2|ch/2|ch/2|ch*1|ch*1|ch=1"  # 13M, 0.047, 24min
  detail:
    - ""
    #- "ch=64|ch*1|ch*1|ch*1|ch=3"
  ks:
    - 3
  pad:
    - 1
  act:
    - "relu6"
  percept_ch:
    - 0
    #- 128

trainer: experiments.ae.trainer.TrainAutoencoderSpecial
feature_loss_weight: 0.

perceptual_model: |
  if ${percept_ch} == 0:
      model = None
  else:
      model = nn.Sequential(
          nn.Conv2d(SHAPE[0], ${percept_ch}, kernel_size=3, padding=1),
      )
  model

train_set: |
  unsplash_dataset(shape=SHAPE, train=True)

validation_set: |
  unsplash_dataset(shape=SHAPE, train=False)

batch_size: 64
learnrate: 0.0003
optimizer: AdamW
scheduler: CosineAnnealingWarmupLR
loss_function: l1
max_inputs: 1_000_000

globals:
  SHAPE: (3, 32, 32)

model: |
  from experiments.ae.scriptae.scriptae2 import ScriptedAE
  
  model = ScriptedAE(
      channels=SHAPE[0],
      kernel_size=${ks},
      padding=${pad},
      activation="${act}",
      script="${script}",
  )
  
  if "${detail}":
      fn = "${PATH}/checkpoints/ae/script/unsplash_detail_script:ch32down4ch2ch2ch2ch1ch1ch1_ks:3_pad:1_act:relu6_percept_ch:0/best.pt"
      data = torch.load(fn)
      model.load_state_dict(data["state_dict"])
  
  class DetailModel(nn.Module):
      def __init__(self, model, detail_model):
          super().__init__()
          #for p in model.parameters():
          #    p.requires_grad = False
          self.encoder = model.encoder
          self.decoder = model.decoder
          self.detail = detail_model
      
      def decode(self, y: torch.Tensor):
          coarse = self.decoder(y)
          latent = F.pixel_shuffle(y.repeat(1, 4*4, 1, 1), 4)
          mix = torch.concat([coarse, latent], dim=1)
          # print("MIX", mix.shape)
          fine = self.detail(mix)
          # print("X", coarse.shape, fine.shape)
          return coarse + fine
  
      def forward(self, x: torch.Tensor):
          y = self.encoder(x)
          y = self.decoder(y)
          return y
  
  if "${detail}":
    model = DetailModel(model, ScriptedAE(
            channels=SHAPE[0] + 1,
            kernel_size=${ks},
            padding=${pad},
            activation="${act}",
            script="${detail}",
        ).encoder
    )
  
  with torch.no_grad():
      print(model)
      inp = torch.ones(1, *SHAPE)
      outp = dump_module_stacktrace(model.encoder, inp)
      print(inp.shape, "->", outp.shape)
      print("RATIO:", math.prod(inp.shape) / math.prod(outp.shape))
      print(f"params: {num_module_parameters(model, trainable=True):,} / {num_module_parameters(model, trainable=False):,}")
      if hasattr(model, "decode"):
          recon = dump_module_stacktrace(model, outp, method_name="decode")
      else:
          recon = dump_module_stacktrace(model.decoder, outp)
  
  model
