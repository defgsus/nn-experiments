matrix:
  bins: [50, 100]

experiment_name: ae/histlayer1

trainer: experiments.ae.trainer.TrainAutoencoderSpecial

train_set: |
  rpg_tile_dataset_3x32x32(SHAPE, validation=False)

validation_set: |
  rpg_tile_dataset_3x32x32(SHAPE, validation=True)

batch_size: 64
learnrate: 0.0003
optimizer: AdamW
scheduler: CosineAnnealingLR
loss_function: l1
max_inputs: 1_000_000

globals:
  SHAPE: (1, 32, 32)
  CODE_SIZE: 128

model: |
  class Encoder(nn.Module):
      def __init__(self):
          super().__init__()
          self.conv = Conv2dBlock(channels=(SHAPE[0], 24, 32, 48), kernel_size=5)
          self.encoded_shape = self.conv.get_output_shape(SHAPE)
          self.hist = HistogramLayer(${bins}, 0, 1, 50)
          self.linear = nn.Linear(math.prod(self.encoded_shape) + ${bins}, CODE_SIZE)
      
      def forward(self, x):
          conv = self.conv(x).flatten(1)
          hist = self.hist(x.flatten(1))
          x = torch.concat([conv, hist], dim=-1)
          return self.linear(x)
  
  encoder = Encoder()
  
  decoder = nn.Sequential(
      nn.Linear(CODE_SIZE, math.prod(encoder.encoded_shape)),
      Reshape(encoder.encoded_shape),
      encoder.conv.create_transposed(act_last_layer=False),
  )

  EncoderDecoder(encoder, decoder)
