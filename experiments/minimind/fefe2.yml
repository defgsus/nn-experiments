experiment_name: minimind/fefe2_${matrix_slug}

matrix:
  trial: [1] #, 2, 3, 4, 5]
  bs:
    - 8
    #- 256
  ga:
    - 1
  opt: ["AdamW"]
  lr:
    - .0005
    #- .002
  len:
    #- 128
    #- [64, 512]
    - [512, 1024]
    #- 8192
  dsm:
    - "concat"
  vocab:
    #- 1024
    - 4096
  l:
    #- 4
    - 6
  dim:
    - 128
    #- 256
    #- 512
  h:
    - 8
  kvh:
    - 2
  diag:
    #- False
    - True
  qkvlayer:
    - "linear"
    #- "cnn"
  fflayer:
    - "linear"
    #- "cnn"
  act:
    - "relu"

globals:
  TOKENIZER: |
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    transformers.AutoTokenizer.from_pretrained(
        str(config.SMALL_DATASETS_PATH / "fefe" / "tokenizer-bpe-${vocab}-spaces")
    )

trainer: experiments.minimind.minimindtrainer.MiniMindTrainer
tokenizer: |
  TOKENIZER
max_seq_length: ${len}

train_set: |
  from experiments.minimind.tokenizedataset import TokenizeDataset
  dataset = FefePostIterableDataset().freeze() 
  TokenizeDataset(
      dataset.skip(1000),
      TOKENIZER,
      min_seq_length=${len}[0],
      max_seq_length=${len}[1],
      batch_size=${bs},
      method="${dsm}",
  )

validation_set: |
  TokenizeDataset(
      dataset.limit(1000),
      TOKENIZER,
      #min_seq_length=${len}[0],
      max_seq_length=${len}[1],
      #batch_size=${bs},
      method="${dsm}",
  )#.freeze()
  

batch_size: ${bs}
gradient_accumulation: ${ga}
learnrate: ${lr}
optimizer: ${opt}
scheduler: CosineAnnealingWarmupLR
#loss_function: l1
max_inputs: 20_000_000
num_epochs_between_validations: 999999
num_inputs_between_validations: 50_000
#freeze_validation_set: True


model: |
  from src.models.minimind import LMConfig, MiniMindLM
  
  model = MiniMindLM(
      LMConfig(
          dim=${dim},
          n_layers=${l},
          n_heads=${h},
          n_kv_heads=${kvh},
          vocab_size=${vocab},
          hidden_dim=None,
          multiple_of=64,
          max_seq_len=${len}[1],
          diagonal_embedding=${diag},
          qkv_layer_type="${qkvlayer}",
          ff_layer_type="${fflayer}",
          activation="${act}",
      )
  )
  with torch.no_grad():
      print(model)
      inp = torch.ones(${bs}, ${len}[0], dtype=torch.long)
      outp = dump_module_stacktrace(model, inp)
      print(f"{inp.shape} -> {outp.logits.shape}")
  
  model

