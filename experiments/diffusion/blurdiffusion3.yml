experiment_name: diffusion/blurdiff3-21x7-tgtall-class_${matrix_slug}

matrix:
  ds: ["mnist"]
  nexp: [1.]
  opt: ["AdamW"]
  lr: [0.004]
  l:
    - 11
  ch:
    - 32
    #- [32, 32, 32, 32, 32, 32, 32, 32]
  ks:
    - 3
    #- [ 3,  3,  3,  3,  3,  3,  3,  3]
    #- [3, 3, 3, 5, 7]
  pad:
    - 1
    #- [ 1,  1,  1,  1,  1,  1,  1,  1]
    #- [1, 1, 1, 2, 3]
  stride:
    - 1
    #- [ 1,  1,  1,  2,  1,  2,  1,  1]
  emb:
    - 12
  act:
    - gelu

trainer: experiments.diffusion.trainer.TrainDiffusion
diffusion_sampler: |
  from experiments.diffusion.sampler import DiffusionSamplerBlur 
  DiffusionSamplerBlur(21, 7.)

with_target_noise: True
target_noise_is_smaller: False

globals:
  SHAPE: (3, 32, 32)
  CLASSES: 10

train_set: |
  ClassLogitsDataset(
    ${ds}_dataset(train=True, shape=SHAPE, interpolation=False) 
      .transform([lambda x: (x * 255).to(torch.uint8), VT.TrivialAugmentWide(), lambda x: x.to(torch.float32) / 255.]),
    num_classes=CLASSES, tuple_position=1, label_to_index=True,
  )

validation_set: |
  ClassLogitsDataset(
    ${ds}_dataset(train=False, shape=SHAPE, interpolation=False),
    num_classes=CLASSES, tuple_position=1, label_to_index=True,
  )

batch_size: 64

learnrate: ${lr}
optimizer: ${opt}
scheduler: CosineAnnealingWarmupLR
loss_function: l1
max_inputs: 1_200_000
num_epochs_between_validations: 1
# num_inputs_between_validations: 120_000
generator_shape: |
  SHAPE
num_class_logits: |
  CLASSES
noise_amount_exponent: ${nexp}

model: |
  from experiments.diffusion.trainer import DiffusionModelOutput
  from experiments.diffusion.unet import UNet
  
  class Module(nn.Module):
      def __init__(self):
          super().__init__()
          self.module = UNet(
              in_channels=SHAPE[0] + 2 + CLASSES,
              out_channels=SHAPE[0],
              num_layers=${l},
              channels=${ch},
              stride=${stride},
              kernel_size=${ks},
              padding=${pad},
              activation="${act}",
              activation_last_layer=None,
              layer_embedding=${emb},
          )
  
      def forward(self, input):
          embedding = input.parameter_embedding()
          x = torch.cat([input.images, embedding], dim=-3)
          return DiffusionModelOutput(noise=self.module(x, embedding)) 
  
  Module()
