experiment_name: denoise/deblur5x10-resconv-${matrix_slug}

matrix:
  bs: [64]
  opt: ["AdamW"]
  lr: [.0003]
  l:
    - 2
  ks:
    - [3, 5]
  pad:
    - [1, 3]
  ch:
    - 32
  stride:
    - 1
  act:
    - gelu

trainer: experiments.denoise.trainer.TrainDenoising

train_set: |
  {
      #"mnist": mnist_dataset(train=True, shape=SHAPE, interpolation=False),
      #"fmnist": fmnist_dataset(train=True, shape=SHAPE, interpolation=False),
      #"rpg": rpg_tile_dataset_3x32x32(validation=False, shape=SHAPE, interpolation=False), 
      #"kali": kali_patch_dataset(shape=SHAPE),
      "all": all_image_patch_dataset(shape=SHAPE),
  }["all"]

validation_set: |
  {
      #"mnist": mnist_dataset(train=False, shape=SHAPE, interpolation=False),
      #"fmnist": fmnist_dataset(train=False, shape=SHAPE, interpolation=False),
      #"rpg": rpg_tile_dataset_3x32x32(validation=True, shape=SHAPE, interpolation=False),
      #"kali": kali_patch_dataset(shape=SHAPE).limit(5000),
      "all": all_image_patch_dataset(shape=SHAPE).limit(5000),
  }["all"]

batch_size: ${bs}
learnrate: ${lr}
optimizer: ${opt}
scheduler: CosineAnnealingWarmupLR
loss_function: l1
max_inputs: 10_000_000
num_inputs_between_validations: 100_000
freeze_validation_set: True
train_input_transforms: |
  [
    #ImageNoise(amt_min=0.01, amt_max=0.15),  # mid
    #ImageNoise(amt_min=.15, amt_max=.3),  # strong
    #ImageNoise(amt_min=.01, amt_max=.6),  # heavy
    
    #RandomBlur(amt_min=.1, amt_max=10.),
    VT.GaussianBlur(kernel_size=[5, 5], sigma=[10., 10.])
  
    # RandomCropHalfImage(),
  ]
  

globals:
  SHAPE: (3, 48, 48)

model: |
  from experiments.denoise.resconv import ResConv
  
  class Module(nn.Module):
      def __init__(self):
          super().__init__()
          self.module = ResConv(
              in_channels=SHAPE[0],
              num_layers=${l},
              channels=${ch},
              stride=${stride},
              kernel_size=${ks},
              padding=${pad},
              activation="${act}",
              activation_last_layer=None,
          )
  
      def forward(self, x):
          return (x - self.module(x)).clamp(0, 1) 
  
  Module()
