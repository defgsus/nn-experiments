matrix:
  patch: [4]
  layer: [4] #8 , 16]
  head: [8] #4, 8, 12, 16]
  hidden: [256] #, 768]
  mlp: [512] #, 1024]
  drop: [0.]
  cs: [128] #, 784]
  $filter: ${hidden} / ${head} == ${hidden} // ${head}

experiment_name: aug/fmnist_vit_trivaug_${matrix_slug}

train_set: |
  ClassLogitsDataset(
      fmnist_dataset(train=True, shape=SHAPE),
      num_classes=CLASSES, tuple_position=1, label_to_index=True,
  )

# freeze_validation_set: True
validation_set: |
  ClassLogitsDataset(
      fmnist_dataset(train=False, shape=SHAPE),
      num_classes=CLASSES, tuple_position=1, label_to_index=True,
  )

trainer: experiments.reptrainer.RepresentationClassTrainer
batch_size: 64
learnrate: 0.0003
optimizer: AdamW
scheduler: CosineAnnealingLR
loss_function: l2
max_inputs: 2_000_000
train_input_transforms: |
  [
      lambda x: (x * 255).to(torch.uint8),
      #VT.AutoAugment(),
      VT.TrivialAugmentWide(),
      lambda x: x.to(torch.float32) / 255.,
  ]

globals:
  SHAPE: (3, 28, 28)
  CODE_SIZE: ${cs}
  CLASSES: 10

model: |
  class Encoder(nn.Module):
      def __init__(self):
          super().__init__()
    
          from torchvision.models import VisionTransformer
          self.encoder = VisionTransformer(
              image_size=SHAPE[-1],
              patch_size=${patch},
              num_layers=${layer},
              num_heads=${head},
              hidden_dim=${hidden},
              mlp_dim=${mlp},
              num_classes=CODE_SIZE,
              dropout=${drop},
          )
          self.linear = nn.Linear(CODE_SIZE, CLASSES)
      
      def forward(self, x):
          return self.linear(self.encoder(x))
    
  Encoder()
