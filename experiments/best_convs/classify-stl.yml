experiment_name: best-convs/classify-stl-${matrix_slug}

matrix:
  trial: [1]
  bs:
    - 64
  lr:
    - 0.0003
  opt:
    - AdamW
  aug:
    - "'wide'"
#    - "'cutoutp'"
  elastic:
    - None
#    - [100., 10.]
  config:
#    - {ch: [32, 32, 32], ks: [5, 3, 5], st: [3, 2, 1], di: [2, 1, 1]}   # highest throughput
#    - {ch: [32, 32, 32], ks: [7, 3, 3], st: [3, 1, 2], di: [3, 1, 1]}
#    - {ch: [32, 32, 32], ks: [7, 3, 3], st: [3, 1, 1], di: [3, 3, 1]}
#    - {ch: [32, 32, 32], ks: [5, 5, 3], st: [3, 1, 1], di: [3, 1, 3]}
#    - {ch: [32, 32, 32], ks: [9, 3, 5], st: [3, 1, 1], di: [1, 1, 1]}   # best train speed #1
#    - {ch: [32, 32, 32], ks: [9, 3, 7], st: [3, 1, 1], di: [2, 1, 1]}   # best train speed #2
#    - {ch: [32, 32, 32], ks: [9, 3, 7], st: [3, 1, 1], di: [1, 1, 1]}   # best val loss
#    - {ch: [32, 32, 32], ks: [7, 3, 9], st: [2, 1, 2], di: [3, 1, 1]}
#    - {ch: [32, 32, 32], ks: [7, 3, 5], st: [2, 1, 2], di: [1, 3, 1]}   # best train speed #3
#    - {ch: [32, 32, 32], ks: [9, 9, 7], st: [2, 2, 1], di: [2, 1, 1]}   # worst val loss with random weights
    # all again with 128 channels
    - {ch: [128, 128, 128], ks: [5, 3, 5], st: [3, 2, 1], di: [2, 1, 1]}   # highest throughput
    - {ch: [128, 128, 128], ks: [7, 3, 3], st: [3, 1, 2], di: [3, 1, 1]}
    - {ch: [128, 128, 128], ks: [7, 3, 3], st: [3, 1, 1], di: [3, 3, 1]}
    - {ch: [128, 128, 128], ks: [5, 5, 3], st: [3, 1, 1], di: [3, 1, 3]}
    - {ch: [128, 128, 128], ks: [9, 3, 5], st: [3, 1, 1], di: [1, 1, 1]}   # best train speed #1
    - {ch: [128, 128, 128], ks: [9, 3, 7], st: [3, 1, 1], di: [2, 1, 1]}   # best train speed #2
    - {ch: [128, 128, 128], ks: [9, 3, 7], st: [3, 1, 1], di: [1, 1, 1]}   # best val loss
    - {ch: [128, 128, 128], ks: [7, 3, 9], st: [2, 1, 2], di: [3, 1, 1]}
    - {ch: [128, 128, 128], ks: [7, 3, 5], st: [2, 1, 2], di: [1, 3, 1]}   # best train speed #3
    - {ch: [128, 128, 128], ks: [9, 9, 7], st: [2, 2, 1], di: [2, 1, 1]}   # worst val loss with random weights

    #- {ch: [32, 32, 32, 32, 32, 32], ks: [9, 3, 5, 9, 3, 5], st: [3, 1, 1, 3, 1, 1], di: [1, 1, 1, 1, 1, 1]}


trainer: experiments.classify.classifier_trainer.ClassifierTrainer
num_classes: |
  CLASSES

train_set: |
  ds = stl10_dataset(shape=SHAPE, train=True)
  if ${aug} == "wide":
      ds = TransformIterableDataset(ds, transforms=[
          lambda x: (x * 255.).to(torch.uint8),
          VT.TrivialAugmentWide(),
          lambda x: x.float() / 255.,
      ])
  elif ${aug} == "cutoutp":
      ds = TransformIterableDataset(ds, transforms=[
          VT.Compose((
              VT.Pad(12),
              VT.RandomCrop((96, 96)),
              VT.RandomHorizontalFlip(),
              VT.RandomErasing(),
          ))
      ])
  if ${elastic}:
      ds = TransformIterableDataset(
          ds,
          transforms=[VT.ElasticTransform(alpha=${elastic}[0], sigma=${elastic}[1])],
      )
  ds

validation_set: |
  stl10_dataset(shape=SHAPE, train=False)

batch_size: ${bs}
learnrate: ${lr}
optimizer: ${opt}
scheduler: CosineAnnealingWarmupLR
max_inputs: 500_000
#num_workers: 4

globals:
  SHAPE: (3, 96, 96)
  CLASSES: 10

model: |
    class ClassificationModel(nn.Module):
        def __init__(self):
            from scripts.test_random_conv import ConvModel
            super().__init__()
            self.conv = ConvModel(
                channels=${config}["ch"],
                kernel_size=${config}["ks"],
                stride=${config}["st"],
                dilation=${config}["di"],
                activation=["relu"] * len(${config}["ch"]),
            )
            with torch.no_grad():
                in_shape = torch.Size((1, *SHAPE))
                out_shape = self.conv(torch.ones(in_shape)).shape
                flat_size = math.prod(out_shape)
                print(f"{in_shape} ({math.prod(in_shape):,}) -> {out_shape} ({math.prod(out_shape):,}) = {math.prod(out_shape) / math.prod(in_shape)}") 
            self.head = nn.Linear(flat_size, CLASSES)
    
        def forward(self, x):
            x = self.conv(x).flatten(1)
            x = self.head(x)
            return x
    
    model = ClassificationModel()
    # print(model)
    model
    
