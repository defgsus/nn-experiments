trainer: src.train.TrainAutoencoder

matrix:
  ds:
    #- "mnist"
    #- "fmnist"
    - "rpg"
  aa:
    #- "True"
    - "False"
  res:
    #- 20
    #- 28
    - 32
  #lr:
   # - 0.0003
   # - 0.000001
  encspd:
    - True
    #- False
  decspd:
    - True
    #- False

experiment_name: vae-test2/vae3_${matrix_slug}

train_set: | 
  {
      "mnist": mnist_dataset(train=True, shape=SHAPE, interpolation=${aa}),
      "fmnist": fmnist_dataset(train=True, shape=SHAPE, interpolation=${aa}),
      "rpg": rpg_tile_dataset_3x32x32(validation=False, shape=SHAPE, interpolation=${aa}) 
  }["${ds}"]

validation_set: |
  {
      "mnist": mnist_dataset(train=False, shape=SHAPE, interpolation=${aa}),
      "fmnist": fmnist_dataset(train=False, shape=SHAPE, interpolation=${aa}),
      "rpg": rpg_tile_dataset_3x32x32(validation=True, shape=SHAPE, interpolation=${aa})
  }["${ds}"]

batch_size: 64
learnrate: 0.001
optimizer: Adam
scheduler: CosineAnnealingLR
max_inputs: 10_000_000

globals:
  SHAPE: (1, ${res}, ${res})
  CODE_SIZE: 128

model: |
  encoder = EncoderConv2d(SHAPE, code_size=CODE_SIZE, channels=(16, 24, 32), kernel_size=[3, 4, 3], space_to_depth=${encspd})
  decoder = DecoderConv2d(SHAPE, code_size=CODE_SIZE, channels=(48, 32, 24), kernel_size=[3, 4, 3], space_to_depth=${decspd})
  
  VariationalAutoencoder(
      encoder = VariationalEncoder(
          encoder, CODE_SIZE, CODE_SIZE
      ),
      decoder = decoder,
      reconstruction_loss = "l1",
      reconstruction_loss_weight = 1.,
      kl_loss_weight = 1.,
  )
