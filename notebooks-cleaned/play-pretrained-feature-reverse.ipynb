{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f656602-a8e0-4688-9d76-f7eeb962f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48243f5a-a180-4e8c-8f99-e87a95a81c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0f161d-c829-49bc-be73-3bdfc938fab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1, progress=True)\n",
    "\n",
    "VF.to_pil_image(get_model_weight_images(model, normalize=\"each\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6aa1de-1b5d-44aa-bb82-010491741c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "\n",
    "def optimize_image(\n",
    "    image: torch.Tensor,\n",
    "    model: nn.Module,\n",
    "    target: torch.Tensor,\n",
    "    steps: int = 1000,\n",
    "    batch_size: int = 4,\n",
    "    device: str = \"auto\",\n",
    "    rand_degrees: float = 5.,\n",
    "    rand_translate: float = 0.1,\n",
    "):\n",
    "    output_widget = ipywidgets.Output()\n",
    "    display(output_widget)\n",
    "    \n",
    "    device = to_torch_device(device)\n",
    "    image = nn.Parameter(image.clone().to(device))\n",
    "    trans = VT.Compose([\n",
    "        #VT.Resize((96, 96), interpolation=VT.InterpolationMode.BILINEAR),\n",
    "        VT.RandomAffine(degrees=rand_degrees, translate=(rand_translate, rand_translate)),\n",
    "    ])\n",
    "    \n",
    "    target = target.unsqueeze(0).repeat(batch_size, 1).to(device)\n",
    "    model.to(device)\n",
    "    optim = torch.optim.Adam([image], lr=0.001)\n",
    "    loss_func = nn.L1Loss()\n",
    "    \n",
    "    progress = tqdm(range(steps))\n",
    "    try:\n",
    "        for step_idx in progress:\n",
    "            batch = torch.concat([\n",
    "                trans(image).unsqueeze(0)\n",
    "                for _ in range(batch_size)\n",
    "            ])\n",
    "            output = model(batch)\n",
    "            output = F.softmax(output, dim=1)\n",
    "            if output.shape != target.shape:\n",
    "                print(f\"output={output.shape}, target={target.shape}\")\n",
    "                return\n",
    "    \n",
    "            loss = loss_func(output, target)\n",
    "            #mean_loss = (.0 - batch.mean()).abs() \n",
    "            progress.set_postfix({\"loss\": loss.item()})#, \"mean_loss\": mean_loss.item(), \"mean\": batch.mean()})\n",
    "            #loss = loss + mean_loss\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            model.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                image[:] = image.clamp(0, 1)\n",
    "    \n",
    "            if step_idx % 50 == 0:\n",
    "                with output_widget:\n",
    "                    output_widget.clear_output()\n",
    "                    display(VF.to_pil_image(resize(image, 2)))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    return image[:].cpu()\n",
    "        \n",
    "img = optimize_image(\n",
    "    torch.rand(3, 128, 128) * .0 + .3, model, torch.Tensor([1, *((0, ) * 999)]),\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db0aa6b-874d-476b-99e7-4f38b0fc9812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "\n",
    "def optimize_image_2(\n",
    "    image: torch.Tensor,\n",
    "    model: nn.Module,\n",
    "    steps: int = 1000,\n",
    "    batch_size: int = 4,\n",
    "    device: str = \"auto\",\n",
    "    rand_degrees: float = 5.,\n",
    "    rand_translate: float = 0.01,\n",
    "):\n",
    "    output_widget = ipywidgets.Output()\n",
    "    output_widget.clear_output()\n",
    "    display(output_widget)\n",
    "    \n",
    "    device = to_torch_device(device)\n",
    "    image = nn.Parameter(image.clone().to(device))\n",
    "    trans = VT.Compose([\n",
    "        #VT.Resize((96, 96), interpolation=VT.InterpolationMode.BILINEAR),\n",
    "        VT.RandomAffine(degrees=rand_degrees, translate=(rand_translate, rand_translate)),\n",
    "        VT.RandomErasing(),\n",
    "    ])\n",
    "    \n",
    "    #target = target.unsqueeze(0).repeat(batch_size, 1).to(device)\n",
    "    model.to(device)\n",
    "    optim = torch.optim.Adam([image], lr=0.001)\n",
    "    loss_func = nn.L1Loss()\n",
    "\n",
    "    layer = find_module_layer(model, \"layer3.3.conv1\")\n",
    "\n",
    "    layer_snapshot = None\n",
    "    def _hook(model, args, kwargs=None):\n",
    "        nonlocal layer_snapshot\n",
    "        layer_snapshot = args[0]\n",
    "        #print(model, len(args), args[0].shape)\n",
    "    hook = layer.register_forward_hook(_hook)\n",
    "    \n",
    "    progress = tqdm(range(steps))\n",
    "    try:\n",
    "        for step_idx in progress:\n",
    "            batch = torch.concat([\n",
    "                trans(image).unsqueeze(0)\n",
    "                for _ in range(batch_size)\n",
    "            ])\n",
    "            model.zero_grad()\n",
    "            output = model(batch)\n",
    "            target_layer = torch.zeros_like(layer_snapshot)\n",
    "            target_layer[:, 9, 0, 0] = 1\n",
    "            loss = loss_func(layer_snapshot, target_layer)\n",
    "            progress.set_postfix({\"loss\": loss.item()})#, \"mean_loss\": mean_loss.item(), \"mean\": batch.mean()})\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            with torch.no_grad():\n",
    "                image[:] = image.clamp(0, 1)\n",
    "    \n",
    "            if step_idx % 50 == 0:\n",
    "                with output_widget:\n",
    "                    output_widget.clear_output()\n",
    "                    display(VF.to_pil_image(resize(image, 2)))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    hook.remove()\n",
    "    \n",
    "    return image[:].cpu()\n",
    "        \n",
    "img = optimize_image_2(\n",
    "    torch.rand(3, 128, 128) * .0 + .3, model, \n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6c5437-1711-4ce5-a0f8-6c47d9602d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "[n for n, l in iter_module_layers(model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc3c5d-52c3-41e2-a1fd-3a90c328e4fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
