{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e36fb7-b6ac-47a3-bd4b-8ac83bb37093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee486d-51c8-4ba9-a0df-ce4eb903745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KANPolyLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    based on https://github.com/SciYu/KAE/blob/main/DenseLayerPack/KAE.py\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim: int,\n",
    "            out_dim: int,\n",
    "            order: int,\n",
    "            bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.order = order\n",
    "        self.coeffs = nn.Parameter(torch.randn(out_dim, input_dim, order + 1) * 0.01)\n",
    "        self.bias = None\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(1, out_dim))\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"input_dim={self.input_dim}, out_dim={self.out_dim}, order={self.order}, bias={self.bias is not None}\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        x_expanded = x.unsqueeze(1).expand(-1, self.out_dim, -1)\n",
    "\n",
    "        y = torch.zeros((x.shape[0], self.out_dim), device=x.device, dtype=x.dtype)\n",
    "\n",
    "        for i in range(self.order + 1):\n",
    "            term = (x_expanded ** i) * self.coeffs[:, :, i]\n",
    "            y += term.sum(dim=-1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            y = y + self.bias\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class KanMLPLayer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            channels_in: int,\n",
    "            channels_out: int,\n",
    "            layer_type: str = \"mlp\",  # \"mlp\", \"kanpoly<O>\",\n",
    "            activation: Union[None, str, Callable] = None,\n",
    "            bias: bool = True,\n",
    "            residual: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._residual = residual and channels_in == channels_out\n",
    "        self._layer_type = layer_type\n",
    "\n",
    "        if layer_type == \"mlp\":\n",
    "            self.module = nn.Linear(channels_in, channels_out, bias=bias)\n",
    "        elif layer_type.startswith(\"kanpoly\"):\n",
    "            self.module = KANPolyLayer(channels_in, channels_out, order=int(layer_type[7:]), bias=bias)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown layer_type '{layer_type}'\")\n",
    "            \n",
    "        self.act = activation_to_module(activation)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.module(x)\n",
    "        if self.act is not None:\n",
    "            y = self.act(y)\n",
    "        if self._residual:\n",
    "            y = y + x\n",
    "        return y\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"layer_type='{self._layer_type}', residual={self._residual}\"\n",
    "\n",
    "\n",
    "class KanMLPStack(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            channels: Tuple[int, ...],\n",
    "            layer_type: Tuple[str, ...],\n",
    "            activation: Tuple[Union[None, str, Callable], ...],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.stack = nn.Sequential()\n",
    "\n",
    "        for (ch, next_ch), layer_type_, act_ in zip(\n",
    "                zip(channels, channels[1:]),\n",
    "                layer_type,\n",
    "                activation,\n",
    "                \n",
    "        ):\n",
    "            self.stack.append(KanMLPLayer(ch, next_ch, layer_type=layer_type_, activation=act_))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.stack(x)  \n",
    "                            \n",
    "\n",
    "class KanMLPAE(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            image_shape: Tuple[int, int, int],\n",
    "            encoder_channels: Tuple[int, ...],\n",
    "            encoder_layer_type: Tuple[str, ...],\n",
    "            encoder_activation: Tuple[Union[None, str, Callable], ...],\n",
    "            decoder_channels: Tuple[int, ...],\n",
    "            decoder_layer_type: Tuple[str, ...],\n",
    "            decoder_activation: Tuple[Union[None, str, Callable], ...],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(-3),\n",
    "            KanMLPStack(\n",
    "                channels=[math.prod(image_shape), *encoder_channels],\n",
    "                layer_type=encoder_layer_type,\n",
    "                activation=encoder_activation,\n",
    "            )\n",
    "        )\n",
    "            \n",
    "        self.decoder = nn.Sequential(\n",
    "            KanMLPStack(\n",
    "                channels=[*decoder_channels, math.prod(image_shape)],\n",
    "                layer_type=decoder_layer_type,\n",
    "                activation=decoder_activation,\n",
    "            ),\n",
    "            Reshape(image_shape),\n",
    "        )\n",
    "        \n",
    "    def forward(self, batch: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.encoder(batch)\n",
    "        y = self.decoder(y)\n",
    "        return y\n",
    "        \n",
    "module = KanMLPAE(\n",
    "    image_shape=(3, 64, 64), \n",
    "    encoder_channels=[128, 64],\n",
    "    encoder_layer_type=[\"mlp\", \"kanpoly3\"],\n",
    "    encoder_activation=[\"gelu\", \"none\"],\n",
    "    decoder_channels=[64, 128],\n",
    "    decoder_layer_type=[\"mlp\", \"mlp\"],\n",
    "    decoder_activation=[\"gelu\", \"none\"],\n",
    ")\n",
    "print(f\"params: {num_module_parameters(module):,}\")\n",
    "inp = torch.ones(2, 3, 64, 64)\n",
    "outp = module(inp)\n",
    "print(\"outp\", outp.shape)\n",
    "display(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a2b82-586c-40a2-8c34-7e1c72dc4e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765528ee-9b71-4f8a-9841-6be23826a366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
