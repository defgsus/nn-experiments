{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f90f53-d12b-4323-88b7-97a25c69878e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Optional, Union\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util import *\n",
    "from src.util.image import * \n",
    "from src.algo import Space2d, IFS\n",
    "from src.datasets import *\n",
    "from src.models.cnn import *\n",
    "from src.util.embedding import *\n",
    "from src.models.clip import ClipSingleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a60b59-ed8e-4e15-951a-cef5d7e07e48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_samples(\n",
    "        iterable, \n",
    "        total: int = 32, \n",
    "        nrow: int = 8, \n",
    "        return_image: bool = False, \n",
    "        show_compression_ratio: bool = False,\n",
    "        label: Optional[Callable] = None,\n",
    "):\n",
    "    samples = []\n",
    "    labels = []\n",
    "    f = ImageFilter()\n",
    "    try:\n",
    "        for idx, entry in enumerate(tqdm(iterable, total=total)):\n",
    "            image = entry\n",
    "            if isinstance(entry, (list, tuple)):\n",
    "                image = entry[0]\n",
    "            if image.ndim == 4:\n",
    "                image = image.squeeze(0)\n",
    "            samples.append(image)\n",
    "            if show_compression_ratio:\n",
    "                labels.append(round(f.calc_compression_ratio(image), 3))\n",
    "            elif label is not None:\n",
    "                labels.append(label(entry) if callable(label) else idx)\n",
    "                \n",
    "            if len(samples) >= total:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    if labels:\n",
    "        image = VF.to_pil_image(make_grid_labeled(samples, nrow=nrow, labels=labels))\n",
    "    else:\n",
    "        image = VF.to_pil_image(make_grid(samples, nrow=nrow, pad_value=0))\n",
    "    if return_image:\n",
    "        return image\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee5360-6793-45ca-af53-91b1d5413c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = RpgTileIterableDataset(shape=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf6215f-22be-4d10-aa04-f29f665e8e78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_samples(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1b483f-ccb3-4686-a583-fac2836a5f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d6cde0-efe0-4f8b-bbf7-8d222945ab50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_train = SplitIterableDataset(ds, ratio=20, train=True)\n",
    "ds_test  = SplitIterableDataset(ds, ratio=20, train=False)\n",
    "print(len(list(ds_train)))\n",
    "print(len(list(ds_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8835d6-4f93-4cbf-83cd-939c6f64febf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90baeb3-80ca-4e6c-9eb4-15a0a2e9134b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = next(iter(DataLoader(ds, batch_size=1_000_000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c681810-0003-4d29-ab4d-5baae3106ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26610cf3-4987-43d1-aebd-49e748cb320d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save((images.clamp(0, 1) * 255).to(torch.uint8), \"../datasets/rpg-3x32x32-uint.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d981e-174a-4abe-8dfa-69d0e2f072b1",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce299dc-d69d-48eb-b72a-3d719eb209f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = PCA(128)\n",
    "pca.fit(images.flatten(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e96fd0-30ed-47ff-b256-acb829eddb05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VF.to_pil_image(make_grid(torch.Tensor(pca.components_).reshape(-1, *ds.shape), normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeafbd3-09cd-4ae7-8226-e1107677ed16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = torch.Tensor(pca.transform(images.flatten(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688e04dc-27f6-4dc5-aee6-67b99e391278",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clusterer = KMeans(100, n_init=\"auto\")\n",
    "labels = clusterer.fit_predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e321710-406d-4db4-9927-16329dcc7a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist, _ = np.histogram(labels, clusterer.n_clusters, (0, clusterer.n_clusters))\n",
    "hist.sort()\n",
    "px.bar(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee49c1bc-4181-467c-99af-fb35ece76a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_images = []\n",
    "grid_labels = []\n",
    "for label in range(clusterer.n_clusters):\n",
    "    indices = np.argwhere(labels == label).reshape(-1)[:30]\n",
    "    sample = images[indices]\n",
    "    for s in sample:\n",
    "        grid_images.append(s)\n",
    "    for i in indices:\n",
    "        grid_labels.append(i)\n",
    "        \n",
    "display(VF.to_pil_image(make_grid(\n",
    "    grid_images, nrow=32, \n",
    "    #labels=grid_labels\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fde8973-0db7-4ab8-8382-d26a07ae1a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfec4a9-bc63-4b77-ba65-0e7842788d53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ana = FactorAnalysis(128)\n",
    "features2 = ana.fit_transform(images.flatten(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8dfc82-6701-4fb5-968c-2805270f39e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VF.to_pil_image(make_grid(torch.Tensor(ana.components_).reshape(-1, *ds.shape), normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5789ec-a7b8-4f41-9ea7-9a564c9027a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clusterer = KMeans(100, n_init=\"auto\")\n",
    "labels2 = clusterer.fit_predict(features2)\n",
    "\n",
    "hist, _ = np.histogram(labels, clusterer.n_clusters, (0, clusterer.n_clusters))\n",
    "hist.sort()\n",
    "px.bar(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54525335-ee2a-4026-a23a-3dcf8b4ee21d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_images = []\n",
    "grid_labels = []\n",
    "for label in range(clusterer.n_clusters):\n",
    "    indices = np.argwhere(labels2 == label).reshape(-1)[:30]\n",
    "    sample = images[indices]\n",
    "    for s in sample:\n",
    "        grid_images.append(s)\n",
    "    for i in indices:\n",
    "        grid_labels.append(i)\n",
    "        \n",
    "display(VF.to_pil_image(make_grid(\n",
    "    grid_images, nrow=32, \n",
    "    #labels=grid_labels\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de9e0e2-1a10-4bd4-857f-ed29558acce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54f5b2b7-439b-4537-95bf-467ce0ebfe68",
   "metadata": {},
   "source": [
    "# via AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c33f3f-7f98-4a89-84d4-31fb4b6641c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts.train_autoencoder_vae import VariationalAutoencoderConv\n",
    "model = VariationalAutoencoderConv((3, 32, 32), channels=[16, 24, 32], kernel_size=5, latent_dims=128)\n",
    "model.load_state_dict(torch.load(\"../checkpoints/vae-rpg-1/snapshot.pt\")[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a8407-3f24-448f-87e6-54d7b2b693fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def model_forward(model: nn.Module, x: torch.Tensor, batch_size: int = 128) -> torch.Tensor:\n",
    "    outputs = []\n",
    "    with tqdm(desc=type(model).__name__, total=x.shape[0]) as progress:\n",
    "        for i in range(0, x.shape[0], batch_size):\n",
    "            outputs.append(model(x[i: i + batch_size]))\n",
    "            progress.update(outputs[-1].shape[0])\n",
    "    return torch.concat(outputs, dim=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39802287-f54c-431d-a935-b05b90540c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    features3 = normalize_embedding(model_forward(model.encoder, images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c06f597-d75a-4a72-b4e8-82d4f92e1964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(features3, \"tmp_features3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db096a5b-f3c3-4a08-8d3b-b7f10be63eb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features3 = torch.load(\"tmp_features3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f8bd6-cd0a-4ab9-99ec-0cecdb6a349b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clusterer = KMeans(100, n_init=\"auto\")\n",
    "labels3 = clusterer.fit_predict(features3)\n",
    "\n",
    "hist, _ = np.histogram(labels3, clusterer.n_clusters, (0, clusterer.n_clusters))\n",
    "hist.sort()\n",
    "px.bar(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ad9b7-fb56-49d6-815b-a67dcb435ddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_images = []\n",
    "grid_labels = []\n",
    "for label in range(clusterer.n_clusters):\n",
    "    indices = np.argwhere(labels3 == label).reshape(-1)[:30]\n",
    "    sample = images[indices]\n",
    "    for s in sample:\n",
    "        grid_images.append(s)\n",
    "    for i in indices:\n",
    "        grid_labels.append(i)\n",
    "        \n",
    "display(VF.to_pil_image(make_grid(\n",
    "    grid_images, nrow=32, \n",
    "    #labels=grid_labels\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f20e812-7c39-4d98-92bd-faf1d58e628d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sim3 = features3 @ features3.T\n",
    "    #sim_indices = sim3.argsort(1)\n",
    "    #print(sim_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49d264-28d0-4f6d-bc98-7cc3fafb0791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.imshow(sim3[:100, :100], height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f71d0-afe9-43b4-b0aa-2411653657e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sorted_indices = sim3[:1000].argsort(dim=-1)\n",
    "sorted_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246df404-a828-4913-b89e-accab9007bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VF.to_pil_image(make_grid(images[sorted_indices[:10, :10].flatten(0)], nrow=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc8e753-1945-4bc9-9224-79bad7a06bce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_indices[:10, :10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
