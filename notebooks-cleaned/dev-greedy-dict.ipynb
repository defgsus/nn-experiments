{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3767f8d3-dbba-4884-8c96-eb602cd2548a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union, Dict\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset, RandomSampler\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.algo import GreedyLibrary\n",
    "from src.util.image import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e304a406-80f9-44d8-9713-53e863cc7704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalGreedyLibrary:\n",
    "    \n",
    "    def __init__(self, n_entries: int, shape: Iterable[int], mean: float = 0., std: float = 0.01):\n",
    "        self.shape = tuple(shape)\n",
    "        self.n_entries = n_entries\n",
    "        self.entries = mean + std * torch.randn(n_entries, *self.shape, generator=torch.Generator().manual_seed(23))\n",
    "        self.hits = [0] * n_entries\n",
    "    \n",
    "    @property\n",
    "    def max_hits(self) -> int:\n",
    "        return max(*self.hits) if self.hits else 0\n",
    "    \n",
    "    def copy(self) -> \"CreedyLibrary\":\n",
    "        d = self.__class__(0, self.shape)\n",
    "        d.n_entries = self.n_entries\n",
    "        d.entries = self.entries[:]\n",
    "        d.hits = self.hits.copy()\n",
    "        return d\n",
    "    \n",
    "    def top_entry_index(self) -> Optional[int]:\n",
    "        top_idx, top_hits = None, None\n",
    "        for i, hits in enumerate(self.hits):\n",
    "            if top_idx is None or hits > top_hits:\n",
    "                top_idx, top_hits = i, hits\n",
    "        return top_idx\n",
    "    \n",
    "    def entry_ranks(self, reverse: bool = False) -> List[int]:\n",
    "        \"\"\"\n",
    "        Returns a list of ranks for each entry, \n",
    "        where rank means the index sorted by number of hits.\n",
    "        \"\"\"\n",
    "        entry_ids = list(range(self.n_entries))\n",
    "        entry_ids.sort(key=lambda i: self.hits[i], reverse=reverse)\n",
    "        return [entry_ids.index(i) for i in range(self.n_entries)]\n",
    "    \n",
    "    def entry_hits(self, reverse: bool = False) -> Dict[int, int]:\n",
    "        \"\"\"\n",
    "        Returns a dict of `entry-index` -> `number-of-hits`.\n",
    "        \n",
    "        Sorted by number of hits.\n",
    "        \"\"\"\n",
    "        entry_ids = list(range(self.n_entries))\n",
    "        entry_ids.sort(key=lambda i: self.hits[i], reverse=reverse)\n",
    "        return {\n",
    "            i: self.hits[i]\n",
    "            for i in entry_ids\n",
    "        }\n",
    "    \n",
    "    def sorted_entry_indices(\n",
    "            self, \n",
    "            by: str = \"hits\", \n",
    "            reverse=True,\n",
    "    ) -> List[int]:\n",
    "        if not self.n_entries:\n",
    "            return []\n",
    "        entry_ids = list(range(self.n_entries))\n",
    "        if self.n_entries < 2:\n",
    "            return entry_ids\n",
    "        \n",
    "        if by == \"hits\":\n",
    "            entry_ids.sort(key=lambda i: self.hits[i], reverse=reverse)\n",
    "        \n",
    "        elif by == \"tsne\":\n",
    "            from sklearn.manifold import TSNE\n",
    "            tsne = TSNE(1, perplexity=min(30, self.n_entries - 1))\n",
    "            positions = tsne.fit_transform(self.entries.reshape(self.entries.shape[0], -1).numpy())\n",
    "            entry_ids.sort(key=lambda i: positions[i], reverse=reverse)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported sort by '{by}'\")\n",
    "\n",
    "        return entry_ids\n",
    "\n",
    "    def fit(self, batch: torch.Tensor, lr: float = 1., skip_top_entries: Union[bool, int] = False):\n",
    "        best_entry_ids = self.best_entries_for(batch, skip_top_entries=skip_top_entries)\n",
    "        #print(best_entry_ids.tolist())\n",
    "        for i in range(batch.shape[0]):\n",
    "            entry_id = best_entry_ids[i]\n",
    "            weight = 1. / (1 + self.hits[entry_id])\n",
    "            self.entries[entry_id] += lr * weight * (batch[i] - self.entries[entry_id])\n",
    "            self.hits[entry_id] += 1\n",
    "           \n",
    "    def best_entries_for(self, batch: torch.Tensor, skip_top_entries: Union[bool, int] = False) -> torch.Tensor:\n",
    "        assert batch.ndim == len(self.shape) + 1, f\"Got {batch.shape}\"\n",
    "        assert batch.shape[1:] == self.shape, f\"Got {batch.shape}\"\n",
    "        ones = tuple(1 for _ in self.shape)\n",
    "        repeated_entries = self.entries.repeat(batch.shape[0], *ones)\n",
    "        repeated_batch = batch.repeat(1, self.n_entries, *ones[1:]).reshape(-1, *self.shape)\n",
    "        dist = (repeated_entries - repeated_batch).abs()\n",
    "        while dist.ndim > 1:\n",
    "            dist = dist.sum(1)\n",
    "        dist = dist.reshape(batch.shape[0], -1)\n",
    "        if not skip_top_entries:\n",
    "            return torch.argmin(dist, 1)\n",
    "        \n",
    "        skip_top_entries = int(skip_top_entries)\n",
    "        sorted_indices = torch.argsort(dist, 1)\n",
    "        entry_ranks = self.entry_ranks(reverse=True)\n",
    "        best_entries = []\n",
    "        for indices in sorted_indices:\n",
    "            idx = 0\n",
    "            while idx + 1 < len(indices) and entry_ranks[indices[idx]] < skip_top_entries:\n",
    "                idx += 1\n",
    "            #print(idx)\n",
    "            best_entries.append(indices[idx])\n",
    "        return torch.Tensor(best_entries).to(torch.int32)\n",
    "    \n",
    "    def plot_entries(self, min_size: int = 300, with_hits: bool = True, sort_by: Optional[str] = None):\n",
    "        if len(self.shape) == 1:\n",
    "            entries = self.entries.reshape(-1, 1, 1, *self.shape)\n",
    "        elif len(self.shape) == 2:\n",
    "            entries = self.entries.reshape(-1, 1, *self.shape)\n",
    "        elif len(self.shape) == 3:\n",
    "            entries = self.entries\n",
    "        else:\n",
    "            raise RuntimeError(f\"Can't plot entries with shape {self.shape} (ndim>3)\")\n",
    "        if entries.shape[0]:\n",
    "            \n",
    "            e_min, e_max = entries.min(), entries.max()\n",
    "            if e_min != e_max:\n",
    "                entries = (entries - e_min) / (e_max - e_min)\n",
    "            \n",
    "            if with_hits:\n",
    "                max_hits = max(1, self.max_hits)\n",
    "                entry_list = []\n",
    "                for entry, hits in zip(entries, self.hits):\n",
    "                    if entry.shape[0] == 1:\n",
    "                        entry = entry.repeat(3, *(1 for _ in entry.shape[1:]))\n",
    "                    elif entry.shape[0] == 3:\n",
    "                        pass\n",
    "                    else:\n",
    "                        raise ValueError(f\"Can't plot entries with {entry.shape[0]} channels\")\n",
    "                    \n",
    "                    background = torch.Tensor([0, hits / max_hits, 0])\n",
    "                    background = background.reshape(3, *((1,) * (len(entry.shape) - 1)))\n",
    "                    background = background.repeat(1, *(s + 2 for s in entry.shape[1:]))\n",
    "                    background[:, 1:-1, 1:-1] = entry\n",
    "                    entry_list.append(background)#VF.pad(entry, 1, (0, hits / max_hits, 0)))\n",
    "                entries = entry_list\n",
    "            \n",
    "            if sort_by:\n",
    "                if not isinstance(entries, list):\n",
    "                    entries = list(entries)\n",
    "                entry_ids = self.sorted_entry_indices(by=sort_by, reverse=True)\n",
    "                entries = [entries[i] for i in entry_ids]\n",
    "            \n",
    "            grid = make_grid(entries, nrow=max(1, int(np.sqrt(self.n_entries))), normalize=False)\n",
    "            if grid.shape[-1] < min_size:\n",
    "                grid = VF.resize(\n",
    "                    grid, (\n",
    "                        int(grid.shape[-2] * min_size / grid.shape[-1]),\n",
    "                        min_size,\n",
    "                    ), \n",
    "                    VF.InterpolationMode.NEAREST\n",
    "                )\n",
    "        else:\n",
    "            grid = torch.zeros(1, min_size, min_size)\n",
    "        return VF.to_pil_image(grid)\n",
    "    \n",
    "    def drop_unused(self):\n",
    "        self.drop_entries(hits_lt=1)\n",
    "        \n",
    "    def drop_entries(self, hits_lt: Optional[int] = None):\n",
    "        drop_idx = set()\n",
    "        if hits_lt is not None:\n",
    "            for i, hits in enumerate(self.hits):\n",
    "                if hits <= hits_lt:\n",
    "                    drop_idx.add(i)\n",
    "        \n",
    "        if drop_idx:\n",
    "            entries = []\n",
    "            hits = []\n",
    "            for i, (entry, h) in enumerate(zip(self.entries, self.hits)):\n",
    "                if i not in drop_idx:\n",
    "                    entries.append(entry.unsqueeze(0))\n",
    "                    hits.append(h)\n",
    "            self.entries = torch.concat(entries) if entries else torch.Tensor()\n",
    "            self.hits = hits\n",
    "            self.n_entries = len(self.hits)\n",
    "            \n",
    "d = LocalGreedyLibrary(4, (3,)).copy()\n",
    "for i in range(10):\n",
    "    d.fit(torch.rand(1, *d.shape), skip_top_entries=2)\n",
    "#d.fit(torch.Tensor([[-1, 1, 0], [0, 1, 0]]), skip_top_entries=1)\n",
    "#d.fit(torch.Tensor([[1, 0, 0], [0, 1, 0]]), skip_top_entries=1)\n",
    "#d.fit(torch.Tensor([[1, 0, 0], [0, 1, 0]]), skip_top_entries=1)\n",
    "#d = CreedyDictionary(10, (3, 2))\n",
    "#d.fit(torch.rand(5, 3, 2))\n",
    "print(\"hits   \", d.hits)\n",
    "print(\"hitmap \", d.entry_hits(reverse=True))\n",
    "print(\"ranks  \", d.entry_ranks(reverse=True))\n",
    "print(\"s hits \", d.sorted_entry_indices(reverse=True))\n",
    "print(\"s tsne \", d.sorted_entry_indices(by=\"tsne\", reverse=True))\n",
    "display(d.plot_entries(sort_by=\"hits\"))\n",
    "d.drop_entries(hits_lt=1)\n",
    "d.plot_entries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0798ba-68df-4397-9614-471f26275089",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = GreedyLibrary(100, (7, 7))\n",
    "for i in tqdm(range(1_00)):\n",
    "    d.fit(torch.randn(1000, *d.shape), skip_top_entries=0, metric=\"corr\")\n",
    "d.drop_unused()\n",
    "display(d.plot_entries(sort_by=\"hits\"))\n",
    "print(d.hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1be291d-c012-49e2-9817-12a3b6ac8c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6d4c1-74e8-4362-8dc3-dddb399d2d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -l ../datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02856b82-56e1-49a3-b954-6e7e40e9e17f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_data = torch.load(f\"../datasets/kali-uint8-{128}x{128}.pt\")#[:30000]\n",
    "#ds_data = torch.load(f\"../datasets/ifs-1x{128}x{128}-uint8-1000x32.pt\")#[:30000]\n",
    "#ds_data = torch.load(f\"../datasets/photos-64x64-bcr03.pt\")#[:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16dab9f-7909-477c-bab9-d5436621e261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = TransformDataset(\n",
    "    TensorDataset(ds_data),\n",
    "    dtype=torch.float, multiply=1. / 255.,\n",
    "    transforms=[\n",
    "        #VT.CenterCrop(64),\n",
    "        #VT.RandomCrop(SHAPE[-2:]),\n",
    "        VT.Grayscale(),\n",
    "    ],\n",
    "    num_repeat=1,\n",
    ")\n",
    "for batch, in DataLoader(ds, batch_size=10000):\n",
    "    ds_mean, ds_std = batch.mean(), batch.std()\n",
    "    break\n",
    "print(f\"mean {ds_mean}, std {ds_std}\")\n",
    "VF.to_pil_image(ds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab20ba8c-96e1-4aec-bf8d-7406448f03d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = GreedyLibrary(100, (1, 11, 11), std=.001*ds_std, mean=0.)\n",
    "#d.entries = d.entries.cuda()\n",
    "\n",
    "ds_small = TransformDataset(ds, transforms=[VT.RandomCrop(d.shape[-2:])], num_repeat=50)\n",
    "#for batch, in DataLoader(ds_small, batch_size=d.n_entries):\n",
    "#    d.entries = batch * torch.rand_like(batch)\n",
    "#    break\n",
    "try:\n",
    "    count, last_count = 0, 0\n",
    "    for batch, in tqdm(DataLoader(ds_small, batch_size=100, shuffle=True)):\n",
    "        d.fit(batch, lr=1., skip_top_entries=0, zero_mean=True, metric=\"corr\")\n",
    "        count += batch.shape[0]\n",
    "        if count > last_count + 5000:\n",
    "            last_count = count\n",
    "            top_idx = d.top_entry_index() \n",
    "            #print(f\"{top_idx}: {d.hits[top_idx]}\")\n",
    "            #break\n",
    "        #    display(d.plot_entries(min_size=100))\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "d.drop_entries(hits_lt=10, inplace=True).sort_entries(inplace=True, reverse=True)\n",
    "display(d.plot_entries(min_size=600, sort_by=\"hits\"))\n",
    "print(sorted(d.hits, reverse=True))\n",
    "display(d.plot_entries(min_size=600, sort_by=\"tsne\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a640c634-2280-498a-b152-0ba508d21885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d.entries.min(), d.entries.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadbeaf1-33c2-4c14-ada8-325fc9f28b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VF.to_pil_image(make_grid(F.max_pool2d(d.convolve((ds[0][0] - ds[0][0].mean()).to(\"cuda\")), 1).unsqueeze(1)).clamp(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd905461-90ab-488b-a61b-7c202fe7a648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(d.entries, \"../datasets/creedylib-1x11x11-signed-photos.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f98400-9118-45fb-bc22-8fd1155117bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VF.to_pil_image(signed_to_image(make_grid(torch.load(\"../datasets/creedylib-1x11x11-signed-kali.pt\"), nrow=30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcff79c-b168-43e7-9fa5-e37b2ac21e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d131e81-eb31-4213-a033-63bfde249a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d485fa41-5c28-422a-a93a-91dd6e3459b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d1bbc-bfe3-4d9d-98b8-a0ff5aab1b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = torch.load(\"../datasets/creedylib-1x11x11-signed-kali.pt\")\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd4029-a04f-459a-a569-18acf1558ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = ds[0][0]\n",
    "VF.to_pil_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3830e1f6-9c0c-42c2-9333-9953c8623231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#c = F.conv2d(image, weights, stride=4)\n",
    "#c = F.conv2d(image - image.mean(), weights - weights.mean(), stride=4)\n",
    "c = F.conv2d(image - image.mean(), d.entries, stride=4)\n",
    "c.shape#.sum(dim=1).sum(dim=1)\n",
    "display(VF.to_pil_image(make_grid(d.entries, nrow=16, normalize=True)))\n",
    "img = make_grid(c.unsqueeze(1), nrow=16, normalize=False)\n",
    "#img = signed_to_image(img)\n",
    "display(VF.to_pil_image(img.clamp(0, 1)))\n",
    "\n",
    "img = make_grid(F.max_pool2d(c, (5, 5)).unsqueeze(1), nrow=16).clamp(0, 1)\n",
    "VF.to_pil_image(VF.resize(img, (img.shape[-2] * 4, img.shape[-1] * 4), VF.InterpolationMode.NEAREST))\n",
    "#img = (img - img.min()) / (img.max() - img.min())\n",
    "#VF.to_pil_image(VF.resize(img, (img.shape[-2] * 4, img.shape[-1] * 4), VF.InterpolationMode.NEAREST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09db7a3b-eae4-4aea-95fa-c1108f1c9a3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    d.entries = d.entries.cuda()\n",
    "    def encode(batch):\n",
    "        c = d.convolve(batch.cuda())\n",
    "        c, _ = c.reshape(*c.shape[:2], -1).max(dim=-1)\n",
    "        return c.cpu()\n",
    "\n",
    "    features = []\n",
    "    try:\n",
    "        for batch, in tqdm(DataLoader(ds, batch_size=100)):\n",
    "            features.append(encode(batch))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    features = torch.concat(features)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dac1d0-01d5-4f42-b49b-29dbb635091e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(1)\n",
    "positions = tsne.fit_transform(features)\n",
    "label_ids = list(range(features.shape[0]))\n",
    "label_ids.sort(key=lambda i: positions[i])\n",
    "VF.to_pil_image(make_grid([ds[i][0] for i in label_ids[:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81c6bab-4eed-4289-8ba9-1ec9a3bf2a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#VF.to_pil_image(make_grid([ds[i][0] for i in label_ids], nrow=int(np.sqrt(len(label_ids))))).save(\"/home/bergi/Pictures/kali-1x128x128-sorted-by-tsne-of-creedylib.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b623a23-c3f7-49b3-9c00-67b93eb6a78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22646ba-5514-4d1b-9d3b-d523ccfb5b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def best_entries_for(\n",
    "        self,\n",
    "        batch: torch.Tensor,\n",
    "        skip_top_entries: Union[bool, int] = False,\n",
    "        metric: str = \"corr\",\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Returns the index of the best matching entry for each patch in the batch.\n",
    "\n",
    "    :param batch: Tensor of N patches of shape matching the library's shape\n",
    "    :param skip_top_entries: bool or int,\n",
    "        Do not match the top N entries (1 for True), sorted by number of hits\n",
    "    :return: tuple of\n",
    "        - Tensor of int64: entry indices\n",
    "        - Tensor of float: distances\n",
    "    \"\"\"\n",
    "    assert batch.ndim == len(self.shape) + 1, f\"Got {batch.shape}\"\n",
    "    assert batch.shape[1:] == self.shape, f\"Got {batch.shape}\"\n",
    "    metric = metric.lower()\n",
    "    \n",
    "    ones = tuple(1 for _ in self.shape)\n",
    "\n",
    "    repeated_entries = self.entries.repeat(batch.shape[0], *ones)\n",
    "    repeated_batch = batch.to(self.device).repeat(1, self.n_entries, *ones[1:]).reshape(-1, *self.shape)\n",
    "            \n",
    "    if metric in (\"l1\", \"mae\"):\n",
    "        dist = (repeated_entries - repeated_batch).abs().flatten(1).sum(1)\n",
    "    elif metric in (\"l2\", \"mse\"):\n",
    "        dist = (repeated_entries - repeated_batch).pow(2).flatten(1).sum(1).sqrt()\n",
    "    elif metric.startswith(\"corr\"):\n",
    "        dist = -(repeated_entries - repeated_batch).flatten(1).sum(1)\n",
    "    \n",
    "    dist = dist.reshape(batch.shape[0], -1)\n",
    "\n",
    "    if not skip_top_entries:\n",
    "        indices = torch.argmin(dist, 1)\n",
    "        return (\n",
    "            indices,\n",
    "            dist.flatten()[\n",
    "                indices + torch.linspace(0, indices.shape[0] - 1, indices.shape[0]).to(torch.int64).to(indices.device) * self.n_entries\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    skip_top_entries = int(skip_top_entries)\n",
    "    sorted_indices = torch.argsort(dist, 1)\n",
    "    entry_ranks = self.entry_ranks(reverse=True)\n",
    "    best_entries = []\n",
    "    for indices in sorted_indices:\n",
    "        idx = 0\n",
    "        while idx + 1 < len(indices) and entry_ranks[indices[idx]] < skip_top_entries:\n",
    "            idx += 1\n",
    "        best_entries.append(indices[idx])\n",
    "\n",
    "    indices = torch.Tensor(best_entries).to(torch.int64)\n",
    "    return (\n",
    "        indices,\n",
    "        dist.flatten()[\n",
    "            indices + torch.linspace(0, indices.shape[0] - 1, indices.shape[0]).to(torch.int64).to(indices.device) * self.n_entries\n",
    "        ]\n",
    "    )\n",
    "\n",
    "lib = GreedyLibrary(10, (1, 2, 3))\n",
    "lib.fit(torch.randn(100, 1, 2, 3))\n",
    "best_entries_for(lib, torch.randn(2, 1, 2, 3), metric=\"l2\")\n",
    "#lib.plot_entries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c53b5-37a0-451e-bf5f-aded4e3039ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7021dc0-ffc5-4f1b-8581-32b5a752dbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409eced7-cbde-4a8d-b07a-b900746d59d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
