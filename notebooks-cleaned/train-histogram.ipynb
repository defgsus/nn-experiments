{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9340b8-d0df-4243-a0ec-12c0daa896c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "#from torchvision.transforms import v2\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import clip\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util import *\n",
    "from src.util.image import * \n",
    "from src.algo import *\n",
    "from src.algo.wangtiles import *\n",
    "from src.datasets.generative import *\n",
    "from src.models.cnn import *\n",
    "from src.models.transform import *\n",
    "from src.util.embedding import *\n",
    "from src.models.loss import *\n",
    "from src.functional import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff632f-3f4a-42be-8bfa-f7d24b36d93b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts import datasets\n",
    "ds = datasets.RpgTileIterableDataset((1, 32, 32))\n",
    "ds = IterableShuffle(ds, 1000)\n",
    "VF.to_pil_image(next(iter(ds)))#.shape\n",
    "samples = next(iter(DataLoader(ds, batch_size=32)))\n",
    "\n",
    "VF.to_pil_image(make_grid(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1500fa73-82a4-442d-9d7f-3fb5a379ee73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Debug(nn.Module):\n",
    "    def __init__(self, name: str = \"Debug\"):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            print(f\"{self.name}: {x.shape}\")\n",
    "        else:\n",
    "            print(f\"{self.name}: {type(x).__name__}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb81de4-4be4-4bc9-b8f1-dbb147226db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ImageFolderIterableDataset(\"/home/bergi/Pictures/__diverse/\")\n",
    "ds = TransformIterableDataset(\n",
    "    ds, transforms=[\n",
    "         \n",
    "         #VF.resized_crop\n",
    "         #Debug(\">>>\"),\n",
    "         lambda x: set_image_channels(x, 3),\n",
    "         #lambda x: VF.resize(x, (max(256, x.shape[-2] // 2), max(256, x.shape[-1] // 2)), antialias=True),\n",
    "         lambda x: image_resize_crop(x, (256, 256)),\n",
    "         VT.CenterCrop((256, 256)),\n",
    "         #Debug(\"  >\"),\n",
    "         \n",
    "    ]\n",
    ")\n",
    "images = [img for img, _ in zip(ds, range(64))]\n",
    "\n",
    "VF.to_pil_image(make_grid_labeled(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d56cd2-2f92-45ec-b078-16fd8a7c3ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelModel(nn.Module):\n",
    "    def __init__(self, image: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.shape = image.shape\n",
    "        self.code = nn.Parameter(image.clone())\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.code\n",
    "\n",
    "    def reset(self):\n",
    "        with torch.no_grad():\n",
    "            self.code[:] = torch.randn_like(self.code) * .1 + .3\n",
    "\n",
    "PixelModel(images[0])().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dbf494-a35e-4802-99ec-d0a7932bae77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_histogram(\n",
    "    image: torch.Tensor,\n",
    "    target_histogram: torch.Tensor,\n",
    "    iters: int = 10,\n",
    "    batch_size: int = 1,\n",
    "    lr: float = 1.,\n",
    "    bins: int = 100,\n",
    "    sigma: float = 100.,\n",
    "    device=\"auto\",\n",
    "    hrange=(0, 1),\n",
    "):    \n",
    "    device = to_torch_device(device)\n",
    "    print(device)\n",
    "\n",
    "    target_batch = target_histogram.to(device).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "    pixel_model = PixelModel(image).to(device)\n",
    "    \n",
    "    #optimizer = torch.optim.Adadelta(pixel_model.parameters(), lr=lr * 100.)\n",
    "    optimizer = torch.optim.Adam(pixel_model.parameters(), lr=lr)\n",
    "\n",
    "    transforms = VT.Compose([\n",
    "        VT.RandomErasing(),\n",
    "        #VT.Pad(30, padding_mode=\"reflect\"),\n",
    "        #Debug(\"model\"),\n",
    "        #RandomWangMap((8, 8), probability=1, overlap=0, num_colors=2),\n",
    "        #Debug(\"wangmap\"),\n",
    "        #lambda x: x + .1 * torch.randn_like(x),\n",
    "        #VT.RandomAffine(\n",
    "        #   degrees=35.,\n",
    "            #scale=(1., 3.),\n",
    "            #scale=(.3, 1.),\n",
    "            #translate=(0, 4. / 64.),\n",
    "        #),\n",
    "        #VT.RandomCrop((224, 224)),\n",
    "    ])\n",
    "        \n",
    "    # train\n",
    "\n",
    "    pixel_history = []\n",
    "    try:\n",
    "        for it in tqdm(range(iters)):\n",
    "            # display(VF.to_pil_image(pixel_model()))\n",
    "            \n",
    "            pixels = pixel_model()\n",
    "            pixel_batch = []\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                pixel_batch.append(transforms(pixels).unsqueeze(0))\n",
    "            pixel_batch = torch.concat(pixel_batch)\n",
    "\n",
    "            B, C = pixel_batch.shape[:2]\n",
    "\n",
    "            hist_batch = soft_histogram(\n",
    "                rgb_to_hsv(pixel_batch).view(B * C, -1),\n",
    "                bins, *hrange, sigma,\n",
    "            ).view(B, C, bins)\n",
    "\n",
    "            #loss = F.kl_div(hist_batch, target_batch, reduction=\"batchmean\")\n",
    "            loss = F.mse_loss(hist_batch, target_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if it % max(2, iters / 100) == 0:\n",
    "                pixel_history.append(pixels.cpu().detach())\n",
    "\n",
    "            if it == 0:\n",
    "                display(px.line(pd.DataFrame({\n",
    "                    \"target\": target_histogram.cpu().view(-1),\n",
    "                    \"image\": hist_batch.detach().cpu().mean(dim=0).view(-1),\n",
    "                })))                \n",
    "            if len(pixel_history) >= 10:\n",
    "                print(float(loss))\n",
    "                #display(VF.to_pil_image(pixels.detach().to(\"cpu\")))\n",
    "                display(VF.to_pil_image(make_grid(pixel_history[:10], nrow=len(pixel_history)).clamp(0, 1)))\n",
    "                #display(VF.to_pil_image(RandomWangMap((8, 40), overlap=5)(pixel_history[-1])))\n",
    "                pixel_history.clear()\n",
    "                display(px.line(pd.DataFrame({\n",
    "                    \"target\": target_histogram.cpu().view(-1),\n",
    "                    \"image\": hist_batch.detach().cpu().mean(dim=0).view(-1),\n",
    "                })))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    if pixel_history:\n",
    "        display(VF.to_pil_image(make_grid(pixel_history[:10], nrow=len(pixel_history)).clamp(0, 1)))\n",
    "                \n",
    "    #display(VF.to_pil_image(pixel_model()))\n",
    "\n",
    "BINS=20\n",
    "train_histogram(\n",
    "    images[31],\n",
    "    soft_histogram(rgb_to_hsv(images[7]), BINS, -1, 2, 10),\n",
    "    #torch.ones(3, 100) * math.prod(images[0].shape[-2:]) / 100, \n",
    "    #batch_size=1,\n",
    "    bins=BINS,\n",
    "    lr=.0001,\n",
    "    iters=10000,\n",
    "    sigma=10,\n",
    "    hrange=(-1, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d8597-dead-41ff-ab3b-fbc9c5bf17c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_histogram_2(\n",
    "    source_image: torch.Tensor,\n",
    "    target_image: torch.Tensor,\n",
    "    iters: int = 10,\n",
    "    batch_size: int = 1,\n",
    "    lr: float = 1.,\n",
    "    bins: int = 100,\n",
    "    sigma: float = 100.,\n",
    "    device=\"auto\",\n",
    "    hrange=(0, 1),\n",
    "):    \n",
    "    device = to_torch_device(device)\n",
    "    print(device)\n",
    "\n",
    "    transforms = VT.Compose([\n",
    "        VT.RandomErasing(),\n",
    "        #VT.Pad(30, padding_mode=\"reflect\"),\n",
    "        #Debug(\"model\"),\n",
    "        #RandomWangMap((8, 8), probability=1, overlap=0, num_colors=2),\n",
    "        #Debug(\"wangmap\"),\n",
    "        #lambda x: x + .1 * torch.randn_like(x),\n",
    "        #VT.RandomAffine(\n",
    "        #   degrees=35.,\n",
    "            #scale=(1., 3.),\n",
    "            #scale=(.3, 1.),\n",
    "            #translate=(0, 4. / 64.),\n",
    "        #),\n",
    "        #VT.RandomCrop((224, 224)),\n",
    "    ])\n",
    "        \n",
    "    # train\n",
    "    pixel_history = []\n",
    "    for rev_it in range(2):\n",
    "        if rev_it == 1:\n",
    "            source_image, target_image = target_image, source_image\n",
    "        \n",
    "        target_histogram = soft_histogram(target_image, bins, *hrange, sigma)\n",
    "        print(target_histogram.shape)\n",
    "        target_batch = target_histogram.to(device).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        pixel_model = PixelModel(source_image).to(device)\n",
    "        \n",
    "        #optimizer = torch.optim.Adadelta(pixel_model.parameters(), lr=lr * 100.)\n",
    "        optimizer = torch.optim.Adam(pixel_model.parameters(), lr=lr)\n",
    "    \n",
    "        for it in tqdm(range(iters)):\n",
    "            # display(VF.to_pil_image(pixel_model()))\n",
    "            \n",
    "            pixels = pixel_model()\n",
    "            pixel_batch = []\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                pixel_batch.append(transforms(pixels).unsqueeze(0))\n",
    "            pixel_batch = torch.concat(pixel_batch)\n",
    "\n",
    "            B, C = pixel_batch.shape[:2]\n",
    "\n",
    "            hist_batch = soft_histogram(\n",
    "                rgb_to_hsv(pixel_batch).view(B * C, -1),\n",
    "                bins, *hrange, sigma,\n",
    "            ).view(B, C, bins)\n",
    "\n",
    "            #loss = F.kl_div(hist_batch, target_batch, reduction=\"batchmean\")\n",
    "            loss = F.mse_loss(hist_batch, target_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if it % max(2, iters / 10) == 0:\n",
    "                pixel_history.append(pixels.cpu().detach())\n",
    "\n",
    "            if it == 0:\n",
    "                display(px.line(pd.DataFrame({\n",
    "                    \"target\": target_histogram.cpu().view(-1),\n",
    "                    \"image\": hist_batch.detach().cpu().mean(dim=0).view(-1),\n",
    "                })))                \n",
    "            \n",
    "            if False and len(pixel_history) >= 10:\n",
    "                print(float(loss))\n",
    "                #display(VF.to_pil_image(pixels.detach().to(\"cpu\")))\n",
    "                display(VF.to_pil_image(make_grid(pixel_history[:10], nrow=len(pixel_history)).clamp(0, 1)))\n",
    "                #display(VF.to_pil_image(RandomWangMap((8, 40), overlap=5)(pixel_history[-1])))\n",
    "                pixel_history.clear()\n",
    "                display(px.line(pd.DataFrame({\n",
    "                    \"target\": target_histogram.cpu().view(-1),\n",
    "                    \"image\": hist_batch.detach().cpu().mean(dim=0).view(-1),\n",
    "                })))\n",
    "\n",
    "    display(VF.to_pil_image(make_grid(pixel_history, nrow=len(pixel_history) // 2).clamp(0, 1)))\n",
    "    \n",
    "    #display(VF.to_pil_image(pixel_model()))\n",
    "\n",
    "train_histogram_2(\n",
    "    images[31],\n",
    "    images[7],\n",
    "    #batch_size=1,\n",
    "    bins=20,\n",
    "    lr=.001,\n",
    "    iters=5000,\n",
    "    sigma=10,\n",
    "    hrange=(-1, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14fc553-bf2b-41cc-a5f3-f7e4709ee107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44239689-64f7-4079-9d34-fa066a28b394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fb2a16-84c4-4e96-87e0-8927b854788a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb30fa5-0cc5-4ab7-bcb1-95010fca06ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_wang(\n",
    "    pixel_model: nn.Module,\n",
    "    text: str,\n",
    "    iters: int = 100,\n",
    "    batch_size: int = 1,\n",
    "    lr: float = 10.,\n",
    "    device=\"auto\",\n",
    "):    \n",
    "    device = to_torch_device(device)\n",
    "    \n",
    "    #optimizer = torch.optim.Adadelta(pixel_model.parameters(), lr=lr * 100.)\n",
    "    optimizer = torch.optim.Adam(pixel_model.parameters(), lr=lr * .1)\n",
    "\n",
    "    \n",
    "    transforms = VT.Compose([\n",
    "        #VT.RandomErasing(),\n",
    "        #VT.Pad(30, padding_mode=\"reflect\"),\n",
    "        RandomWangMap((30, 30), probability=1),\n",
    "        VT.RandomAffine(\n",
    "            degrees=35.,\n",
    "            scale=(.3, 1.),\n",
    "            #translate=(0, 4. / 64.),\n",
    "        ),\n",
    "        VT.RandomCrop((224, 224)),\n",
    "    ])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        target_embeddings = ClipSingleton.encode_text(text).to(device)\n",
    "        target_embeddings = target_embeddings / target_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    target_dots = torch.ones(batch_size, 1).half().to(device)\n",
    "\n",
    "    def _pixels_for_clip(pixels):\n",
    "        pixels = pixels.to(device).half()\n",
    "        if pixels.shape[-3] != 3:\n",
    "            pixels = set_image_channels(pixels, 3)\n",
    "        #if pixels.shape[-2:] != (224, 224):\n",
    "        #    pixels = VF.resize(pixels, (224, 224), VT.InterpolationMode.BILINEAR, antialias=True)\n",
    "        return pixels\n",
    "    \n",
    "    def _clip_size(pixels):\n",
    "        if pixels.shape[-2:] != (224, 224):\n",
    "            pixels = VF.resize(pixels, (224, 224), VT.InterpolationMode.BILINEAR, antialias=True)\n",
    "        return pixels\n",
    "    \n",
    "    # find best start candidate\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        wang_template = VF.to_tensor(PIL.Image.open(\"/home/bergi/prog/python/thegame/thegame/assets/cr31/path.png\"))[:3] * 150\n",
    "        wang_template = VF.resize(wang_template, (32, 32), VF.InterpolationMode.BILINEAR, antialias=True)\n",
    "        code = model.encoder(wang_template.unsqueeze(0)).squeeze(0)\n",
    "        pixel_model.code[:] = code\n",
    "    \n",
    "    display(VF.to_pil_image(pixel_model()))\n",
    "    \n",
    "    # train\n",
    "    \n",
    "    pixel_history = []\n",
    "    try:\n",
    "        for it in tqdm(range(iters)):\n",
    "\n",
    "            pixel_batch = []\n",
    "            pixels = _pixels_for_clip(pixel_model())\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                pixel_batch.append(_clip_size(transforms(pixels)).unsqueeze(0))\n",
    "            pixel_batch = torch.concat(pixel_batch)\n",
    "\n",
    "            image_embeddings = ClipSingleton.encode_image(pixel_batch, requires_grad=True)\n",
    "            image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            dots = image_embeddings @ target_embeddings.T\n",
    "\n",
    "            loss = F.l1_loss(dots, target_dots)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if it % 2 == 0:\n",
    "                pixel_history.append(pixels.to(\"cpu\").detach())\n",
    "\n",
    "            if len(pixel_history) >= 10:\n",
    "                print(float(loss))\n",
    "                #display(VF.to_pil_image(pixels.detach().to(\"cpu\")))\n",
    "                display(VF.to_pil_image(make_grid(pixel_history, nrow=len(pixel_history))))\n",
    "                display(VF.to_pil_image(RandomWangMap((8, 40))(pixel_history[-1])))\n",
    "                pixel_history.clear()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    result = pixel_model().detach().to(\"cpu\")\n",
    "    #filename = \"\".join(\n",
    "    return result\n",
    "        \n",
    "\n",
    "VF.to_pil_image(visualize_wang(\n",
    "    #PixelModel((3, 224, 224)).to(\"cuda\"),\n",
    "    DecoderModel(dalle_decoder, 128, std=.1),\n",
    "    #DecoderModelHxW(dalle_decoder, 128, (4, 4), std=0.5),\n",
    "    #\"a house in the woods\",\n",
    "    #\"deep in the woods\",\n",
    "    #\"blood splattered wall\",\n",
    "    #\"cracked stone surface\",\n",
    "    #\"close-up of a smiling face\",\n",
    "    #\"yellow square on blue background\",\n",
    "    #\"lava river between black rock\",\n",
    "    #\"the sphere of planet earth in front of a black background\",\n",
    "    #\"checkerboard texture\",\n",
    "    #\"moss and stone\",\n",
    "    #\"stars in the sky\",\n",
    "    #\"deep space photography\",\n",
    "    #\"evil spiderweb\",\n",
    "    #\"weapons of mass destruction\",\n",
    "    #\"organic structures\",\n",
    "    #\"Bob Dobbs\",\n",
    "    \"underwater cobble texture\",\n",
    "    #\"top-down view of a river between meadows\",\n",
    "    #\"pile of guts\",\n",
    "    #\"stone texture\",\n",
    "    #\"top-down view of a city maze\",\n",
    "    #\"knotted ropes\",\n",
    "    #\"an unfriendly stone texture\",\n",
    "    #\"love & peace pattern\",\n",
    "    #\"friendly pattern\",\n",
    "    batch_size=2,\n",
    "    lr=1.5,\n",
    "    iters=1000,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ced827-d57b-4fd6-bff3-43cffcab88f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f41d5-5948-4d05-9fb6-1b38c5dbf955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e64267-659d-49d3-bc0e-feedacca8b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2bebc6-71fd-47e7-abc4-bd1a98497359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5751a4d5-f427-43d9-82f3-d46142c492bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VT.Pad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249eb18-f292-441d-9996-cafb0cae780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
