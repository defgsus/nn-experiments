{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffef548-0f17-4354-bec4-6d5bb8b0b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1364e6-5677-4312-b508-be665c995498",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = FefePostIterableDataset().freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16fb38d-cd8f-45b6-b81a-d3befff13af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(config.SMALL_DATASETS_PATH / \"fefe/tokenizer-bpe-4096-spaces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ac09df-46f6-4aed-a9b7-9fe1b1c2a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextStrideIterableDataset(BaseIterableDataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset: Union[Dataset, IterableDataset],\n",
    "            stride: Union[int, Tuple[int, int]],\n",
    "            split_character: Optional[str] = None,\n",
    "            min_length: int = 100,\n",
    "            seed: Optional[int] = None,\n",
    "    ):\n",
    "        self._dataset = dataset\n",
    "        self._stride = stride\n",
    "        self._split_character = split_character\n",
    "        self._min_length = min_length\n",
    "        if seed is None:\n",
    "            self._rng = random\n",
    "        else:\n",
    "            self._rng = random.Random(seed)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for text in self._dataset:\n",
    "            while len(text) >= self._min_length:\n",
    "                if not text:\n",
    "                    break\n",
    "                yield text\n",
    "\n",
    "                stride = self._get_stride()\n",
    "                if self._split_character is None:\n",
    "                    text = text[stride:]\n",
    "                else:\n",
    "                    count = 0\n",
    "                    for i, ch in enumerate(text):\n",
    "                        if ch == self._split_character:\n",
    "                            count += 1\n",
    "                            if count >= stride:\n",
    "                                break\n",
    "                    text = text[i + 1:]\n",
    "\n",
    "    def _get_stride(self) -> int:\n",
    "        if isinstance(self._stride, int):\n",
    "            return self._stride\n",
    "        else:\n",
    "            return self._rng.randint(self._stride[0], self._stride[1])\n",
    "\n",
    "ds = TextStrideIterableDataset(text_dataset, stride=1, split_character=\" \")\n",
    "for text in ds.limit(20):\n",
    "    print(repr(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d27871-f579-49d8-8176-bccffd98b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class TokenizeDataset(BaseIterableDataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            texts: Sequence[str],\n",
    "            tokenizer,\n",
    "            max_seq_length: int,\n",
    "            min_seq_length: int = None,\n",
    "            batch_size: int = None,\n",
    "            method: str = \"concat\",  # \"truncate\", \"fragments\", \"concat\", \"concatstride\"\n",
    "            stride: Union[int, Tuple[int, int]] = 1,\n",
    "            return_types: str = \"X,Y,lossmatrix\",  # \"X,Y,lossmatrix\", \"X[:-1],X[-1:]\"\n",
    "    ):\n",
    "        self._texts = texts\n",
    "        self._tokenizer = tokenizer\n",
    "        self._min_seq_length = min_seq_length\n",
    "        self._max_seq_length = max_seq_length\n",
    "        self._batch_size = batch_size\n",
    "        self._method = method\n",
    "        self._stride = stride\n",
    "        self._return_types = return_types\n",
    "\n",
    "    #    def __len__(self):\n",
    "    #        return len(self._texts)\n",
    "\n",
    "    def __iter__(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        #from tqdm import tqdm\n",
    "        if self._method == \"truncate\":\n",
    "            iterable = self._iter_truncated()\n",
    "        elif self._method == \"fragments\":\n",
    "            iterable = self._iter_fragments()\n",
    "        elif self._method == \"concat\":\n",
    "            iterable = self._iter_concat()\n",
    "        elif self._method == \"concatstride\":\n",
    "            iterable = self._iter_concat_stride()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method `{self._method}`\")\n",
    "\n",
    "        for token_ids in iterable:\n",
    "            if token_ids.shape[0]:\n",
    "                loss_mask = (token_ids != self._tokenizer.pad_token_id)\n",
    "\n",
    "                if self._return_types == \"X,Y,lossmatrix\":\n",
    "                    X = token_ids[:-1]\n",
    "                    Y = token_ids[1:]\n",
    "                    loss_mask = loss_mask[1:]\n",
    "                    yield X, Y, loss_mask\n",
    "                elif self._return_types == \"X[:-1],X[-1:]\":\n",
    "                    yield token_ids[:-1], token_ids[-1:]\n",
    "\n",
    "    def _iter_truncated(self):\n",
    "        seq_length = self._max_seq_length\n",
    "        for i, text in enumerate(self._texts):\n",
    "\n",
    "            if self._min_seq_length:\n",
    "                assert self._batch_size, \"Must define `batch_size` when defining `min_seq_length`\"\n",
    "                if i % self._batch_size == 0:\n",
    "                    seq_length = random.randint(self._min_seq_length, self._max_seq_length)\n",
    "\n",
    "            encoding = self._tokenizer(\n",
    "                text,\n",
    "                max_length=seq_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            yield encoding.input_ids.squeeze()\n",
    "\n",
    "    def _iter_fragments(self):\n",
    "        seq_length = self._max_seq_length\n",
    "        count = 0\n",
    "        for text in self._texts:\n",
    "            encoding = self._tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = encoding.input_ids.squeeze()\n",
    "            if input_ids.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            while True:\n",
    "                if self._min_seq_length is not None:\n",
    "                    assert self._batch_size, \"Must define `batch_size` when defining `min_seq_length`\"\n",
    "                    if count % self._batch_size == 0:\n",
    "                        seq_length = random.randint(self._min_seq_length, self._max_seq_length)\n",
    "                count += 1\n",
    "\n",
    "                if input_ids.shape[0] == seq_length:\n",
    "                    yield input_ids\n",
    "                    break\n",
    "\n",
    "                elif input_ids.shape[0] < seq_length:\n",
    "                    yield torch.cat([\n",
    "                        torch.ones((seq_length - input_ids.shape[0], ), dtype=input_ids.dtype) * self._tokenizer.pad_token_id,\n",
    "                        input_ids\n",
    "                    ])\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    yield input_ids[:seq_length]\n",
    "                    input_ids = input_ids[seq_length // 2:]\n",
    "\n",
    "    def _iter_concat(self):\n",
    "        seq_length = self._max_seq_length\n",
    "        count = 0\n",
    "        current_ids = None\n",
    "        for text in self._texts:\n",
    "            encoding = self._tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = encoding.input_ids.squeeze()\n",
    "            if input_ids.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            while True:\n",
    "                if self._min_seq_length is not None:\n",
    "                    assert self._batch_size, \"Must define `batch_size` when defining `min_seq_length`\"\n",
    "                    if count % self._batch_size == 0:\n",
    "                        seq_length = random.randint(self._min_seq_length, self._max_seq_length)\n",
    "\n",
    "                if input_ids.shape[0] == seq_length:\n",
    "                    yield input_ids\n",
    "                    count += 1\n",
    "                    break\n",
    "\n",
    "                elif input_ids.shape[0] < seq_length:\n",
    "                    if current_ids is None:\n",
    "                        current_ids = input_ids\n",
    "                    else:\n",
    "                        current_ids = torch.cat([\n",
    "                            current_ids,\n",
    "                            torch.ones((1, ), dtype=input_ids.dtype) * self._tokenizer.sep_token_id,\n",
    "                            input_ids\n",
    "                        ])\n",
    "\n",
    "                    if current_ids.shape[0] >= seq_length:\n",
    "                        yield current_ids[:seq_length]\n",
    "                        count += 1\n",
    "                        current_ids = current_ids[seq_length:]\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    yield input_ids[:seq_length]\n",
    "                    count += 1\n",
    "                    input_ids = input_ids[seq_length // 2:]\n",
    "\n",
    "    def _iter_concat_stride(self):\n",
    "        seq_length = self._max_seq_length\n",
    "        count = 0\n",
    "        current_ids = None\n",
    "        for text in self._texts:\n",
    "            encoding = self._tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            token_ids = encoding.input_ids.squeeze()\n",
    "            if token_ids.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            if current_ids is None:\n",
    "                current_ids = token_ids\n",
    "            else:\n",
    "                current_ids = torch.cat([\n",
    "                    current_ids,\n",
    "                    torch.ones((1, ), dtype=token_ids.dtype) * self._tokenizer.sep_token_id,\n",
    "                    token_ids\n",
    "                ])\n",
    "\n",
    "            while current_ids.shape[0] >= seq_length:\n",
    "                yield current_ids[:seq_length]\n",
    "                count += 1\n",
    "                if self._min_seq_length is not None:\n",
    "                    assert self._batch_size, \"Must define `batch_size` when defining `min_seq_length`\"\n",
    "                    if count % self._batch_size == 0:\n",
    "                        seq_length = random.randint(self._min_seq_length, self._max_seq_length)\n",
    "\n",
    "                stride = self._get_stride()\n",
    "                current_ids = current_ids[stride:]\n",
    "        \n",
    "    def _get_stride(self) -> int:\n",
    "        if isinstance(self._stride, int):\n",
    "            return self._stride\n",
    "        else:\n",
    "            return random.randint(self._stride[0], self._stride[1])\n",
    "\n",
    "ds = TokenizeDataset(text_dataset, tokenizer, 20, method=\"concatstride\", return_types=\"X[:-1],X[-1:]\", stride=[1, 10])\n",
    "for X, Y in ds.limit(20):\n",
    "    print(\"\".join(tokenizer.convert_ids_to_tokens(X)).replace(\"⬇\", \" \").replace(\"⬅\", \"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f89436-bf18-41f7-b4b3-3ed050f72e97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
