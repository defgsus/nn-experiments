{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4e63b-3889-45f0-aa9f-0583a452e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c9be1-aa81-4ccb-a0ff-aab692e68081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import ipywidgets\n",
    "from src.clipig.clipig_task import ClipigTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4396f0a0-bee3-484f-b50c-bed6108a98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Path(\"../src/clipig/presets/fractal-224.yaml\").read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba123e3-7fa6-4651-86d4-f7ab417ff8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_1 = \"\"\"\n",
    "clip_model_name: ViT-B/32\n",
    "device: auto\n",
    "initialize: random\n",
    "num_iterations: 10000\n",
    "source_model:\n",
    "  name: pixels\n",
    "  params:\n",
    "    channels: RGB\n",
    "    size:\n",
    "    - 224\n",
    "    - 224\n",
    "targets:\n",
    "- batch_size: 5\n",
    "  optimizer:\n",
    "    betas:\n",
    "    - 0.9\n",
    "    - 0.999\n",
    "    learnrate: 0.02\n",
    "    optimizer: Adam\n",
    "    weight_decay: 1.0e-06\n",
    "  target_features:\n",
    "  - text: cthulhu's cave in the city of r'lyeh\n",
    "    weight: 1.0\n",
    "  #- text: fractal patterns\n",
    "  #  weight: 0.2\n",
    "  - text: words, letters\n",
    "    weight: -1.\n",
    "  transformations:\n",
    "  - name: repeat\n",
    "    params:\n",
    "      active: true\n",
    "      repeat_xy:\n",
    "      - 2\n",
    "      - 2\n",
    "  - name: random_affine\n",
    "    params:\n",
    "      active: true\n",
    "      degrees_min_max:\n",
    "      - -10.6\n",
    "      - 10.0\n",
    "      interpolation: bilinear\n",
    "      scale_min_max:\n",
    "      - 0.9\n",
    "      - 1.1\n",
    "      shear_min_max:\n",
    "      - -15.0\n",
    "      - 15.0\n",
    "      translate_xy:\n",
    "      - 0.01\n",
    "      - 0.01\n",
    "  - name: random_crop\n",
    "    params:\n",
    "      active: true\n",
    "      size: 224\n",
    "  - name: multiplication\n",
    "    params:\n",
    "      active: false\n",
    "      add: 0.3\n",
    "      multiply: 0.5\n",
    "  - name: blur\n",
    "    params:\n",
    "      active: false\n",
    "      kernel_size:\n",
    "      - 3\n",
    "      - 3\n",
    "      mix: 0.7\n",
    "      sigma:\n",
    "      - 1.0\n",
    "      - 1.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fc1354-2208-4242-aab0-b26cbb9a5101",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_2 = \"\"\"\n",
    "clip_model_name: ViT-B/32\n",
    "device: auto\n",
    "initialize: random\n",
    "num_iterations: 10000\n",
    "source_model:\n",
    "  name: pixels\n",
    "  params:\n",
    "    channels: RGB\n",
    "    size:\n",
    "    - 224\n",
    "    - 224\n",
    "targets:\n",
    "- batch_size: 5\n",
    "  optimizer:\n",
    "    betas:\n",
    "    - 0.9\n",
    "    - 0.999\n",
    "    learnrate: 0.02\n",
    "    optimizer: RAdam\n",
    "    weight_decay: 1.0e-06\n",
    "  target_features:\n",
    "  - text: unicorns flying through the clouds\n",
    "    weight: 1.0\n",
    "  #- text: fractal patterns\n",
    "  #  weight: 0.2\n",
    "  - text: words, letters\n",
    "    weight: -1.\n",
    "  transformations:\n",
    "  - name: repeat\n",
    "    params:\n",
    "      active: true\n",
    "      repeat_xy:\n",
    "      - 2\n",
    "      - 2\n",
    "  - name: random_affine\n",
    "    params:\n",
    "      active: true\n",
    "      degrees_min_max:\n",
    "      - -10.6\n",
    "      - 10.0\n",
    "      interpolation: bilinear\n",
    "      scale_min_max:\n",
    "      - 0.9\n",
    "      - 1.1\n",
    "      shear_min_max:\n",
    "      - -15.0\n",
    "      - 15.0\n",
    "      translate_xy:\n",
    "      - 0.01\n",
    "      - 0.01\n",
    "  - name: random_crop\n",
    "    params:\n",
    "      active: true\n",
    "      size: 224\n",
    "  - name: multiplication\n",
    "    params:\n",
    "      active: true\n",
    "      add: 0.\n",
    "      multiply: 0.2\n",
    "  - name: blur\n",
    "    params:\n",
    "      active: false\n",
    "      kernel_size:\n",
    "      - 3\n",
    "      - 3\n",
    "      mix: 0.7\n",
    "      sigma:\n",
    "      - 1.0\n",
    "      - 1.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1075ce11-ce26-4efd-8adb-6bdbf6e71d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1920/1080\n",
    "225 * 16/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879c4412-27ae-4dfb-80a1-0051bc95ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_3 = \"\"\"\n",
    "clip_model_name: ViT-B/32\n",
    "device: auto\n",
    "initialize: random\n",
    "num_iterations: 10000\n",
    "source_model:\n",
    "  name: pixels\n",
    "  params:\n",
    "    channels: RGB\n",
    "    size:\n",
    "    - 400\n",
    "    - 225\n",
    "targets:\n",
    "- batch_size: 1\n",
    "  optimizer:\n",
    "    betas:\n",
    "    - 0.9\n",
    "    - 0.999\n",
    "    learnrate: 0.02\n",
    "    optimizer: Adam\n",
    "    weight_decay: 1.0e-06\n",
    "  target_features:\n",
    "  - image: ''\n",
    "    #text: norwegian landscape, huge flowers in the foreground\n",
    "    text: expressions of anger\n",
    "    type: text\n",
    "    weight: 1.0\n",
    "  - image: ''\n",
    "    text: fires and explosions  \n",
    "    type: text\n",
    "    weight: 0.5\n",
    "  - image: ''\n",
    "    text: words, letters\n",
    "    type: text\n",
    "    weight: -0.5\n",
    "  - image: ''\n",
    "    text: people \n",
    "    type: text\n",
    "    weight: -0.5\n",
    "  - image: ''\n",
    "    text: repetitive \n",
    "    type: text\n",
    "    weight: -0.5\n",
    "  transformations:\n",
    "  - name: padding\n",
    "    params:\n",
    "      active: true\n",
    "      pad_left: 100\n",
    "      pad_right: 100\n",
    "      pad_top: 50\n",
    "      pad_bottom: 150\n",
    "      padding_mode: symmetric\n",
    "  - name: random_affine\n",
    "    params:\n",
    "      active: true\n",
    "      degrees_min_max:\n",
    "      - -5.6\n",
    "      - 5.0\n",
    "      interpolation: bilinear\n",
    "      scale_min_max:\n",
    "      - 0.9\n",
    "      - 1.1\n",
    "      shear_min_max:\n",
    "      - -15.0\n",
    "      - 15.0\n",
    "      translate_xy:\n",
    "      - 0.01\n",
    "      - 0.01\n",
    "  - name: random_crop\n",
    "    params:\n",
    "      active: true\n",
    "      pad_if_needed: true\n",
    "      padding_mode: constant\n",
    "      size: 224\n",
    "  - name: multiplication\n",
    "    params:\n",
    "      active: true\n",
    "      add: 0.5\n",
    "      multiply: 0.1\n",
    "  - name: blur\n",
    "    params:\n",
    "      active: true\n",
    "      kernel_size:\n",
    "      - 3\n",
    "      - 3\n",
    "      mix: 0.7\n",
    "      sigma:\n",
    "      - 1.0\n",
    "      - 1.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc5b07-0703-4f63-b407-5660b1af0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.clipig.transformations.value_trans import Denoising\n",
    "denoiser = Denoising(\n",
    "    #model=\"denoise-mid-64x64-150k\",\n",
    "    #model=\"degradient-mid-64x64-150k\",\n",
    "    #model=\"declip-1\",\n",
    "    model=\"denoise-heavy-2\",\n",
    "    mix=.5,\n",
    "    overlap=(7, 7),\n",
    ")\n",
    "denoiser.model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff4ad1-b4f4-4162-ba4a-19dc3308e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perspective_transform(\n",
    "        image: torch.Tensor, \n",
    "        top: float = 1.,\n",
    "        left: float = 1.,\n",
    "        bottom: float = 1.,\n",
    "        right: float = 1.,\n",
    "):\n",
    "    h, w = image.shape[-2:]\n",
    "    top = max(-w // 2 + 1, (top - 1.) * w / 2)\n",
    "    bottom = max(-w // 2 + 1, (bottom - 1.) * w / 2)\n",
    "    left = max(-h // 2 + 1, (left - 1.) * h / 2)\n",
    "    right = max(-h // 2 + 1, (right - 1.) * h / 2)\n",
    "    return VF.perspective(\n",
    "        image,\n",
    "        [[0, 0], [w, 0], [w, h], [0, h]],\n",
    "        [[-top, -left], [w + top, -right], [w + bottom, h + right], [-bottom, h + left]],\n",
    "        interpolation=VF.InterpolationMode.BILINEAR,\n",
    "        \n",
    "    )\n",
    "\n",
    "#VF.to_pil_image(perspective_transform(\n",
    "#    image,\n",
    "    #top=2,\n",
    "    #bottom=.5,\n",
    "#    left=1.5,\n",
    "#    right=1.1,\n",
    "#))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4c4d8d-a674-4a6b-86cd-3542e06c051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_config_video(\n",
    "        config: str,\n",
    "        length_seconds: float = 120,\n",
    "        fps: int = 30,\n",
    "        frame_stride: int = 10,\n",
    "        store_directory: Optional[Union[str, Path]] = None,\n",
    "        reset: bool = False,\n",
    "        dummy: bool = False,\n",
    "):\n",
    "    num_iterations = int(length_seconds * fps * frame_stride)\n",
    "    \n",
    "    fp = io.StringIO(config)\n",
    "    config = yaml.safe_load(fp)\n",
    "    config[\"num_iterations\"] = num_iterations\n",
    "    config[\"pixel_yield_delay_sec\"] = 0.\n",
    "    \n",
    "    image_widget = ImageWidget()\n",
    "    status_widget = ipywidgets.Text()\n",
    "    display(image_widget)\n",
    "    display(status_widget)\n",
    "\n",
    "    image_idx = -1\n",
    "    frame_idx = -1\n",
    "    second = 0\n",
    "\n",
    "    if store_directory:\n",
    "        store_directory = Path(store_directory)\n",
    "        if store_directory.exists():\n",
    "            if reset:\n",
    "                shutil.rmtree(store_directory)\n",
    "            else:\n",
    "                filenames = sorted(store_directory.glob(\"*.png\"))\n",
    "                if filenames:\n",
    "                    frame_idx = len(filenames) - 1\n",
    "                    image_idx = frame_idx * frame_stride\n",
    "                    config[\"initialize\"] = \"input\"\n",
    "                    config[\"input_image\"] = VF.to_tensor(PIL.Image.open(str(filenames[-1])))\n",
    "\n",
    "        os.makedirs(store_directory, exist_ok=True)\n",
    "\n",
    "    if dummy:\n",
    "        config[\"dummy_mode\"] = True\n",
    "    task = ClipigTask(config)    \n",
    "    status = \"requested\"\n",
    "    \n",
    "    try:\n",
    "        with tqdm(total=num_iterations) as progress:\n",
    "            for event in task.run():\n",
    "                if \"status\" in event:\n",
    "                    status = event[\"status\"]\n",
    "        \n",
    "                if \"pixels\" in event:\n",
    "                    image_idx += 1\n",
    "                    progress.update(1)\n",
    "                    if image_idx % frame_stride == 0:\n",
    "                        frame_idx += 1\n",
    "                        second = frame_idx / fps\n",
    "                        \n",
    "                        pixels = event[\"pixels\"].clamp(0, 1)\n",
    "                        with torch.no_grad():\n",
    "                            pixels_denoised = (denoiser(pixels) + 0.004).clamp(0, 1)\n",
    "\n",
    "                        pixels = pixels + .7 * (pixels_denoised - pixels)\n",
    "                    \n",
    "                        pixels_pil = VF.to_pil_image(pixels_denoised)\n",
    "                        if store_directory:\n",
    "                            pixels_pil.save(str(store_directory / f\"frame-{frame_idx:08}.png\"))\n",
    "                            \n",
    "                        image_widget.set_pil(resize(pixels_pil, 2))\n",
    "\n",
    "                        f = 0.01\n",
    "                        s = math.sin(second / 4.7)\n",
    "                        s2 = math.sin(second / 3.)\n",
    "                        pixels = perspective_transform(\n",
    "                            pixels,\n",
    "                            top=1 - f/5,\n",
    "                            bottom=1 + f * (2. + 8. * abs(s)),\n",
    "                            left=1 + f * s,\n",
    "                            right=1 + f * s2,\n",
    "                        )\n",
    "                        pixels = VF.affine(\n",
    "                            pixels, \n",
    "                            angle=-s / 4., \n",
    "                            translate=[0.0, 0.0],\n",
    "                            scale=1 + .5 * f * (.5 + s2 - s), \n",
    "                            shear=[-s2, s],\n",
    "                            #shear=[0.1*math.sin(second/1.3), 0.1*math.sin(second/1.7)],\n",
    "                            interpolation=VF.InterpolationMode.BILINEAR,\n",
    "                            #center=[pixels.shape[-1] * (.5 + .4*s), pixels.shape[-2] * .5],\n",
    "                        )\n",
    "                        \n",
    "                        task.source_model.set_image(pixels.clamp(0, 1))               \n",
    "    \n",
    "                status_widget.value = (\n",
    "                    f\"status: {status}\"\n",
    "                    f\", second={second:.2f}\"\n",
    "                    f\", image_idx={image_idx}, frame_idx={frame_idx}\"\n",
    "                    \n",
    "                )\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"stopped\")\n",
    "        pass\n",
    "\n",
    "run_config_video(\n",
    "    config_3,\n",
    "    store_directory=\"./clipig-frames/movement\",\n",
    "    #reset=True,\n",
    "    frame_stride=60,\n",
    "    #dummy=True, frame_stride=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f8604-7187-4844-b3de-7a34eae22807",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = VF.to_tensor(PIL.Image.open(\"/home/bergi/Pictures/bob/9872432.jpeg\"))\n",
    "VF.to_pil_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0486f87-77ed-4df9-8a27-9e3cf339dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 10\n",
    "h, w = image.shape[-2:]\n",
    "VF.to_pil_image(VF.perspective(\n",
    "    image,\n",
    "    [[0, 0], [w, 0], [w, h], [0, h]],\n",
    "    [[0, 0], [w, 0], [w+f, h], [0-f, h]],\n",
    "    #[[1, 0], [1, 1], [0, 1], [0, 0]],\n",
    "    #[[1.001, 0.001], [1, 1], [0, 1], [0, 0]],\n",
    "    #[[1, -0.1], [1, 1.1], [0, 1], [0, 0]],\n",
    "    interpolation=VF.InterpolationMode.BILINEAR,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed03b7d5-7f2f-4b8e-b6fa-0b4bce149907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.clipig.parameters import get_complete_clipig_task_config\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ecaa3b-9fc2-4b7a-8e3c-b7da25692a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipigVideoRenderer:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            config: Union[dict, str, Path],\n",
    "            fps: int = 30,\n",
    "            video_frame_stride: int = 1,\n",
    "            transformation_frame_stride: Optional[int] = None,\n",
    "            store_directory: Optional[Union[str, Path]] = None,\n",
    "            display_jupyter: bool = False,\n",
    "    ):\n",
    "        if not isinstance(config, dict):\n",
    "            fp = io.StringIO(config)\n",
    "            config = yaml.safe_load(fp)\n",
    "            \n",
    "        self.config = get_complete_clipig_task_config(config)\n",
    "        self.store_directory = Path(store_directory) if store_directory is not None else None\n",
    "        self.fps = fps\n",
    "        self.video_frame_stride = video_frame_stride\n",
    "        self.transformation_frame_stride = video_frame_stride if transformation_frame_stride is None else transformation_frame_stride\n",
    "        self.display_jupyter = display_jupyter\n",
    "\n",
    "        self.clipig_frame = 0\n",
    "        self.video_frame = 0\n",
    "        self.task: Optional[ClipigTask] = None\n",
    "\n",
    "        if self.display_jupyter:\n",
    "            from IPython.display import display\n",
    "            import ipywidgets\n",
    "            from src.util.widgets import ImageWidget\n",
    "            self._image_widget = ImageWidget()\n",
    "            self._status_widget = ipywidgets.Text()\n",
    "            display(self._image_widget)\n",
    "            display(self._status_widget)\n",
    "\n",
    "    @property\n",
    "    def second(self) -> float:\n",
    "        return self.clipig_frame / self.video_frame_stride / self.fps\n",
    "\n",
    "    def transform(self, pixels: torch.Tensor, delta: float) -> torch.Tensor:\n",
    "        return pixels\n",
    "\n",
    "    def post_process(self, pixels: torch.Tensor, delta: float) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        post-processing step before video frame writing.\n",
    "\n",
    "        If returning a tuple, the second argument is put back into clipig\n",
    "        \"\"\"\n",
    "        return pixels\n",
    "\n",
    "    def run(\n",
    "            self,\n",
    "            seconds: float,\n",
    "            reset: bool = False,\n",
    "    ):\n",
    "        num_iterations = seconds * self.video_frame_stride * self.fps\n",
    "\n",
    "        config = deepcopy(self.config)\n",
    "        config[\"num_iterations\"] = num_iterations\n",
    "        config[\"pixel_yield_delay_sec\"] = 0.\n",
    "\n",
    "        image_idx = 0\n",
    "        frame_idx = 0\n",
    "\n",
    "        if self.store_directory is not None:\n",
    "            if self.store_directory.exists():\n",
    "                if reset:\n",
    "                    shutil.rmtree(self.store_directory)\n",
    "                else:\n",
    "                    filenames = sorted(self.store_directory.glob(\"*.png\"))\n",
    "                    if filenames:\n",
    "                        frame_idx = len(filenames)\n",
    "                        image_idx = frame_idx * self.video_frame_stride\n",
    "                        config[\"initialize\"] = \"input\"\n",
    "                        config[\"input_image\"] = VF.to_tensor(PIL.Image.open(str(filenames[-1])))\n",
    "\n",
    "            os.makedirs(self.store_directory, exist_ok=True)\n",
    "\n",
    "        self.video_frame = frame_idx\n",
    "        self.clipig_frame = image_idx\n",
    "        self.task = ClipigTask(config)\n",
    "        status = \"requested\"\n",
    "\n",
    "        last_video_frame = self.clipig_frame\n",
    "        last_transformation_frame = self.clipig_frame\n",
    "        try:\n",
    "            with tqdm(total=num_iterations) as progress:\n",
    "                for event in self.task.run():\n",
    "                    if \"status\" in event:\n",
    "                        status = event[\"status\"]\n",
    "\n",
    "                    if \"pixels\" in event:\n",
    "                        progress.update(1)\n",
    "                        clipig_frame = self.clipig_frame + 1\n",
    "                        pixels = event[\"pixels\"].clamp(0, 1)\n",
    "\n",
    "                        if clipig_frame - last_transformation_frame >= self.transformation_frame_stride:\n",
    "                            delta = (clipig_frame - last_transformation_frame) / self.video_frame_stride / self.fps\n",
    "                            last_transformation_frame = clipig_frame\n",
    "\n",
    "                            with torch.no_grad():\n",
    "                                pixels = self.transform(pixels, delta).clamp(0, 1)\n",
    "                                self.task.source_model.set_image(pixels)\n",
    "\n",
    "                        if clipig_frame - last_video_frame >= self.video_frame_stride:\n",
    "                            delta = (clipig_frame - last_video_frame) / self.video_frame_stride / self.fps\n",
    "                            last_video_frame = clipig_frame\n",
    "                            with torch.no_grad():\n",
    "                                pixels = self.post_process(pixels, delta)\n",
    "                                if isinstance(pixels, (list, tuple)):\n",
    "                                    pixels, source_pixels = pixels\n",
    "                                    self.task.source_model.set_image(source_pixels.clamp(0, 1))\n",
    "                                pixels = pixels.clamp(0, 1)\n",
    "\n",
    "                            if self.store_directory is not None or self.display_jupyter:\n",
    "                                pixels_pil = VF.to_pil_image(pixels)\n",
    "                                if self.store_directory is not None:\n",
    "                                    pixels_pil.save(self.store_directory / f\"frame-{self.video_frame:08}.png\")\n",
    "\n",
    "                                if self.display_jupyter:\n",
    "                                    self._image_widget.set_pil(image_minimum_size(pixels_pil, width=500))\n",
    "\n",
    "                            self.video_frame += 1\n",
    "    \n",
    "                        self.clipig_frame += 1\n",
    "\n",
    "                    if self.display_jupyter:\n",
    "                        self._status_widget.value = (\n",
    "                            f\"status: {status}\"\n",
    "                            f\", second={self.second:.2f}\"\n",
    "                            f\", video_frame={self.video_frame}, clipg_frame={self.clipig_frame}\"\n",
    "\n",
    "                        )\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"stopped\")\n",
    "            pass\n",
    "\n",
    "\n",
    "class MyRenderer(ClipigVideoRenderer):\n",
    "\n",
    "    def post_process(self, pixels: torch.Tensor, delta: float):\n",
    "        #pixels = denoiser(pixels)\n",
    "        pixels = denoiser.model(pixels.unsqueeze(0))[0]\n",
    "        return pixels\n",
    "    \n",
    "    def transform(self, pixels: torch.Tensor, delta: float):\n",
    "        s = math.sin(self.second)\n",
    "        s2 = math.sin(self.second * 1.3)\n",
    "        pixels = perspective_transform(\n",
    "            pixels, \n",
    "            top=1. + (1-s2)*delta, \n",
    "            bottom=1. + (1+s2)*delta, \n",
    "            left=1. + (1+s)*delta,\n",
    "            right=1. + (1-s)*delta,\n",
    "        )\n",
    "        return pixels\n",
    "        \n",
    "\n",
    "renderer = MyRenderer(\n",
    "    config_3,\n",
    "    video_frame_stride=20,\n",
    "    transformation_frame_stride=1,\n",
    "    display_jupyter=True,\n",
    "    store_directory=\"./clipig-frames/denoise\",\n",
    ")\n",
    "renderer.run(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ce7e6-8ef8-4dc2-b1cb-ef46cb3f2540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dce210-5052-4f89-ad91-19bc5eab4856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52d1219-e1d7-4ec3-a897-a285bd2bc705",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_4 = \"\"\"\n",
    "clip_model_name: ViT-B/32\n",
    "device: auto\n",
    "initialize: random\n",
    "num_iterations: 10000\n",
    "source_model:\n",
    "  name: pixels\n",
    "  params:\n",
    "    channels: RGB\n",
    "    size:\n",
    "    - 400\n",
    "    - 225\n",
    "targets:\n",
    "- batch_size: 1\n",
    "  optimizer:\n",
    "    betas:\n",
    "    - 0.9\n",
    "    - 0.999\n",
    "    learnrate: 0.02\n",
    "    optimizer: RAdam\n",
    "    weight_decay: 1.0e-06\n",
    "  target_features:\n",
    "  - image: ''\n",
    "    #text: norwegian landscape, huge flowers in the foreground\n",
    "    text: desolated streets\n",
    "    type: text\n",
    "    weight: 1.0\n",
    "  - image: ''\n",
    "    text: fires and explosions  \n",
    "    type: text\n",
    "    weight: 0.0\n",
    "  - image: ''\n",
    "    text: words, letters\n",
    "    type: text\n",
    "    weight: -1.0\n",
    "  - image: ''\n",
    "    text: people \n",
    "    type: text\n",
    "    weight: -0.0\n",
    "  - image: ''\n",
    "    text: repetitive \n",
    "    type: text\n",
    "    weight: -0.0\n",
    "  transformations:\n",
    "  - name: padding\n",
    "    params:\n",
    "      active: true\n",
    "      pad_left: 100\n",
    "      pad_right: 100\n",
    "      pad_top: 100\n",
    "      pad_bottom: 100\n",
    "      padding_mode: symmetric\n",
    "  - name: random_affine\n",
    "    params:\n",
    "      active: true\n",
    "      degrees_min_max:\n",
    "      - -5.6\n",
    "      - 5.0\n",
    "      interpolation: bilinear\n",
    "      scale_min_max:\n",
    "      - 0.9\n",
    "      - 1.1\n",
    "      shear_min_max:\n",
    "      - -15.0\n",
    "      - 15.0\n",
    "      translate_xy:\n",
    "      - 0.01\n",
    "      - 0.01\n",
    "  - name: random_crop\n",
    "    params:\n",
    "      active: true\n",
    "      pad_if_needed: true\n",
    "      padding_mode: constant\n",
    "      size: 224\n",
    "  - name: multiplication\n",
    "    params:\n",
    "      active: true\n",
    "      add: 0.5\n",
    "      multiply: 0.1\n",
    "  - name: blur\n",
    "    params:\n",
    "      active: true\n",
    "      kernel_size:\n",
    "      - 3\n",
    "      - 3\n",
    "      mix: 0.7\n",
    "      sigma:\n",
    "      - 1.0\n",
    "      - 1.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6194283c-15ea-4074-a55d-6f88b8af74ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRenderer(ClipigVideoRenderer):\n",
    "\n",
    "    def post_process(self, pixels: torch.Tensor, delta: float):\n",
    "        #pixels = denoiser(pixels)\n",
    "        pixels = denoiser.model(pixels.unsqueeze(0))[0]\n",
    "        return pixels  , pixels\n",
    "    \n",
    "    def transform(self, pixels: torch.Tensor, delta: float):\n",
    "        #pixels = VF.adjust_hue(pixels, delta * 10.)\n",
    "        #pixels = perspective_transform(pixels, top=1., bottom=1.+delta/10.)\n",
    "        if 0:\n",
    "            pixels = VF.affine(\n",
    "                pixels, \n",
    "                angle=0., \n",
    "                translate=[0, 0], \n",
    "                scale=1,#+delta/6., \n",
    "                shear=[0, 0],\n",
    "            )\n",
    "        #space = Space2d([2, *pixels.shape[-2:]]).space()\n",
    "        #space[0] = -delta * torch.sin((space[0]+self.second) * 3.1415/2.)\n",
    "        #space[1] = -delta * torch.cos(space[1] * 3.1415/2.)                                      \n",
    "        #space = space.permute(1,2, 0).unsqueeze(0)\n",
    "        if not hasattr(self, \"_space\"):\n",
    "            space = numpy_perlin_noise_2d((400*2, 400), (10, 5))\n",
    "            space = torch.Tensor(space).cuda().view(2, 400, 400)[:, :225, :].permute(1, 2, 0).unsqueeze(0)\n",
    "            self._space = space\n",
    "        pixels += .6 * (VF.elastic_transform(pixels, delta/5. * self._space) - pixels)\n",
    "        #pixels = pixels[:, 1:-1, 1:-1]\n",
    "        #pixels = F.pad(pixels, [1, 1, 1, 1], mode=\"circular\")\n",
    "        return pixels        \n",
    "\n",
    "renderer = MyRenderer(\n",
    "    config_4,\n",
    "    video_frame_stride=50,\n",
    "    transformation_frame_stride=10,\n",
    "    display_jupyter=True,\n",
    "    store_directory=\"./clipig-frames/subgenius\",\n",
    ")\n",
    "renderer.run(\n",
    "    100,\n",
    "    #reset=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1bb7d5-8c58-4808-9fbc-fd97590f9c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73688f5f-1851-4e53-a735-e487f58666cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052cba1-9260-4ed9-bbc6-b7068e593b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(1, 5, 3)\n",
    "conv.weight.shape\n",
    "conv_weights = torch.randn(4, *conv.weight.shape)\n",
    "bs = 2\n",
    "feature = torch.Tensor(bs, 4)\n",
    "conv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e238878-01a8-4c69-9e8e-4b7e3a042332",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_w = conv_weights.unsqueeze(0).expand(bs, -1, -1, -1, -1, -1) * feature[:, :, None, None, None, None]\n",
    "cur_w = cur_w.sum(1)\n",
    "cur_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c8e55e-5271-4f8e-a4cb-271ed449f0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.conv2d(torch.zeros(bs, 1, 10, 10), cur_w)#, stride=[1, 1, 1, 1], padding=[0]*4, dilation=[1]*4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080d2189-b847-438e-b04e-ab5ca2540037",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.conv2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1670eb0-921b-4d85-88c3-3d1f97e6393b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74932087-362d-4d67-83fe-f54a279c076b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
