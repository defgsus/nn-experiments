{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4e63b-3889-45f0-aa9f-0583a452e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import *\n",
    "from src.util import *\n",
    "from src.algo import *\n",
    "from src.models.decoder import *\n",
    "from src.models.transform import *\n",
    "from src.models.util import *\n",
    "from experiments import datasets\n",
    "from experiments.denoise.resconv import ResConv\n",
    "\n",
    "def resize(img, scale: float, mode: VF.InterpolationMode = VF.InterpolationMode.NEAREST):\n",
    "    if isinstance(img, PIL.Image.Image):\n",
    "        shape = (img.height, img.width)\n",
    "    else:\n",
    "        shape = img.shape[-2:]\n",
    "    return VF.resize(img, [max(1, int(s * scale)) for s in shape], mode, antialias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c9be1-aa81-4ccb-a0ff-aab692e68081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import ipywidgets\n",
    "from src.clipig.clipig_task import ClipigTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d0c42e-b321-4bc7-8e60-9d69f2f228cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageWidget(ipywidgets.Image):\n",
    "\n",
    "    def set_pil(self, image: PIL.Image.Image):\n",
    "        fp = io.BytesIO()\n",
    "        image.save(fp, \"png\")\n",
    "        fp.seek(0)\n",
    "        self.format = \"png\"\n",
    "        self.value = fp.read()\n",
    "\n",
    "    def set_tensor(self, image: torch.Tensor):\n",
    "        image = VF.to_pil_image(image)\n",
    "        self.set_pil(image)\n",
    "        \n",
    "image_widget = ImageWidget()\n",
    "display(image_widget)\n",
    "\n",
    "image_widget.set_pil(PIL.Image.open(\"/home/bergi/Pictures/bob/bob-trans-back.png\"))\n",
    "#image.value = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4396f0a0-bee3-484f-b50c-bed6108a98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Path(\"../src/clipig/presets/fractal-224.yaml\").read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba123e3-7fa6-4651-86d4-f7ab417ff8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_1 = \"\"\"\n",
    "clip_model_name: ViT-B/32\n",
    "device: auto\n",
    "initialize: random\n",
    "num_iterations: 10000\n",
    "source_model:\n",
    "  name: pixels\n",
    "  params:\n",
    "    channels: RGB\n",
    "    size:\n",
    "    - 224\n",
    "    - 224\n",
    "targets:\n",
    "- batch_size: 5\n",
    "  optimizer:\n",
    "    betas:\n",
    "    - 0.9\n",
    "    - 0.999\n",
    "    learnrate: 0.02\n",
    "    optimizer: Adam\n",
    "    weight_decay: 1.0e-06\n",
    "  target_features:\n",
    "  - text: cthulhu's cave in the city of r'lyeh\n",
    "    weight: 1.0\n",
    "  #- text: fractal patterns\n",
    "  #  weight: 0.2\n",
    "  - text: words, letters\n",
    "    weight: -1.\n",
    "  transformations:\n",
    "  - name: repeat\n",
    "    params:\n",
    "      active: true\n",
    "      repeat_xy:\n",
    "      - 2\n",
    "      - 2\n",
    "  - name: random_affine\n",
    "    params:\n",
    "      active: true\n",
    "      degrees_min_max:\n",
    "      - -10.6\n",
    "      - 10.0\n",
    "      interpolation: bilinear\n",
    "      scale_min_max:\n",
    "      - 0.9\n",
    "      - 1.1\n",
    "      shear_min_max:\n",
    "      - -15.0\n",
    "      - 15.0\n",
    "      translate_xy:\n",
    "      - 0.01\n",
    "      - 0.01\n",
    "  - name: random_crop\n",
    "    params:\n",
    "      active: true\n",
    "      size: 224\n",
    "  - name: multiplication\n",
    "    params:\n",
    "      active: false\n",
    "      add: 0.3\n",
    "      multiply: 0.5\n",
    "  - name: blur\n",
    "    params:\n",
    "      active: false\n",
    "      kernel_size:\n",
    "      - 3\n",
    "      - 3\n",
    "      mix: 0.7\n",
    "      sigma:\n",
    "      - 1.0\n",
    "      - 1.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fc1354-2208-4242-aab0-b26cbb9a5101",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_2 = \"\"\"\n",
    "clip_model_name: ViT-B/32\n",
    "device: auto\n",
    "initialize: random\n",
    "num_iterations: 10000\n",
    "source_model:\n",
    "  name: pixels\n",
    "  params:\n",
    "    channels: RGB\n",
    "    size:\n",
    "    - 224\n",
    "    - 224\n",
    "targets:\n",
    "- batch_size: 5\n",
    "  optimizer:\n",
    "    betas:\n",
    "    - 0.9\n",
    "    - 0.999\n",
    "    learnrate: 0.02\n",
    "    optimizer: RAdam\n",
    "    weight_decay: 1.0e-06\n",
    "  target_features:\n",
    "  - text: unicorns flying through the clouds\n",
    "    weight: 1.0\n",
    "  #- text: fractal patterns\n",
    "  #  weight: 0.2\n",
    "  - text: words, letters\n",
    "    weight: -1.\n",
    "  transformations:\n",
    "  - name: repeat\n",
    "    params:\n",
    "      active: true\n",
    "      repeat_xy:\n",
    "      - 2\n",
    "      - 2\n",
    "  - name: random_affine\n",
    "    params:\n",
    "      active: true\n",
    "      degrees_min_max:\n",
    "      - -10.6\n",
    "      - 10.0\n",
    "      interpolation: bilinear\n",
    "      scale_min_max:\n",
    "      - 0.9\n",
    "      - 1.1\n",
    "      shear_min_max:\n",
    "      - -15.0\n",
    "      - 15.0\n",
    "      translate_xy:\n",
    "      - 0.01\n",
    "      - 0.01\n",
    "  - name: random_crop\n",
    "    params:\n",
    "      active: true\n",
    "      size: 224\n",
    "  - name: multiplication\n",
    "    params:\n",
    "      active: true\n",
    "      add: 0.\n",
    "      multiply: 0.2\n",
    "  - name: blur\n",
    "    params:\n",
    "      active: false\n",
    "      kernel_size:\n",
    "      - 3\n",
    "      - 3\n",
    "      mix: 0.7\n",
    "      sigma:\n",
    "      - 1.0\n",
    "      - 1.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879c4412-27ae-4dfb-80a1-0051bc95ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_3 = \"\"\"\n",
    "clip_model_name: ViT-B/32\n",
    "device: auto\n",
    "initialize: random\n",
    "num_iterations: 10000\n",
    "source_model:\n",
    "  name: pixels\n",
    "  params:\n",
    "    channels: RGB\n",
    "    size:\n",
    "    - 224\n",
    "    - 224\n",
    "targets:\n",
    "- batch_size: 1\n",
    "  optimizer:\n",
    "    betas:\n",
    "    - 0.9\n",
    "    - 0.999\n",
    "    learnrate: 0.02\n",
    "    optimizer: Adam\n",
    "    weight_decay: 1.0e-06\n",
    "  target_features:\n",
    "  - image: ''\n",
    "    text: fisheye view of a cthulhu fractal\n",
    "    type: text\n",
    "    weight: 1.0\n",
    "  - image: ''\n",
    "    text: words, letters\n",
    "    type: text\n",
    "    weight: -1.0\n",
    "  transformations:\n",
    "  - name: padding\n",
    "    params:\n",
    "      active: true\n",
    "      pad_left: 100\n",
    "      pad_right: 100\n",
    "      pad_top: 50\n",
    "      pad_bottom: 150\n",
    "      padding_mode: symmetric\n",
    "  - name: random_affine\n",
    "    params:\n",
    "      active: true\n",
    "      degrees_min_max:\n",
    "      - -5.6\n",
    "      - 5.0\n",
    "      interpolation: bilinear\n",
    "      scale_min_max:\n",
    "      - 0.9\n",
    "      - 1.1\n",
    "      shear_min_max:\n",
    "      - -15.0\n",
    "      - 15.0\n",
    "      translate_xy:\n",
    "      - 0.01\n",
    "      - 0.01\n",
    "  - name: random_crop\n",
    "    params:\n",
    "      active: true\n",
    "      pad_if_needed: true\n",
    "      padding_mode: constant\n",
    "      size: 224\n",
    "  - name: multiplication\n",
    "    params:\n",
    "      active: true\n",
    "      add: 0.5\n",
    "      multiply: 0.1\n",
    "  - name: blur\n",
    "    params:\n",
    "      active: true\n",
    "      kernel_size:\n",
    "      - 3\n",
    "      - 3\n",
    "      mix: 0.7\n",
    "      sigma:\n",
    "      - 1.0\n",
    "      - 1.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc5b07-0703-4f63-b407-5660b1af0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.clipig.transformations.value_trans import Denoising\n",
    "denoiser = Denoising(\n",
    "    model=\"denoise-mid-64x64-150k\",\n",
    "    mix=.5,\n",
    "    overlap=(7, 7),\n",
    ")\n",
    "denoiser.model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff4ad1-b4f4-4162-ba4a-19dc3308e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perspective_transform(\n",
    "        image: torch.Tensor, \n",
    "        top: float = 1.,\n",
    "        left: float = 1.,\n",
    "        bottom: float = 1.,\n",
    "        right: float = 1.,\n",
    "):\n",
    "    h, w = image.shape[-2:]\n",
    "    top = max(-w // 2 + 1, (top - 1.) * w / 2)\n",
    "    bottom = max(-w // 2 + 1, (bottom - 1.) * w / 2)\n",
    "    left = max(-h // 2 + 1, (left - 1.) * h / 2)\n",
    "    right = max(-h // 2 + 1, (right - 1.) * h / 2)\n",
    "    return VF.perspective(\n",
    "        image,\n",
    "        [[0, 0], [w, 0], [w, h], [0, h]],\n",
    "        [[-top, -left], [w + top, -right], [w + bottom, h + right], [-bottom, h + left]],\n",
    "        interpolation=VF.InterpolationMode.BILINEAR,\n",
    "        \n",
    "    )\n",
    "\n",
    "#VF.to_pil_image(perspective_transform(\n",
    "#    image,\n",
    "    #top=2,\n",
    "    #bottom=.5,\n",
    "#    left=1.5,\n",
    "#    right=1.1,\n",
    "#))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4c4d8d-a674-4a6b-86cd-3542e06c051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_config(\n",
    "        config: str,\n",
    "        length_seconds: float = 60,\n",
    "        fps: int = 30,\n",
    "        frame_stride: int = 10,\n",
    "        store_directory: Optional[Union[str, Path]] = None,\n",
    "        reset: bool = False,\n",
    "        dummy: bool = False,\n",
    "):\n",
    "    num_iterations = int(length_seconds * fps * frame_stride)\n",
    "    \n",
    "    fp = io.StringIO(config)\n",
    "    config = yaml.safe_load(fp)\n",
    "    config[\"num_iterations\"] = num_iterations\n",
    "    config[\"pixel_yield_delay_sec\"] = 0.\n",
    "    \n",
    "    image_widget = ImageWidget()\n",
    "    status_widget = ipywidgets.Text()\n",
    "    display(image_widget)\n",
    "    display(status_widget)\n",
    "\n",
    "    image_idx = -1\n",
    "    frame_idx = -1\n",
    "    second = 0\n",
    "\n",
    "    if store_directory:\n",
    "        store_directory = Path(store_directory)\n",
    "        if store_directory.exists():\n",
    "            if reset:\n",
    "                shutil.rmtree(store_directory)\n",
    "            else:\n",
    "                filenames = sorted(store_directory.glob(\"*.png\"))\n",
    "                if filenames:\n",
    "                    frame_idx = len(filenames) - 1\n",
    "                    image_idx = frame_idx * frame_stride\n",
    "                    config[\"initialize\"] = \"input\"\n",
    "                    config[\"input_image\"] = VF.to_tensor(PIL.Image.open(str(filenames[-1])))\n",
    "\n",
    "        os.makedirs(store_directory, exist_ok=True)\n",
    "\n",
    "    if dummy:\n",
    "        config[\"dummy_mode\"] = True\n",
    "    task = ClipigTask(config)    \n",
    "    status = \"requested\"\n",
    "    \n",
    "    try:\n",
    "        with tqdm(total=num_iterations) as progress:\n",
    "            for event in task.run():\n",
    "                if \"status\" in event:\n",
    "                    status = event[\"status\"]\n",
    "        \n",
    "                if \"pixels\" in event:\n",
    "                    image_idx += 1\n",
    "                    progress.update(1)\n",
    "                    if image_idx % frame_stride == 0:\n",
    "                        frame_idx += 1\n",
    "                        second = frame_idx / fps\n",
    "                        \n",
    "                        pixels = event[\"pixels\"].clamp(0, 1)\n",
    "                        with torch.no_grad():\n",
    "                            pixels_denoised = (denoiser(pixels) + 0.004).clamp(0, 1)\n",
    "                    \n",
    "                        pixels_pil = VF.to_pil_image(pixels_denoised)\n",
    "                        if store_directory:\n",
    "                            pixels_pil.save(str(store_directory / f\"frame-{frame_idx:08}.png\"))\n",
    "                            \n",
    "                        image_widget.set_pil(resize(pixels_pil, 2))\n",
    "\n",
    "                        f = 0.005\n",
    "                        s = math.sin(second / 3.)\n",
    "                        pixels = perspective_transform(\n",
    "                            pixels,\n",
    "                            top=1 - f/5,\n",
    "                            bottom=1 + f,\n",
    "                        )\n",
    "                        pixels = VF.affine(\n",
    "                            pixels, \n",
    "                            angle=-s / 5, \n",
    "                            translate=[0., 0.],\n",
    "                            scale=1 + f/4, \n",
    "                            shear=[0., 0.],\n",
    "                            #shear=[0.1*math.sin(second/1.3), 0.1*math.sin(second/1.7)],\n",
    "                            interpolation=VF.InterpolationMode.BILINEAR,\n",
    "                            center=[pixels.shape[-1] * (.5 + .4*s), pixels.shape[-2] * .5],\n",
    "                        )\n",
    "                        \n",
    "                        task.source_model.set_image(pixels.clamp(0, 1))               \n",
    "    \n",
    "                status_widget.value = (\n",
    "                    f\"status: {status}\"\n",
    "                    f\", second={second:.2f}\"\n",
    "                    f\", image_idx={image_idx}, frame_idx={frame_idx}\"\n",
    "                    \n",
    "                )\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"stopped\")\n",
    "        pass\n",
    "\n",
    "run_config(\n",
    "    config_3,\n",
    "    store_directory=\"./clipig-frames/cthulhu-fractal\",\n",
    "    reset=True,\n",
    "    frame_stride=30,\n",
    "    #dummy=True, frame_stride=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f8604-7187-4844-b3de-7a34eae22807",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = VF.to_tensor(PIL.Image.open(\"/home/bergi/Pictures/bob/9872432.jpeg\"))\n",
    "VF.to_pil_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0486f87-77ed-4df9-8a27-9e3cf339dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 10\n",
    "h, w = image.shape[-2:]\n",
    "VF.to_pil_image(VF.perspective(\n",
    "    image,\n",
    "    [[0, 0], [w, 0], [w, h], [0, h]],\n",
    "    [[0, 0], [w, 0], [w+f, h], [0-f, h]],\n",
    "    #[[1, 0], [1, 1], [0, 1], [0, 0]],\n",
    "    #[[1.001, 0.001], [1, 1], [0, 1], [0, 0]],\n",
    "    #[[1, -0.1], [1, 1.1], [0, 1], [0, 0]],\n",
    "    interpolation=VF.InterpolationMode.BILINEAR,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed03b7d5-7f2f-4b8e-b6fa-0b4bce149907",
   "metadata": {},
   "outputs": [],
   "source": [
    "VF.perspective?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ecaa3b-9fc2-4b7a-8e3c-b7da25692a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
