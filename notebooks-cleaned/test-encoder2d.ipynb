{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339552d7-378f-495e-b241-933909db31db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union, Dict\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset, RandomSampler\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import *\n",
    "from src.util import to_torch_device\n",
    "from src.models.cnn import *\n",
    "from src.models.encoder import *\n",
    "from scripts import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15c6aa-f71e-48ba-9971-27e74d9db5ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls ../models/encoder2d/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b6d62-e357-4b11-b950-820afdeb775a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def stack_rbms(*filenames, shape=None):\n",
    "    layers = []\n",
    "    for fn in filenames:\n",
    "        data = torch.load(fn)\n",
    "        if \"state_dict\" in data:\n",
    "            data = data[\"state_dict\"]\n",
    "        \n",
    "        if \"_extra_state\" in data:\n",
    "            if \"shape\" in data[\"_extra_state\"] and shape is None:\n",
    "                shape = data[\"_extra_state\"][\"shape\"]\n",
    "                \n",
    "        #print(data.keys())\n",
    "        if \"bias_visible\" in data:\n",
    "            layers.append({\n",
    "                \"bias_visible\": data[\"bias_visible\"], \n",
    "                \"bias_hidden\": data[\"bias_hidden\"], \n",
    "                \"weight\": data[\"weight\"],\n",
    "            })\n",
    "        elif \"rbms.0.bias_visible\" in data:\n",
    "            layers.append({\n",
    "                \"bias_visible\": data[\"rbms.0.bias_visible\"], \n",
    "                \"bias_hidden\": data[\"rbms.0.bias_hidden\"], \n",
    "                \"weight\": data[\"rbms.0.weight\"],\n",
    "            })\n",
    "        else:\n",
    "            raise RuntimeError(f\"Can't read file {fn} with keys {data.keys()}\")\n",
    "    assert shape\n",
    "    encoder = BoltzmanEncoder2d(\n",
    "        shape=shape,\n",
    "        code_size=layers[-1][\"bias_hidden\"].shape[-1],\n",
    "        hidden_size=[l[\"bias_hidden\"].shape[-1] for l in layers[:-1]]\n",
    "    )\n",
    "    for layer, rbm in zip(layers, encoder.rbms):\n",
    "        rbm.weight[:] = layer[\"weight\"]\n",
    "        rbm.bias_visible[:] = layer[\"bias_visible\"]\n",
    "        rbm.bias_hidden[:] = layer[\"bias_hidden\"]\n",
    "    return encoder\n",
    "        \n",
    "    #encoder = BoltzmanEncoder2d((1, 32, 32), 128, [128])\n",
    "    #data = torch.load()\n",
    "    #print(\"{:,} steps\".format(data[\"num_input_steps\"]))\n",
    "    #encoder.load_state_dict(data[\"state_dict\"])\n",
    "    #torch.save(encoder.state_dict(), \"../models/encoder2d/boltzman-1x32x32-128-photo-300M.pt\")\n",
    "\n",
    "encoder = stack_rbms(\n",
    "    \"../checkpoints/rbm-k1/best.pt\",\n",
    "    \"../checkpoints/rbm-k2/best.pt\"\n",
    ")\n",
    "encoder.device\n",
    "#BoltzmanEncoder2d((1, 10, 10), 200).rbms[0].bias_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591894de-ffa5-4058-9502-0446d215a55f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    encoder = BoltzmanEncoder2d((1, 32, 32), 128)\n",
    "    data = torch.load(\"../checkpoints/rbm9/best.pt\")\n",
    "    print(\"{:,} steps\".format(data[\"num_input_steps\"]))\n",
    "    encoder.load_state_dict(data[\"state_dict\"])\n",
    "    #torch.save(encoder.state_dict(), \"../models/encoder2d/boltzman-1x32x32-128-photo-300M.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d7feaf-f782-4599-a1da-8d7d5751eef5",
   "metadata": {},
   "source": [
    "# load encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f44a28-d96f-47ae-a89f-35e74895148f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    encoder = BoltzmanEncoder2d.from_torch(    \n",
    "        #\"../models/encoderconv/encoder-1x32x32-128-photo-5.pt\", \n",
    "        \"../checkpoints/rbm-all1/best.pt\", \n",
    "    )\n",
    "elif 1:\n",
    "    encoder = EncoderConv2d.from_torch(    \n",
    "        \"../models/encoder2d/conv-1x32x32-128-all1.pt\", \n",
    "        #\"../checkpoints/rbm-all1/best.pt\", \n",
    "    )\n",
    "else:\n",
    "    encoder = BoltzmanEncoder((1, 32, 32), 128, [1024])\n",
    "    encoder.load_state_dict(torch.load(\"../checkpoints/rbm5/best.pt\", map_location=torch.device('cpu'))[\"state_dict\"])\n",
    "encoder.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab609d0-ad15-4d01-9e8d-0e3fd7209645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = make_image_patch_dataset(\n",
    "    encoder.shape, \n",
    "    [\"~/Pictures/photos\", \"~/Pictures/__diverse\", \"../db/images/kali/\"],\n",
    "    recursive=True, \n",
    "    interleave_images=4, patch_shuffle=100_000,\n",
    "    scales=[1., 1./6.],\n",
    ")\n",
    "ds = ImageFilterIterableDataset(ds, filter=ImageFilter(min_mean=.1))\n",
    "ds = DissimilarImageIterableDataset(ds, max_similarity=.9, max_age=100_000, verbose=True)\n",
    "patches = next(iter(DataLoader(ds, batch_size=32*32)))\n",
    "VF.to_pil_image(make_grid_labeled(patches[:32*3], nrow=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74e84f-2b90-40c0-a508-eebb69d56520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    features = torch.round(encoder(patches.to(encoder.device)).cpu(), decimals=5)\n",
    "    features /= features.norm(dim=1, keepdim=True)\n",
    "\n",
    "#df = pd.DataFrame(model(patches).detach().numpy())\n",
    "display(px.line(pd.DataFrame(features[:50]).T.copy(), title=\"random embeddings\"))\n",
    "display(px.line(features.std(0), title=\"embeddings std\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf7cadd-5896-4b11-9d52-01eb8d20f925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sim = features[:100] @ features[:100].T\n",
    "print(sim.mean())\n",
    "px.imshow(sim, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0615156f-57ab-46dc-9b16-c458ac5d0a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = (sim * (1. - torch.diag(torch.Tensor([1] * sim.shape[0])))).argmax(dim=1)\n",
    "values, _ = (sim * (1. - torch.diag(torch.Tensor([1] * sim.shape[0])))).max(dim=1)\n",
    "grid = []\n",
    "grid_labels = []\n",
    "for i, (label, value) in enumerate(zip(labels, values)):\n",
    "    if value > .5:\n",
    "        grid.append(patches[i])\n",
    "        grid.append(patches[label])\n",
    "        grid_labels.append(\"\")\n",
    "        grid_labels.append(f\"{float(value):.3f}\")\n",
    "VF.to_pil_image(make_grid_labeled(grid, labels=grid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064aa342-7352-40dc-b363-d902b4e6f3eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "big_sim = features @ features.T\n",
    "values, labels = big_sim.sort(dim=1, descending=True)\n",
    "grid = []\n",
    "grid_labels = []\n",
    "for i, (label_row, value_row) in enumerate(zip(labels[:50], values[:50])):\n",
    "    for l, v in zip(label_row[:30], value_row[:30]):\n",
    "        grid.append(patches[l])\n",
    "        grid_labels.append(f\"{float(v):.3f}\")\n",
    "VF.to_pil_image(make_grid_labeled(grid, nrow=30, labels=grid_labels))\n",
    "VF.to_pil_image(make_grid(grid, nrow=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed9077-565a-4d01-b296-cf66a0637873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc2ba3-1381-4c79-8f5b-3e4d3d210a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa8cfd57-cd98-4de9-a0ef-e159d5e31f5c",
   "metadata": {},
   "source": [
    "# feature vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f43a036-cb97-4274-af83-5d88aa94d93a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RandomRotation(nn.Module):\n",
    "    def __init__(self, degree: float = 10., random_center: float = 1.):\n",
    "        super().__init__()\n",
    "        self.degree = degree\n",
    "        self.random_center = random_center\n",
    "        \n",
    "    def forward(self, x):\n",
    "        degree = (torch.rand(1).item() * 2. - 1.) * self.degree\n",
    "        center = (torch.rand(2) - .5) * self.random_center + .5\n",
    "        center = [\n",
    "            max(0, min(x.shape[-2] - 1, int(center[0] * x.shape[-2]))),\n",
    "            max(0, min(x.shape[-1] - 1, int(center[1] * x.shape[-1])))\n",
    "        ]\n",
    "        return VF.rotate(x, angle=degree, center=center)\n",
    "        \n",
    "def feature_visualization(\n",
    "    encoder: EncoderConv2d,\n",
    "    target: torch.Tensor,\n",
    "    shape: Optional[Tuple[int, int]] = None,\n",
    "    std: float = .1,\n",
    "    mean: float = .5,\n",
    "    num_iter: int = 10,\n",
    "    batch_size: int = 5,\n",
    "    lr: float = 1.,\n",
    "):  \n",
    "    target = target.unsqueeze(0).expand(batch_size, -1).to(encoder.device)\n",
    "    pixel_shape = encoder.shape\n",
    "    if shape:\n",
    "        pixel_shape = (encoder.shape[0], *shape)\n",
    "\n",
    "    run_again = True\n",
    "    while run_again:\n",
    "        run_again = False\n",
    "        \n",
    "        pixels = nn.Parameter(torch.rand(pixel_shape).to(encoder.device) * std + mean)\n",
    "\n",
    "        optimizer = torch.optim.Adadelta([pixels], lr=lr * 5.)\n",
    "        optimizer = torch.optim.Adamax([pixels], lr=lr * .04)\n",
    "\n",
    "        augmentations = [\n",
    "            VT.Pad(2, padding_mode=\"reflect\"),\n",
    "            #VT.RandomAffine(15, (.3, .3), scale=(.9, 1.1)),\n",
    "            #RandomRotation(4, 1),\n",
    "            #VT.RandomPerspective(1, .6),\n",
    "        ]\n",
    "        if shape:\n",
    "            augmentations.append(VT.RandomCrop(encoder.shape[-2:]))\n",
    "\n",
    "        for itr in range(num_iter):\n",
    "\n",
    "            with torch.no_grad():\n",
    "                mix = .35\n",
    "                pixels[:] = pixels * (1.-mix) + mix * VF.gaussian_blur(pixels, 5, 5)\n",
    "\n",
    "            pixel_batch = []\n",
    "            for batch_idx in range(batch_size):\n",
    "                aug_pixels = pixels\n",
    "                for aug in augmentations:\n",
    "                    aug_pixels = aug(aug_pixels)\n",
    "\n",
    "                pixel_batch.append(aug_pixels.unsqueeze(0))\n",
    "\n",
    "            pixel_batch = torch.concat(pixel_batch)\n",
    "\n",
    "            output = encoder(pixel_batch)\n",
    "            if torch.any(torch.isnan(output)):\n",
    "                run_again = True\n",
    "                print(\"NaN eNcOuNtErEd\")\n",
    "                break\n",
    "\n",
    "            #loss = F.l1_loss(target, output)\n",
    "            #loss = F.mse_loss(target, output)\n",
    "            loss = -F.cosine_similarity(target, output, dim=1).mean()\n",
    "            #loss = F.soft_margin_loss(output, target)\n",
    "\n",
    "            encoder.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    return pixels.detach().clamp(0, 1).cpu()\n",
    "IDX = 15\n",
    "settings = dict(\n",
    "    num_iter=5, lr=2., batch_size=5,\n",
    ")\n",
    "img = feature_visualization(encoder, features[IDX], shape=(40, 40), **settings)\n",
    "display(VF.to_pil_image(patches[IDX]))    \n",
    "VF.to_pil_image(VF.resize(img, [s * 4 for s in img.shape[-2:]], interpolation=VF.InterpolationMode.NEAREST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b87989-8172-4c4a-beaf-029de5ed3df2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3a3c27-9ab1-4764-858b-29fc51813829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "for i in tqdm(range(4*4)):\n",
    "    target = features[i] \n",
    "    #target = torch.randn_like(target) * target.std() + target.mean()\n",
    "    images.append(VF.resize(patches[i], (32, 32)))\n",
    "    images.append(feature_visualization(encoder, target, shape=(32, 32), **settings))\n",
    "    \n",
    "img = make_grid(images, nrow=4)\n",
    "VF.to_pil_image(VF.resize(img, [s * 4 for s in img.shape[-2:]], interpolation=VF.InterpolationMode.NEAREST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b1dee4-99a6-4df6-98c5-1858359f98eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _make2d(vec):\n",
    "    size = vec.shape[-1]\n",
    "    center = int(math.sqrt(size))\n",
    "    div = 1\n",
    "    for i in range(size - center):\n",
    "        f = size / (center + i)\n",
    "        if f == int(f):\n",
    "            div = center + i\n",
    "            break\n",
    "        f = size / (center - i)\n",
    "        if f == int(f):\n",
    "            div = center - i\n",
    "            break\n",
    "    return vec.view(div, size // div)\n",
    "\n",
    "_make2d(torch.rand(112)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328b38df-5d7a-45fc-9010-fdd221770efb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "128 / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454acca4-c06d-462f-a1f7-62dba2378f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_window(shape: Tuple[int, int]):\n",
    "    return (\n",
    "          torch.hamming_window(shape[-1], periodic=True).unsqueeze(0).expand(shape[-2], -1)\n",
    "        * torch.hamming_window(shape[-2], periodic=True).unsqueeze(0).expand(shape[-1], -1).T\n",
    "    )\n",
    "#px.imshow(get_window((10, 15)))\n",
    "\n",
    "def reconstruct_image(\n",
    "        encoder: EncoderConv2d, \n",
    "        original: torch.Tensor, \n",
    "        sub_sample: float = 1, \n",
    "        noise: float = 0.,\n",
    "        patch_shape: Optional[Tuple[int, int]] = None,\n",
    "        num_iter: int = 10,\n",
    "        lr: float = 1.,\n",
    "        **kwargs,\n",
    "):\n",
    "    _patch_shape = encoder.shape[-2:]\n",
    "    _scale = [1, 1]\n",
    "    if patch_shape:\n",
    "        _patch_shape = (\n",
    "            int(original.shape[-2] / encoder.shape[-2] * patch_shape[-2]),\n",
    "            int(original.shape[-1] / encoder.shape[-1] * patch_shape[-1]),\n",
    "        )\n",
    "        _scale = [\n",
    "            1. / encoder.shape[-2] * patch_shape[-2],\n",
    "            1. / encoder.shape[-1] * patch_shape[-1],\n",
    "        ]\n",
    "    recon = torch.zeros(encoder.shape[0], *_patch_shape)\n",
    "    recon_sum = torch.zeros(encoder.shape[0], *_patch_shape)\n",
    "    window = get_window(patch_shape or encoder.shape[-2:])\n",
    "\n",
    "    try:\n",
    "        patches = []\n",
    "        positions = []\n",
    "        for patch, pos in iter_image_patches(\n",
    "            original, shape=encoder.shape[-2:],\n",
    "            stride=(int(s / sub_sample) for s in encoder.shape[-2:]),\n",
    "            with_pos=True,\n",
    "        ):\n",
    "            pos = [int(p) for p in pos]\n",
    "            if patch.shape[0] != encoder.shape[0]:\n",
    "                for chan in range(patch.shape[0]):\n",
    "                    patches.append(patch[chan].unsqueeze(0).unsqueeze(0))\n",
    "                    positions.append([chan] + pos)\n",
    "            else:\n",
    "                patches.append(patch.unsqueeze(0))\n",
    "                positions.append([slice(0, patch.shape[0])] + pos)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            features = encoder(torch.concat(patches).to(encoder.device))\n",
    "            if noise:\n",
    "                features = features + noise * torch.randn_like(features)\n",
    "        \n",
    "        for feature, pos in tqdm(zip(features, positions), total=len(positions)):\n",
    "            chan, pos = pos[0], pos[1:]\n",
    "            patch_recon = feature_visualization(encoder, feature, shape=patch_shape, lr=lr, num_iter=num_iter)\n",
    "            s1 = chan\n",
    "            s2 = slice(int(pos[0] * _scale[0]), int(pos[0] * _scale[0]) + patch_recon.shape[-2])\n",
    "            s3 = slice(int(pos[1] * _scale[1]), int(pos[1] * _scale[1]) + patch_recon.shape[-1])\n",
    "            recon[s1, s2, s3] = recon[s1, s2, s3] + patch_recon * window\n",
    "            recon_sum[s1, s2, s3] = recon_sum[s1, s2, s3] + window\n",
    "            #recon[chan, pos[0]: pos[0] + patch.shape[-2], pos[1]: pos[1] + patch.shape[-1]] = patch_recon \n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    mask = recon_sum > 0\n",
    "    recon[mask] = recon[mask] / recon_sum[mask]\n",
    "    return recon\n",
    "\n",
    "original = PIL.Image.open(\n",
    "    \"/home/bergi/Pictures/csv-turing.png\"\n",
    "    #\"/home/bergi/Pictures/__diverse/28580_1.jpg\"\n",
    "    #\"/home/bergi/Pictures/__diverse/merkel_sarkozy_g8_regierungOnline_Kuehler_CMS_small_620.jpeg\"\n",
    "   # \"/home/bergi/Pictures/__diverse/honecker.jpg\"\n",
    "    #\"/home/bergi/Pictures/__diverse/plakat01.jpg\"\n",
    "    #\"/home/bergi/Pictures/DWlZbQ5WsAQEzHT.jpg\"\n",
    "    #\"/home/bergi/Pictures/there_is_no_threat.jpeg\"\n",
    "    #\"/home/bergi/Pictures/diffusion/cthulhu-09.jpeg\"\n",
    ")\n",
    "original = VF.to_tensor(original)\n",
    "original = set_image_channels(original, 1)\n",
    "original = VF.resize(original, [s // 2 for s in original.shape[-2:]])\n",
    "display(VF.to_pil_image(original))\n",
    "\n",
    "img = reconstruct_image(encoder, original, sub_sample=1.1, noise=.000, patch_shape=(64, 64), **settings)\n",
    "VF.to_pil_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ff594-da32-4026-a226-50c99693f00d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc7b470-f256-4fc7-a272-025ca7e96c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1fa367-6629-497d-a24a-69006b83c79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d2af29-1a81-41c7-9fc8-51864a79c844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69292917-077a-49f9-a42f-7d01fb086e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051c6b29-a99a-4323-a680-36a692224ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = encoder.rbms[0].weight.view(-1, 1, 32, 32)\n",
    "VF.to_pil_image(make_grid(img, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05061eee-d889-4fbf-9876-661b2bd641d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = encoder.rbms[1].weight.permute(1, 0).view(-1, 1, 32, 32)\n",
    "img.shape\n",
    "VF.to_pil_image(make_grid(img, normalize=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
