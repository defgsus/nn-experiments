{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6101993a-07f2-4cdc-9c51-987532ba6681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb3515-f119-4d13-b79c-94f4ffe708e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.clipig.transformations.value_trans import Denoising\n",
    "print(next(filter(lambda p: p[\"name\"] == \"model\", Denoising.PARAMS))[\"choices\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee7b73-d71a-4f3a-95ba-3d040f66c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoiser = Denoising(\n",
    "    #model=\"degradient-mid-64x64-150k\",\n",
    "    #model=\"degradient-strong-64x64-230k\",\n",
    "    #model=\"denoise-mid-64x64-150k\",\n",
    "    #model=\"declip-1\",\n",
    "    model=\"denoise-heavy-2\",\n",
    "    mix=1.,\n",
    ")\n",
    "denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c275c916-bad7-4a4b-a229-1f1b7b0439e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093faa4b-4a30-415b-9648-7e8a68382cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965796f3-7f09-4161-9c6d-5457b80bef9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2fbd3d-eb3b-45de-bb8b-0e526a99e65f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f84dcb-1f90-483a-9b6c-a95465080a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def denoise_loop(\n",
    "    image: torch.Tensor,\n",
    "):\n",
    "    images = []\n",
    "    for i in tqdm(range(8*8)):\n",
    "        image = image - image.min()\n",
    "        image = image / image.max()\n",
    "        #print(image.min(), image.max())\n",
    "\n",
    "        image = denoiser.model(image.unsqueeze(0))[0]\n",
    "        images.append(image)\n",
    "        image = (image * 1.01 - 0.01)\n",
    "        #image = image + .8 * torch.randn_like(image)\n",
    "        image = image.clamp(0, 1)\n",
    "\n",
    "    display(VF.to_pil_image(make_grid(images)))\n",
    "\n",
    "denoise_loop(\n",
    "    #torch.rand(3, 128, 128)\n",
    "    torch.randn(3, 128, 128).abs()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27084f7-0d1b-4f41-b4a7-fa333194ef8d",
   "metadata": {},
   "source": [
    "## conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91981710-977c-492f-821d-4921c89b664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.denoise.resconv_cond import ConditionalResConv\n",
    "\n",
    "kernel_size = []\n",
    "padding = []\n",
    "for i in range(9):\n",
    "    t = i / (9 - 1)\n",
    "    ks = 3 * (1. - t) + t * 9\n",
    "    ks = int(ks / 2) * 2 + 1\n",
    "    kernel_size.append(ks)\n",
    "    padding.append(int(math.floor(ks / 2)))\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.module = ConditionalResConv(\n",
    "            in_channels=3,\n",
    "            condition_size=30,\n",
    "            num_layers=9,\n",
    "            channels=32,\n",
    "            stride=1,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding,\n",
    "            activation=\"gelu\",\n",
    "            activation_last_layer=None,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, condition):\n",
    "        return (x - self.module(x, condition)).clamp(0, 1) \n",
    "\n",
    "model = Module().eval()\n",
    "data = torch.load(\n",
    "    #\"../checkpoints/denoise/conresconv-denoise_ds-mnist_l-9_ks1-3_ks2-9_ch-32_stride-1_act-gelu/snapshot.pt\"\n",
    "    #\"../checkpoints/denoise/conresconv-denoise_ds-mnist_noise-0,1.1_l-9_ks1-3_ks2-9_ch-32_stride-1_act-gelu/snapshot.pt\",\n",
    "    \n",
    "    #\"../checkpoints/denoise/conresconv-denoise_ds-pix_noise-0.5,1.0_l-9_ks1-3_ks2-9_ch-32_stride-1_act-gelu/snapshot.pt\"\n",
    "    #\"../checkpoints/denoise/conresconv-difnoise_ds-pix_noise-0.1,1.0_l-9_ks1-3_ks2-9_ch-32_stride-1_act-gelu/snapshot.pt\"\n",
    "    \"../checkpoints/denoise/conresconv-difnoise_ds-pix_noise-0.1,1.3_l-9_ks1-3_ks2-9_ch-32_stride-1_act-gelu/snapshot.pt\"\n",
    ")\n",
    "print(data[\"num_input_steps\"], \"steps\")\n",
    "model.load_state_dict(data[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd04660c-3607-487a-9606-74ee09df8686",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def denoise_conditional_loop(\n",
    "    image_batch: torch.Tensor,\n",
    "    conditionals: torch.Tensor,\n",
    "    frames: int = 20,\n",
    "    frame_stride: int = 1,\n",
    "):\n",
    "    images = list(image_batch)\n",
    "    try:\n",
    "        for i in tqdm(range(frames * frame_stride)):\n",
    "            #image_batch = image_batch - image_batch.min()\n",
    "            #image_batch = image_batch / image_batch.max()\n",
    "            #print(image_batch.min(), image_batch.max())\n",
    "            \n",
    "            image_batch = model(image_batch, conditionals)\n",
    "            #image_batch += .5 * (image_batch_d - image_batch)\n",
    "            if i % frame_stride == 0:\n",
    "                for image in image_batch:\n",
    "                    images.append(image)\n",
    "\n",
    "            image_batch = .5 + (image_batch - image_batch.mean()) * 1.1\n",
    "            #image_batch = (image_batch * 1.02 - 0.01)\n",
    "            image_batch = image_batch + .1 * torch.randn_like(image_batch)\n",
    "            \n",
    "            image_batch = image_batch.clamp(0, 1)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "        \n",
    "    display(VF.to_pil_image(make_grid(images, nrow=image_batch.shape[0])))\n",
    "\n",
    "N=30\n",
    "denoise_conditional_loop(\n",
    "    #torch.rand(3, 128, 128)\n",
    "    (.5 + 2*torch.randn(N, 3, 32, 32)).clamp(0, 1),\n",
    "    \n",
    "    torch.diag(torch.Tensor([1] * N))[:30],\n",
    "    #torch.Tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "    frame_stride=1, frames=20,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126d0e9-fba1-4592-bed2-7396b8d9d736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc584428-9fef-410e-9201-cb9a24ddf6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd4c8cc-27ec-43bb-a6a2-f166aed03110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
