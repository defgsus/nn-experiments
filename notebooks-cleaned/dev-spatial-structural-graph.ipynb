{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda29861-af9d-4099-a664-5f8c125dac68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import *\n",
    "from src.util import *\n",
    "from src.algo import *\n",
    "from src.models.decoder import *\n",
    "from src.models.transform import *\n",
    "from src.models.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea566b99-131d-49d6-b211-1b034bcae778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resize(img, scale: float, mode: VF.InterpolationMode = VF.InterpolationMode.NEAREST):\n",
    "    return VF.resize(img, [max(1, int(s * scale)) for s in img.shape[-2:]], mode, antialias=False)\n",
    "\n",
    "def plot_samples(\n",
    "        iterable, \n",
    "        total: int = 32, \n",
    "        nrow: int = 8, \n",
    "        return_image: bool = False, \n",
    "        show_compression_ratio: bool = False,\n",
    "        label: Optional[Callable] = None,\n",
    "):\n",
    "    samples = []\n",
    "    labels = []\n",
    "    f = ImageFilter()\n",
    "    try:\n",
    "        for idx, entry in enumerate(tqdm(iterable, total=total)):\n",
    "            image = entry\n",
    "            if isinstance(entry, (list, tuple)):\n",
    "                image = entry[0]\n",
    "            if image.ndim == 4:\n",
    "                image = image.squeeze(0)\n",
    "            samples.append(image)\n",
    "            if show_compression_ratio:\n",
    "                labels.append(round(f.calc_compression_ratio(image), 3))\n",
    "            elif label is not None:\n",
    "                labels.append(label(entry) if callable(label) else idx)\n",
    "                \n",
    "            if len(samples) >= total:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    if labels:\n",
    "        image = VF.to_pil_image(make_grid_labeled(samples, nrow=nrow, labels=labels))\n",
    "    else:\n",
    "        image = VF.to_pil_image(make_grid(samples, nrow=nrow))\n",
    "    if return_image:\n",
    "        return image\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107b910-8c6b-4b11-9b69-1307e328a59d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SpatialGraphConv(nn.Module):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/pdf/2308.07946.pdf\n",
    "    https://github.com/Juntongkuki/Pytorch-DSFNet/blob/main/lib/DSFNet.py\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_channels: int,\n",
    "            act: Callable = F.relu_,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.act = act\n",
    "        num_hidden = num_channels // 2\n",
    "        self.conv_b = nn.Conv2d(num_channels, num_hidden, kernel_size=1)\n",
    "        self.conv_c = nn.Conv2d(num_channels, num_hidden, kernel_size=1)\n",
    "        self.conv_d = nn.Conv2d(num_channels, num_channels, kernel_size=1)\n",
    "        self.conv_e = nn.Conv2d(num_hidden, num_channels, kernel_size=1)\n",
    "        self.bn_e = nn.BatchNorm2d(num_channels)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        assert x.ndim == 4, x.ndim                # BxCxHxW\n",
    "        b = self.conv_b(x).flatten(-2)            # BxCxN\n",
    "        c = self.conv_c(x).flatten(-2)            # BxCxN\n",
    "        b_c = b @ c.permute(0, 2, 1)              # BxCxC\n",
    "        b_c = F.softmax(b_c, dim=-1)              # BxCxC\n",
    "        d = self.conv_d(x).flatten(-2)            # BxCxN\n",
    "        print(d.shape, \"d\")\n",
    "        print(b_c.shape, \"bc\")\n",
    "        b_c_d = b_c.flatten(-2) @ d               # BxCxN\n",
    "        print(b_c_d.shape, \"bcd\")\n",
    "        b_c_d = b_c_d.view(x.shape[0], -1, *x.shape[-2:])  # Bx1xHxW\n",
    "        print(b_c_d.shape)\n",
    "        e = self.conv_e(b_c_d)                    # BxCxHxW\n",
    "        print(e.shape)\n",
    "        e = self.bn_e(e)\n",
    "        print(x.shape, b_c_d.shape, e.shape)\n",
    "        y = self.act(x + e)\n",
    "        return y\n",
    "    \n",
    "img = torch.randn(3, 4, 10, 10)\n",
    "out = SpatialGraphConv(4)(img)\n",
    "VF.to_pil_image(make_grid(torch.concat([img, out]), nrow=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac1b144-3536-4b7c-8ffa-283d9ea8b195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.bmm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5803f1-fd66-41b7-8fc3-969fab26fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfeca90-131e-45e6-9eb5-54d4c4c4535b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba8039-88bb-4e20-addf-0d79ebc9dc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SpatialGCN(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            plane: int,\n",
    "            inter_plane: Optional[int] = None,\n",
    "            act: Callable = F.relu_,\n",
    "            residual: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if inter_plane is None:\n",
    "            inter_plane = plane // 2\n",
    "        \n",
    "        self.residual = residual\n",
    "        self.node_k = nn.Conv2d(plane, inter_plane, kernel_size=1)\n",
    "        self.node_v = nn.Conv2d(plane, inter_plane, kernel_size=1)\n",
    "        self.node_q = nn.Conv2d(plane, inter_plane, kernel_size=1)\n",
    "\n",
    "        self.conv_wg = nn.Conv1d(inter_plane, inter_plane, kernel_size=1, bias=False)\n",
    "        self.bn_wg = nn.BatchNorm1d(inter_plane)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(inter_plane, plane, kernel_size=1),\n",
    "            nn.BatchNorm2d(plane),\n",
    "        )\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        assert x.ndim == 4, x.ndim \n",
    "        # b, c, h, w = x.size()\n",
    "        node_k = self.node_k(x)\n",
    "        node_v = self.node_v(x)\n",
    "        node_q = self.node_q(x)\n",
    "        print(node_k.shape, node_v.shape, node_q.shape)\n",
    "        b,c,h,w = node_k.size()\n",
    "        node_k = node_k.contiguous().view(b, c, -1).permute(0, 2, 1)\n",
    "        node_q = node_q.contiguous().view(b, c, -1)\n",
    "        node_v = node_v.contiguous().view(b, c, -1).permute(0, 2, 1)\n",
    "        print(node_k.shape, node_v.shape, node_q.shape)\n",
    "        # A = k * q\n",
    "        # AV = k * q * v\n",
    "        # AVW = k *(q *v) * w\n",
    "        AV = torch.bmm(node_q,node_v)\n",
    "        AV = self.softmax(AV)\n",
    "        AV = torch.bmm(node_k, AV)\n",
    "        AV = AV.transpose(1, 2).contiguous()\n",
    "        AVW = self.conv_wg(AV)\n",
    "        AVW = self.bn_wg(AVW)\n",
    "        AVW = AVW.contiguous().view(b, c, h, -1)\n",
    "        \n",
    "        out = self.out(AVW)\n",
    "        if self.residual:\n",
    "            out = out + x\n",
    "        return self.act(out)\n",
    "        \n",
    "        \n",
    "img = torch.linspace(0, 399, 400).view(1, 4, 10, 10)\n",
    "out = SpatialGCN(4)(img)\n",
    "VF.to_pil_image(resize(make_grid(torch.concat([img, out]), nrow=8, normalize=True), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534b9f80-0249-4257-8b40-e87352ce3144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6384c84a-35a9-4d4a-aa33-f20bf946714b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee65572-21e5-425c-9cf8-23be5d309c44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f5042d-8180-4ab9-9c32-263f7e5c2e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211813df-0afb-45cf-ac88-b85ff675e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, num_in, num_out, kernel_size: int = 5):\n",
    "        super().__init__()\n",
    "        num_hid = max(2, num_in)\n",
    "        self.q_conv = nn.Conv2d(num_in, num_hid, kernel_size)\n",
    "        self.k_conv = nn.Conv2d(num_in, num_hid, kernel_size)\n",
    "        self.v_conv = nn.Conv2d(num_in, num_hid, kernel_size, padding=kernel_size // 2)\n",
    "        self.s_conv = nn.Conv2d(num_hid * num_hid, num_hid, 1)\n",
    "        self.out_conv = nn.Conv2d(num_hid, num_out, 1)\n",
    "        self.residual = nn.Identity() if num_in == num_out else nn.Conv2d(num_in, num_out, 1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert x.ndim == 4, x.shape\n",
    "        bs = x.shape[0]\n",
    "        q = self.q_conv(x).flatten(-2)                # BxCx(HxW)\n",
    "        k = self.k_conv(x).flatten(-2)\n",
    "        v = self.v_conv(x)\n",
    "        s = F.softmax(q @ k.permute(0, 2, 1), dim=1)  # BxCxC\n",
    "        s = self.s_conv(s.view(bs, -1, 1, 1))         # BxCx1x1\n",
    "        y = v * s                                     # BxCxHxW\n",
    "        y = self.out_conv(y)                          # BxCxHxW\n",
    "        y = F.relu(self.residual(x) + y)\n",
    "        return y\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            shape: Tuple[int, int, int],\n",
    "            num_out: int,\n",
    "            channels_ks: Tuple[Tuple[int, int], ...] = ((16, 3), (32, 5), (32, 7)),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.shape = tuple(shape)\n",
    "        self.num_out = num_out\n",
    "        self.channels_ks = tuple(channels_ks)\n",
    "        \n",
    "        self.blocks = nn.Sequential(OrderedDict([\n",
    "            (\n",
    "                f\"block_{i + 1}\",\n",
    "                EncoderBlock(\n",
    "                    num_in=self.shape[0] if i == 0 else self.channels_ks[i - 1][0], \n",
    "                    num_out=ch_out, \n",
    "                    kernel_size=ks,\n",
    "                )\n",
    "            )\n",
    "            for i, (ch_out, ks) in enumerate(self.channels_ks)\n",
    "        ]))\n",
    "        with torch.no_grad():\n",
    "            self._conv_shape = self.blocks(torch.zeros(1, *self.shape)).shape\n",
    "            \n",
    "        self.w_out = nn.Linear(math.prod(self._conv_shape), self.num_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert x.ndim == 4, x.shape\n",
    "        conv = self.blocks(x)\n",
    "        code = self.w_out(conv.flatten(1))\n",
    "        \n",
    "    def extra_repr(self):\n",
    "        return (\n",
    "            f\"shape={self.shape}, num_out={self.num_out},\\nchannels_ks={self.channels_ks}\"\n",
    "            f\",\\n_conv_shape={self._conv_shape}\"\n",
    "        )\n",
    "    \n",
    "#EncoderBlock(1, 3, 3)(torch.rand(1, 1, 5, 7).bernoulli()).round(decimals=2)\n",
    "Encoder((1, 32, 32), 10)#(torch.rand(1, 1, 32, 32).bernoulli()).round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ccb0e-9faa-468d-93ae-4e66b8b27f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#VF.to_pil_image(make_grid(EncoderBlock(1, 3, 5)(torch.rand(8*8, 1, 64, 64).bernoulli())))\n",
    "VF.to_pil_image(make_grid(Encoder(1, 3)(torch.rand(8*8, 1, 64, 64).bernoulli())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f1adb2-341f-48eb-9c0b-805166a175e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VF.to_pil_image(make_grid([\n",
    "    EncoderBlock(1, 3, 25)(torch.rand(1, 1, 64, 64).bernoulli()).squeeze(0)\n",
    "    for i in range(64)\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c52c2-8749-47cb-86e8-465ec43a0672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn.MultiheadAttention(100, 4)(torch.randn(1, 100), torch.randn(1, 100), torch.randn(1, 100))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc21084-6b00-4a53-a274-31e2d40912ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            shape: Tuple[int, int, int],\n",
    "            num_out: int,\n",
    "            patch_size: int = 8,\n",
    "            num_layers: int = 4,\n",
    "            num_heads: int = 4,\n",
    "            hidden_dim: int = 64,\n",
    "            mlp_dim: Optional[int] = None,\n",
    "            representation_size: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(shape) == 3, shape\n",
    "        assert shape[-2] == shape[-1], shape\n",
    "        \n",
    "        self.shape = shape\n",
    "        self.transformer = torchvision.models.VisionTransformer(\n",
    "            image_size=shape[-1],\n",
    "            patch_size=patch_size,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_classes=num_out,\n",
    "            mlp_dim=mlp_dim or hidden_dim,\n",
    "            representation_size=representation_size,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert x.ndim == 4, x.shape\n",
    "        B, C, H, W = x.shape\n",
    "        assert C <= 3, x.shape\n",
    "        \n",
    "        if C < 3:\n",
    "            x = x.expand(B, 3, H, W)\n",
    "        return self.transformer(x)\n",
    "    \n",
    "enc = TransformerEncoder((1, 32, 32), 100) \n",
    "print(enc(torch.rand(1, 1, 32, 32)))\n",
    "print(f\"{num_module_parameters(enc):,}\")\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83612f2-6b02-411a-87f1-c048648ec5e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "torchvision.models.VisionTransformer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0ff1ba-3cb1-45f3-b409-48dd44adc69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_in: int,\n",
    "            shape: Tuple[int, int, int],\n",
    "            patch_size: int = 8,\n",
    "            num_layers: int = 4,\n",
    "            num_heads: int = 4,\n",
    "            hidden_dim: int = 64,\n",
    "            mlp_dim: Optional[int] = None,\n",
    "            representation_size: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(shape) == 3, shape\n",
    "        assert shape[-2] == shape[-1], shape\n",
    "        \n",
    "        self.shape = shape\n",
    "        self.transformer = torchvision.models.VisionTransformer(\n",
    "            image_size=shape[-1],\n",
    "            patch_size=patch_size,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_classes=num_out,\n",
    "            mlp_dim=mlp_dim or hidden_dim,\n",
    "            representation_size=representation_size,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert x.ndim == 4, x.shape\n",
    "        B, C, H, W = x.shape\n",
    "        assert C <= 3, x.shape\n",
    "        \n",
    "        if C < 3:\n",
    "            x = x.expand(B, 3, H, W)\n",
    "        return self.transformer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b234b5-d54a-42f6-8fd7-a22fb01492f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn.modules.TransformerDecoderLayer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03715b1a-9b35-4a87-8bf7-3447de5aa975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer = nn.TransformerDecoderLayer(d_model=64, nhead=8)\n",
    "dec = nn.TransformerDecoder?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ef6906-6743-4aba-876e-19a4086c698c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            shape: Tuple[int, int, int],\n",
    "            code_size: int,\n",
    "            patch_size: Union[int, Tuple[int, int]] = 8,\n",
    "            stride: Union[None, int, Tuple] = None,\n",
    "            num_layers: int = 4,\n",
    "            num_hidden: int = 64,\n",
    "            num_heads: int = 4,\n",
    "            mlp_dim: Optional[int] = None,\n",
    "            dropout: float = 0.1,\n",
    "            activation: Union[None, str, Callable] = F.relu,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        conv_in = nn.Conv2d(shape[0], num_hidden, kernel_size=patch_size, stride=stride or patch_size)\n",
    "        conv_out = nn.ConvTranspose2d(num_hidden, shape[0], kernel_size=patch_size, stride=stride or patch_size)\n",
    "        self.conv_shape = conv_in(torch.empty(1, *shape)).shape[-3:]\n",
    "\n",
    "        self.proj = nn.Linear(code_size, num_hidden)\n",
    "        self.transformer = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(\n",
    "                d_model=num_hidden,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=mlp_dim or num_hidden,\n",
    "                dropout=dropout,\n",
    "                activation=activation,\n",
    "            ),\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.proj_out = nn.Linear(num_hidden, math.prod(self.conv_shape))\n",
    "        self.patches = conv_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.ndim == 2, x.shape\n",
    "        y = self.proj(x)\n",
    "        y = self.transformer(y, y)\n",
    "        y = self.proj_out(y).view(-1, *self.conv_shape)\n",
    "        y = self.patches(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class TransformerAutoencoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            shape: Tuple[int, int, int],\n",
    "            code_size: int,\n",
    "            patch_size: Union[int, Tuple[int, int]] = 8,\n",
    "            stride: Union[None, int, Tuple] = None,\n",
    "            num_layers: int = 4,\n",
    "            num_hidden: int = 64,\n",
    "            num_heads: int = 4,\n",
    "            mlp_dim: Optional[int] = None,\n",
    "            dropout: float = 0.1,\n",
    "            activation: Union[None, str, Callable] = F.relu,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert len(shape) == 3, shape\n",
    "\n",
    "        self.shape = shape\n",
    "\n",
    "        conv_in = nn.Conv2d(shape[0], num_hidden, kernel_size=patch_size, stride=stride or patch_size)\n",
    "        conv_shape = conv_in(torch.empty(1, *shape)).shape[-3:]\n",
    "\n",
    "        self.encoder = nn.Sequential(OrderedDict([\n",
    "            (\"patches\", conv_in),\n",
    "            ('flatten', nn.Flatten(-3)),\n",
    "            (\"proj\", nn.Linear(math.prod(conv_shape), num_hidden)),\n",
    "            (\"transformer\", nn.TransformerEncoder(\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    d_model=num_hidden,\n",
    "                    nhead=num_heads,\n",
    "                    dim_feedforward=mlp_dim or num_hidden,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation,\n",
    "                ),\n",
    "                num_layers=num_layers,\n",
    "            )),\n",
    "            (\"proj_out\", nn.Linear(num_hidden, code_size)),\n",
    "        ]))\n",
    "        self.decoder = TransformerDecoder(\n",
    "            shape=shape,\n",
    "            code_size=code_size,\n",
    "            patch_size=patch_size,\n",
    "            stride=stride,\n",
    "            num_layers=num_layers,\n",
    "            num_hidden=num_hidden,\n",
    "            num_heads=num_heads,\n",
    "            mlp_dim=mlp_dim,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.ndim == 4, x.shape\n",
    "        y = self.encoder(x)\n",
    "        return self.decoder(y)\n",
    "    \n",
    "           \n",
    "TransformerAutoencoder((1, 28, 28), 76, dropout=0.77, patch_size=8, num_layers=2, num_hidden=256, num_heads=8)(torch.ones(1, 1, 28, 28)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395cf3b0-cfee-4904-8c63-354cc1aff396",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn.TransformerDecoderLayer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b0f186-8a9b-4e50-a980-5067a44bd7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dec(torch.ones(1, 64), torch.zeros(1, 64)))\n",
    "print(dec(torch.zeros(1, 64), torch.ones(1, 64)))\n",
    "print(dec(torch.ones(1, 64), torch.ones(1, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e393fd65-fb05-4505-b5ed-b05d72c1c043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c699e347-44a1-4db7-aff0-4f1c95ba7bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35730b1f-0dd6-40c4-8c6c-83cd97a7c932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.models.cnn import *\n",
    "\n",
    "class ConvUpscaleDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            shape: Tuple[int, int, int],\n",
    "            code_size: int,\n",
    "            kernel_size: Union[int, Tuple[int, int]] = 3,\n",
    "            stride: Union[None, int, Tuple] = None,\n",
    "            num_layers: int = 4,\n",
    "            num_hidden: int = 64,\n",
    "            upscale_every: int = 3,\n",
    "            mlp_dim: Optional[int] = None,\n",
    "            dropout: float = 0.1,\n",
    "            activation: Union[None, str, Callable] = F.relu,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = [kernel_size, kernel_size]\n",
    "        \n",
    "        def _is_upscale(i):\n",
    "            return (i + 1) % upscale_every == 0\n",
    "        \n",
    "        # calculate shape at start of convolution\n",
    "        s = list(shape[-2:])\n",
    "        for i in range(num_layers - 1, -1, -1):\n",
    "            if _is_upscale(i):\n",
    "                for j in range(2):\n",
    "                    if s[j] % 2 != 0:\n",
    "                        raise ValueError(f\"Upscaling in layer {i+1} requires scale divisible by 2, got {s}\")\n",
    "                    s[j] //= 2\n",
    "            \n",
    "            for j in range(2):\n",
    "                s[j] -= int(math.ceil(kernel_size[j] / 2))\n",
    "            for j in range(2):\n",
    "                if s[j] <= 0:\n",
    "                    raise ValueError(f\"Convolution in layer {i+1} requires scale > 0, got {s}\")\n",
    "        \n",
    "        start_shape = (num_hidden, *s)\n",
    "        \n",
    "        self.layers = nn.Sequential(OrderedDict([\n",
    "            (\"proj\", nn.Linear(code_size, math.prod(start_shape))),\n",
    "            (\"reshape\", Reshape(start_shape)),\n",
    "        ]))\n",
    "        if activation is not None:\n",
    "            self.layers.add_module(\"act_0\", activation_to_module(activation))\n",
    "            \n",
    "        ch_in = start_shape[0]\n",
    "        ch_out = num_hidden\n",
    "        for i in range(num_layers):\n",
    "            is_last = i == num_layers - 1\n",
    "                        \n",
    "            self.layers.add_module(f\"conv_{i + 1}\", nn.ConvTranspose2d(\n",
    "                ch_in, \n",
    "                ch_out * 4 if _is_upscale(i) else ch_out, \n",
    "                kernel_size\n",
    "            ))\n",
    "            if activation is not None:\n",
    "                self.layers.add_module(f\"act_{i + 1}\", activation_to_module(activation))\n",
    "            ch_in = ch_out\n",
    "            \n",
    "            if _is_upscale(i):\n",
    "                self.layers.add_module(f\"up_{i // upscale_every + 1}\", nn.PixelShuffle(2))\n",
    "        \n",
    "        self.layers.add_module(f\"conv_out\", nn.ConvTranspose2d(\n",
    "            ch_in, \n",
    "            shape[0], \n",
    "            1\n",
    "        ))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)\n",
    "    \n",
    "dec = ConvUpscaleDecoder((1, 32, 32), 20, 3, num_layers=4, upscale_every=2)\n",
    "print(dec(torch.ones(1, 20)).shape)\n",
    "print(f\"{num_module_parameters(dec):,}\")\n",
    "#display(VF.to_pil_image(make_grid(dec(torch.randn(4, 1, 100, 100)))))\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2ea9aa-7630-4d69-8a93-f25eb620c1eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder = Conv2dBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed7dce8-d15a-4080-985e-d7dd97837a13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn.PixelUnshuffle(2)(torch.ones(1, 4, 24, 32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1b981c-c847-4ede-af1d-ad74de7f8b97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.models.encoder import Encoder2d\n",
    "\n",
    "class ResidualEncoderConv2d(Encoder2d):\n",
    "        \n",
    "    def __init__(\n",
    "            self,\n",
    "            shape: Tuple[int, int, int],\n",
    "            code_size: int,\n",
    "            kernel_size: Union[int, Iterable[int]] = 3,\n",
    "            channels: Iterable[int] = (16, 32),\n",
    "            stride: int = 1,\n",
    "            layers_per_block: int = 2,\n",
    "            #blocks: Iterable[BlockConfig],\n",
    "            act_fn: Union[None, str, Callable, nn.Module] = nn.ReLU(),\n",
    "    ):\n",
    "        super().__init__(shape=shape, code_size=code_size)\n",
    "        self.channels = tuple(channels)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        # self.act_fn = act_fn\n",
    "        \n",
    "        self.conv_in = nn.Conv2d(shape[0], self.channels[0], 1)\n",
    "        \n",
    "        channels = [self.shape[0], *self.channels]\n",
    "        if layers_per_block >= len(channels):\n",
    "            raise ValueError(f\"`layers_per_block` must be <= number of layers, got {layers_per_block} and {len(channels)} channels\")\n",
    "\n",
    "        self.convolution = nn.Sequential()\n",
    "        num_blocks = int(math.ceil(len(channels) / layers_per_block))\n",
    "        for block_idx in range(num_blocks):\n",
    "            conv = Conv2dBlock(\n",
    "                channels=channels[:layers_per_block],\n",
    "                kernel_size=self.kernel_size,\n",
    "                act_fn=act_fn,\n",
    "                stride=self.stride,\n",
    "            )\n",
    "            self.convolution.add_module(f\"conv_{block_idx+1}\", conv)\n",
    "            channels = channels[layers_per_block:]\n",
    "        \n",
    "        #with torch.no_grad():\n",
    "        #    encoded_shape = self.convolution(torch.empty(shape)).shape\n",
    "        #self.linear = nn.Linear(math.prod(encoded_shape), self.code_size)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.linear.weight.device\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #return self.linear\n",
    "        return (self.convolution(x).flatten(1))\n",
    "\n",
    "    def get_extra_state(self):\n",
    "        return {\n",
    "            **super().get_extra_state(),\n",
    "            \"kernel_size\": self.kernel_size,\n",
    "            \"channels\": self.channels,\n",
    "            \"act_fn\": self.convolution._act_fn,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_data(cls, data: dict):\n",
    "        extra = data[\"_extra_state\"]\n",
    "        model = cls(\n",
    "            shape=extra[\"shape\"],\n",
    "            kernel_size=extra[\"kernel_size\"],\n",
    "            stride=extra.get(\"stride\", 1),\n",
    "            channels=extra[\"channels\"],\n",
    "            code_size=extra[\"code_size\"],\n",
    "            act_fn=extra[\"act_fn\"],\n",
    "        )\n",
    "        model.load_state_dict(data)\n",
    "        return model\n",
    "\n",
    "    \n",
    "enc = ResidualEncoderConv2d(\n",
    "    (1, 32, 32), 20, kernel_size=3, \n",
    "    channels=(16, 32, 48), layers_per_block=2,        \n",
    ")\n",
    "#print(enc(torch.ones(1, 1, 32, 32)).shape)\n",
    "print(f\"{num_module_parameters(enc):,}\")\n",
    "#display(VF.to_pil_image(make_grid(dec(torch.randn(4, 1, 100, 100)))))\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826574e3-35d7-4a99-9435-be3acb2c447b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "from src.models.encoder import resnet\n",
    "    \n",
    "enc = resnet.resnet18_open(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1).cpu()\n",
    "enc.eval()\n",
    "print(f\"{num_module_parameters(enc):,}\")\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd2509-d7f7-419b-a020-ebbb645e1468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ed941-630e-447d-a3a2-9dc17d7f114d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enc(torch.ones(1, 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189ad41b-8e04-4883-a348-b9ca299c3d00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
