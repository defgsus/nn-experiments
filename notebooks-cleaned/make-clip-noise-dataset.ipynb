{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2263afff-dcec-4fa3-9a75-7034bb08b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import *\n",
    "from src.util import *\n",
    "from src.algo import *\n",
    "from src.models.decoder import *\n",
    "from src.models.transform import *\n",
    "from src.models.util import *\n",
    "from experiments import datasets\n",
    "from experiments.denoise.resconv import ResConv\n",
    "\n",
    "import yaml\n",
    "import ipywidgets\n",
    "from src.clipig.clipig_task import ClipigTask\n",
    "\n",
    "def resize(img, scale: float, mode: VF.InterpolationMode = VF.InterpolationMode.NEAREST):\n",
    "    if isinstance(img, PIL.Image.Image):\n",
    "        shape = (img.height, img.width)\n",
    "    else:\n",
    "        shape = img.shape[-2:]\n",
    "    return VF.resize(img, [max(1, int(s * scale)) for s in shape], mode, antialias=False)\n",
    "\n",
    "class ImageWidget(ipywidgets.Image):\n",
    "\n",
    "    def set_pil(self, image: PIL.Image.Image):\n",
    "        fp = io.BytesIO()\n",
    "        image.save(fp, \"png\")\n",
    "        fp.seek(0)\n",
    "        self.format = \"png\"\n",
    "        self.value = fp.read()\n",
    "\n",
    "    def set_torch(self, image: torch.Tensor):\n",
    "        image = VF.to_pil_image(image)\n",
    "        self.set_pil(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c060049-4e95-4747-aa7c-9d180991fd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = VF.to_tensor(PIL.Image.open(\n",
    "    \"/home/bergi/Pictures/diffusion/cthulhu-16.jpeg\"\n",
    "    #\"/home/bergi/Pictures/diffusion/cables-14.jpeg\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91182c2d-04f5-40eb-82e5-ba3007822920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(image, factor=.5):\n",
    "    org_size = image.shape\n",
    "    image = resize(image, factor)\n",
    "    image = VF.resize(image, org_size[-2:], interpolation=VF.InterpolationMode.BILINEAR, antialias=True)\n",
    "    return image\n",
    "\n",
    "dimage = downsample(image)\n",
    "s1, s2 = slice(100, 132), slice(100, 132)\n",
    "patch = image[:, s1, s2]\n",
    "dpatch = dimage[:, s1, s2]\n",
    "grid = make_grid([patch, dpatch, (patch - dpatch).abs()])\n",
    "VF.to_pil_image(resize(grid, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c72be-2ff7-40d8-aa0d-506bc38e1e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"\"\"\n",
    "clip_model_name: ViT-B/32\n",
    "device: auto\n",
    "initialize: random\n",
    "num_iterations: 10000\n",
    "source_model:\n",
    "  name: pixels\n",
    "  params:\n",
    "    channels: RGB\n",
    "    size:\n",
    "    - 224\n",
    "    - 224\n",
    "targets:\n",
    "- batch_size: 2\n",
    "  optimizer:\n",
    "    betas:\n",
    "    - 0.9\n",
    "    - 0.999\n",
    "    learnrate: 0.01\n",
    "    optimizer: Adam\n",
    "    weight_decay: 0\n",
    "  target_features:\n",
    "  - image: ''\n",
    "    text: fisheye view of a cthulhu fractal\n",
    "    type: image\n",
    "    weight: 1.0\n",
    "  transformations:\n",
    "  - name: padding\n",
    "    params:\n",
    "      active: true\n",
    "      pad_left: 100\n",
    "      pad_right: 100\n",
    "      pad_top: 100\n",
    "      pad_bottom: 100\n",
    "      padding_mode: symmetric\n",
    "  - name: random_scale\n",
    "    params:\n",
    "      active: true\n",
    "      scale_min_xy: [.4, .4]\n",
    "      scale_max_xy: [1., 1.]\n",
    "  - name: random_affine\n",
    "    params:\n",
    "      active: true\n",
    "      degrees_min_max:\n",
    "      - -5.6\n",
    "      - 5.0\n",
    "      interpolation: bilinear\n",
    "      scale_min_max:\n",
    "      - 0.9\n",
    "      - 1.1\n",
    "      shear_min_max:\n",
    "      - -15.0\n",
    "      - 15.0\n",
    "      translate_xy:\n",
    "      - 0.01\n",
    "      - 0.01\n",
    "  - name: random_crop\n",
    "    params:\n",
    "      active: true\n",
    "      pad_if_needed: true\n",
    "      padding_mode: constant\n",
    "      size: 224\n",
    "  - name: multiplication\n",
    "    params:\n",
    "      active: false\n",
    "      add: 0.0\n",
    "      multiply: 0.1\n",
    "  - name: blur\n",
    "    params:\n",
    "      active: false\n",
    "      kernel_size:\n",
    "      - 3\n",
    "      - 3\n",
    "      mix: 0.7\n",
    "      sigma:\n",
    "      - 1.0\n",
    "      - 1.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869ffe3-fb33-4925-8207-31900409a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_config(\n",
    "        config: str,\n",
    "        image: torch.Tensor,\n",
    "        num_iterations: int = 1000,\n",
    "        max_mae: float = 0.1,\n",
    "        preview: bool = True,\n",
    "):\n",
    "    #image = VF.resize(image, (224, 224), VF.InterpolationMode.BILINEAR, antialias=True).cuda()\n",
    "    image = image.cuda()\n",
    "    \n",
    "    fp = io.StringIO(config)\n",
    "    config = yaml.safe_load(fp)\n",
    "    config[\"num_iterations\"] = num_iterations\n",
    "    config[\"pixel_yield_delay_sec\"] = 0.\n",
    "    config[\"initialize\"] = \"input\"\n",
    "    config[\"input_image\"] = image\n",
    "    config[\"source_model\"][\"params\"][\"size\"] = tuple(reversed(image.shape[-2:]))\n",
    "    config[\"targets\"][0][\"target_features\"][0][\"image\"] = image\n",
    "\n",
    "    if preview:\n",
    "        image_widget = ImageWidget()\n",
    "        display(image_widget)\n",
    "    status_widget = ipywidgets.Text()\n",
    "    display(status_widget)\n",
    "\n",
    "    task = ClipigTask(config)    \n",
    "    status = \"requested\"\n",
    "    \n",
    "    try:\n",
    "        with tqdm(total=num_iterations) as progress:\n",
    "            mae = 0\n",
    "            for event in task.run():\n",
    "                if \"status\" in event:\n",
    "                    status = event[\"status\"]\n",
    "        \n",
    "                if \"pixels\" in event:\n",
    "                    pixels = event[\"pixels\"].clamp(0, 1)\n",
    "                    progress.update(1)\n",
    "                    if preview:\n",
    "                        image_widget.set_torch(resize(pixels, 1))\n",
    "\n",
    "                    mae = (pixels - image).abs().mean()\n",
    "                    if mae >= max_mae:\n",
    "                        break\n",
    "                    \n",
    "                status_widget.value = (\n",
    "                    f\"status: {status}, mae={mae}\"\n",
    "                )\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"stopped\")\n",
    "        pass\n",
    "\n",
    "    return pixels.detach().cpu()\n",
    "\n",
    "image = VF.to_tensor(PIL.Image.open(\n",
    "    \"/home/bergi/Pictures/diffusion/cthulhu-16.jpeg\"\n",
    "    #\"/home/bergi/Pictures/diffusion/cables-14.jpeg\"\n",
    "    #\"../datasets/declipig/original/100_1571.jpg\"\n",
    "))\n",
    "\n",
    "\n",
    "noisy_image = run_config(\n",
    "    config,\n",
    "    image,\n",
    "    #downsample(image),\n",
    "    #num_iterations=10,\n",
    "    max_mae=0.07,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8021007d-eb94-426b-bc7d-3a08432d0a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "VF.to_pil_image(\n",
    "    (image - noisy_image).abs()*3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d136534d-60a4-4a18-b9f4-264c24ca702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_PATH = Path(\"../datasets/declipig/clipig-noise-07\")\n",
    "os.makedirs(STORAGE_PATH, exist_ok=True)\n",
    "for filename in sorted(Path(\"../datasets/declipig/original/\").glob(\"*.*\")):\n",
    "    target_filename = STORAGE_PATH / f\"{filename.name}.pt\"\n",
    "    if target_filename.exists():\n",
    "        continue\n",
    "        \n",
    "    print(filename)\n",
    "    image = VF.to_tensor(PIL.Image.open(filename))\n",
    "    \n",
    "    noisy_image = run_config(\n",
    "        config,\n",
    "        image,\n",
    "        #downsample(image),\n",
    "        #num_iterations=10,\n",
    "        max_mae=0.07,\n",
    "        preview=False,\n",
    "    )\n",
    "    noise = noisy_image - image\n",
    "    torch.save(noise, target_filename)\n",
    "    #VF.to_pil_image(noisy_image).save(str(target_filename))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e0a93f-e738-4516-9469-5b107aef2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c944c578-4343-4b65-9f52-62e696da5ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7398738b-7f3b-47af-bcc0-1ded833a3d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c293d27-57f8-4bf2-8296-e18d8aea4bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788a5cb-066a-4b84-be7d-9e6b3d0b4c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b28235-ee2e-4089-b98f-cedb0e77f1c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e1f01-b1af-4bb9-9265-a0b1baf6a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.base_iterable import BaseIterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3902551-fee6-4833-9be3-d8fcbe6f5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipNoiseDataset(BaseIterableDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            patch_size: Tuple[int, int],\n",
    "            interleave_images: int = 1,\n",
    "    ):\n",
    "        self._patch_size = patch_size\n",
    "        self._interleave_images = interleave_images\n",
    "        self._directory_orig = Path(\"~/Pictures/diffusion\").expanduser()\n",
    "        #self._directory_noisy = Path(__file__).resolve().parent.parent.parent / \"datasets/diffusion-clip-noised\"\n",
    "        self._directory_noisy = Path(\"../datasets/diffusion-clip-noised/\")\n",
    "        \n",
    "    def __iter__(self):\n",
    "        ps = self._patch_size\n",
    "        image_pairs = []\n",
    "        image_pair_iterable = self._iter_image_pairs()\n",
    "        iter_count = -1\n",
    "        while True:\n",
    "            iter_count += 1\n",
    "            while len(image_pairs) < self._interleave_images:\n",
    "                try:\n",
    "                    image_orig, image_noisy = next(image_pair_iterable)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "\n",
    "                size = image_orig.shape[-2:]\n",
    "                count = (size[-2] // ps[-2]) * (size[-1] // ps[-1])\n",
    "                count *= 3\n",
    "\n",
    "                image_pairs.append({\"count\": count, \"images\": (image_orig, image_noisy)})\n",
    "\n",
    "            if not image_pairs:\n",
    "                break\n",
    "        \n",
    "            pair_index = iter_count % len(image_pairs)\n",
    "            image_orig, image_noisy = image_pairs[pair_index][\"images\"]\n",
    "\n",
    "            size = image_orig.shape[-2:]\n",
    "\n",
    "            pos = (\n",
    "                random.randrange(0, size[-2] - ps[-2]),\n",
    "                random.randrange(0, size[-2] - ps[-2])\n",
    "            )\n",
    "\n",
    "            patch_orig = image_orig[:, pos[-2]: pos[-2] + ps[-2], pos[-1]: pos[-1] + ps[-1]]\n",
    "            patch_noisy = image_noisy[:, pos[-2]: pos[-2] + ps[-2], pos[-1]: pos[-1] + ps[-1]]\n",
    "\n",
    "            yield patch_orig, patch_noisy\n",
    "\n",
    "            image_pairs[pair_index][\"count\"] -= 1\n",
    "            if image_pairs[pair_index][\"count\"] <= 0:\n",
    "                image_pairs.pop(pair_index)\n",
    "\n",
    "    def _iter_image_pairs(self):\n",
    "        for filename in sorted(self._directory_noisy.glob(\"*.jpeg\")):\n",
    "            image_noisy = VF.to_tensor(PIL.Image.open(filename))\n",
    "            image_orig = VF.to_tensor(PIL.Image.open(\n",
    "                self._directory_orig / filename.name #[:-4]\n",
    "            ))\n",
    "            yield image_orig, image_noisy\n",
    "\n",
    "ds = ClipNoiseDataset((64, 64), interleave_images=3)\n",
    "#next(iter(ds))\n",
    "VF.to_pil_image(make_grid(ds.sample(8*8)[0]))\n",
    "#ds.sample(8*8)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78657a9-a32a-463d-b402-000bb0ca4d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in tqdm(ds):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e5640-663c-479e-9b6f-523dbf428180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13844a-7898-45fb-b1ed-4a8a2ac68dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331a1d3e-fa07-4efc-8c38-f6313eafd231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
