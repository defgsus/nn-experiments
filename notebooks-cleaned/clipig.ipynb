{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9340b8-d0df-4243-a0ec-12c0daa896c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "#from torchvision.transforms import v2\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import clip\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util import *\n",
    "from src.util.image import * \n",
    "from src.algo import *\n",
    "from src.algo.wangtiles import *\n",
    "from src.datasets.generative import *\n",
    "from src.models.cnn import *\n",
    "from src.models.transform import *\n",
    "from src.util.embedding import *\n",
    "from src.models.clip import ClipSingleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff632f-3f4a-42be-8bfa-f7d24b36d93b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts import datasets\n",
    "ds = datasets.RpgTileIterableDataset((1, 32, 32))\n",
    "ds = IterableShuffle(ds, 1000)\n",
    "VF.to_pil_image(next(iter(ds)))#.shape\n",
    "samples = next(iter(DataLoader(ds, batch_size=32)))\n",
    "\n",
    "VF.to_pil_image(make_grid(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c5ca5d-8c47-4dd4-a11d-ed7edcfe9211",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PixelModel(nn.Module):\n",
    "    def __init__(self, shape: Tuple[int, int, int] = (3, 224, 224)):\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "        self.code = nn.Parameter(torch.randn(self.shape) * .1 + .3)\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.code.clamp(0, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        with torch.no_grad():\n",
    "            self.code[:] = torch.randn_like(self.code) * .1 + .3\n",
    "    \n",
    "class DecoderModel(nn.Module):\n",
    "    def __init__(self, decoder: nn.Module, code_size: int, std: float = 1.):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.code_size = code_size\n",
    "        self.std = std\n",
    "        self.code = nn.Parameter(torch.randn(1, code_size) * std)\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.decoder(self.code).squeeze(0).clamp(0, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        with torch.no_grad():\n",
    "            self.code[:] = torch.randn_like(self.code) * self.std\n",
    "            \n",
    "class DecoderModelRGB(nn.Module):\n",
    "    def __init__(self, decoder: nn.Module, code_size: int, std: float = 1.):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.code_size = code_size\n",
    "        self.std = nn.Parameter(std, requires_grad=False) if isinstance(std, torch.Tensor) else std\n",
    "        self.code = nn.Parameter(torch.randn(3, code_size) * std)\n",
    "        \n",
    "    def forward(self):\n",
    "        rgb = self.decoder(self.code).squeeze(1)\n",
    "        return rgb.clamp(0, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        with torch.no_grad():\n",
    "            self.code[:] = torch.randn_like(self.code) * self.std\n",
    "\n",
    "#print(DecoderModelRGB(dalle_decoder, 128)().shape)\n",
    "\n",
    "class DecoderModelHSV(DecoderModelRGB):\n",
    "    \n",
    "    def forward(self):\n",
    "        hsv = self.decoder(self.code).squeeze(1)\n",
    "        hsv[0] = hsv[0] * 3.\n",
    "        return hsv_to_rgb(hsv).clamp(0, 1)\n",
    "\n",
    "class DecoderModelHxW(nn.Module):\n",
    "    def __init__(self, decoder: nn.Module, code_size: int, shape: Tuple[int, int], std: float = 1.):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.code_size = code_size\n",
    "        self.shape = shape\n",
    "        self.std = std\n",
    "        self.code = nn.Parameter(torch.randn(math.prod(shape), code_size) * std)\n",
    "    \n",
    "    def forward(self):\n",
    "        images = self.decoder(self.code).clamp(0, 1)\n",
    "        #x = make_grid(x, nrow=self.shape[1], padding=0)[:1]\n",
    "        shape = images.shape\n",
    "        output = torch.zeros_like(images).view(shape[-3], shape[-2] * self.shape[-2], shape[-1] * self.shape[-1])\n",
    "        for y in range(self.shape[-2]):\n",
    "            for x in range(self.shape[-1]):\n",
    "                output[:, y * shape[-2]: (y + 1) * shape[-2], x * shape[-1]: (x + 1) * shape[-1]] \\\n",
    "                    = images[y * self.shape[-1] + x]\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def reset(self):\n",
    "        with torch.no_grad():\n",
    "            self.code[:] = torch.randn_like(self.code) * self.std\n",
    "    \n",
    "    #def set_pixels(pixels: torch.Tensor, decoder_shape: Tuple[int, int, int] = (1, 64, 64)):\n",
    "        #pixels = VF.resize(pixels, (self.shape[-2] * decoder_shape[-2\n",
    "                                                                   \n",
    "#VF.to_pil_image(DecoderModelHxW(dalle_decoder, 128, (4, 4), .2)())\n",
    "\n",
    "class DecoderModelRGBHxW(nn.Module):\n",
    "    def __init__(self, decoder: nn.Module, code_size: int, shape: Tuple[int, int], std: float = 1.):\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.code_size = code_size\n",
    "        self.shape = shape\n",
    "        self.std = std\n",
    "        self.code = nn.Parameter(torch.randn(math.prod(shape) * 3, code_size) * std)\n",
    "    \n",
    "    def forward(self):\n",
    "        images = self.decoder(self.code).clamp(0, 1)\n",
    "        images = images.view(images.shape[0] // 3, 3, *images.shape[-2:])\n",
    "        shape = images.shape\n",
    "        output = torch.zeros_like(images).view(shape[-3], shape[-2] * self.shape[-2], shape[-1] * self.shape[-1])\n",
    "        for y in range(self.shape[-2]):\n",
    "            for x in range(self.shape[-1]):\n",
    "                output[:, y * shape[-2]: (y + 1) * shape[-2], x * shape[-1]: (x + 1) * shape[-1]] \\\n",
    "                    = images[y * self.shape[-1] + x]\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def reset(self):\n",
    "        with torch.no_grad():\n",
    "            self.code[:] = torch.randn_like(self.code) * self.std\n",
    "\n",
    "#VF.to_pil_image(DecoderModelRGBHxW(dalle_decoder, 128, (4, 4), .2)())            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3ae8c-1777-4df7-9997-3650d52ca489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts.train_autoencoder import DalleAutoencoder, DalleManifoldAutoencoder\n",
    "\n",
    "if 0: # gray\n",
    "    model = DalleAutoencoder((1, 64, 64), vocab_size=128, n_hid=64, group_count=1, n_blk_per_group=1, act_fn=nn.GELU)\n",
    "    model.load_state_dict(torch.load(\"../checkpoints/ae-d3/best.pt\")[\"state_dict\"])\n",
    "elif 0: # gray\n",
    "    model = DalleAutoencoder((1, 32, 32), vocab_size=128, n_hid=96, group_count=4, n_blk_per_group=2, act_fn=nn.GELU, space_to_depth=True)\n",
    "    model.load_state_dict(torch.load(\"../checkpoints/ae-d11-32/best.pt\")[\"state_dict\"])\n",
    "elif 1: # gray\n",
    "    model = DalleManifoldAutoencoder((1, 32, 32), vocab_size=128, n_hid=64, n_blk_per_group=1, act_fn=nn.GELU, space_to_depth=True, decoder_n_blk=8, decoder_n_layer=2, decoder_n_hid=128)\n",
    "    model.load_state_dict(torch.load(\"../checkpoints/ae-manifold9d-8l/best.pt\")[\"state_dict\"])\n",
    "elif 1: # gray\n",
    "    model = DalleManifoldAutoencoder((1, 32, 32), vocab_size=128, n_hid=64, n_blk_per_group=1, act_fn=nn.GELU, space_to_depth=True, decoder_n_blk=8, decoder_n_layer=2, decoder_n_hid=128)\n",
    "    #model = DalleManifoldAutoencoder((1, 32, 32), vocab_size=128, n_hid=64, n_blk_per_group=1, act_fn=nn.GELU, space_to_depth=True, decoder_n_blk=8, decoder_n_layer=2, decoder_n_hid=300)\n",
    "    model.load_state_dict(torch.load(\"../checkpoints/ae-manifold-8/snapshot.pt\")[\"state_dict\"])\n",
    "elif 0: # color\n",
    "    model = DalleAutoencoder((3, 64, 64), vocab_size=128, n_hid=64, group_count=1, n_blk_per_group=1, act_fn=nn.GELU)\n",
    "    model.load_state_dict(torch.load(\"../checkpoints/ae-d4-color/best.pt\")[\"state_dict\"])\n",
    "else:\n",
    "    model = DalleAutoencoder((1, 64, 64), vocab_size=128, n_hid=64, group_count=1, n_blk_per_group=1, act_fn=nn.GELU, space_to_depth=True)\n",
    "    model.load_state_dict(torch.load(\"../checkpoints/ae-d8-sobel-spd/best.pt\")[\"state_dict\"])\n",
    "    \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "dalle_decoder = model.decoder\n",
    "print(f\"{num_module_parameters(dalle_decoder):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecf009e-70e8-49e3-b7fa-147cb658d3da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    print(DecoderModel(dalle_decoder, 128)().shape)\n",
    "    print(DecoderModelHxW(dalle_decoder, 128, (4, 4))().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4f0181-020a-4233-b7e8-8aaac17386b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    display(VF.to_pil_image(make_grid([DecoderModelRGB(dalle_decoder, 128, std=.5)() for _ in range(16)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a081b1-fc65-48fb-867b-ec1fa9231aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "code_mean = None\n",
    "\n",
    "@torch.no_grad()\n",
    "def reproduce(samples):\n",
    "    global code_mean\n",
    "    codes = model.encoder(samples)\n",
    "    code_mean = codes.mean(0)\n",
    "    print(\"std:\", codes.std(1).mean())\n",
    "    repros = dalle_decoder(codes)\n",
    "    display(VF.to_pil_image(\n",
    "        make_grid([\n",
    "            make_grid(samples),\n",
    "            make_grid(repros)\n",
    "        ])\n",
    "    ))\n",
    "    display(px.line(codes.T))\n",
    "    display(px.line(code_mean))\n",
    "    try:\n",
    "        repros_big = dalle_decoder(codes, (96, 96))\n",
    "    except:\n",
    "        return\n",
    "    display(VF.to_pil_image(\n",
    "        make_grid(repros_big)\n",
    "    ))\n",
    "reproduce(samples.mean(-3, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab3e5fa-e522-4a45-8762-055df72bf163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    display(VF.to_pil_image(make_grid([DecoderModelRGB(dalle_decoder, 128, std=code_mean)() for _ in range(16)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1500fa73-82a4-442d-9d7f-3fb5a379ee73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Debug(nn.Module):\n",
    "    def __init__(self, name: str = \"Debug\"):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            print(f\"{self.name}: {x.shape}\")\n",
    "        else:\n",
    "            print(f\"{self.name}: {type(x).__name__}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dbf494-a35e-4802-99ec-d0a7932bae77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize(\n",
    "    pixel_model: nn.Module,\n",
    "    text: str,\n",
    "    # text_neg: Optional[str] = None,\n",
    "    iters: int = 100,\n",
    "    batch_size: int = 1,\n",
    "    lr: float = 10.,\n",
    "    device=\"auto\",\n",
    "):    \n",
    "    device = to_torch_device(device)\n",
    "    print(device)\n",
    "    pixel_model = pixel_model.to(device)\n",
    "    \n",
    "    #optimizer = torch.optim.Adadelta(pixel_model.parameters(), lr=lr * 100.)\n",
    "    optimizer = torch.optim.Adam(pixel_model.parameters(), lr=lr * .1)\n",
    "\n",
    "    transforms = VT.Compose([\n",
    "        #VT.RandomErasing(),\n",
    "        #VT.Pad(30, padding_mode=\"reflect\"),\n",
    "        #Debug(\"model\"),\n",
    "        RandomWangMap((8, 8), probability=1, overlap=0, num_colors=2),\n",
    "        #Debug(\"wangmap\"),\n",
    "        VT.RandomAffine(\n",
    "            degrees=35.,\n",
    "            #scale=(1., 3.),\n",
    "            #scale=(.3, 1.),\n",
    "            #translate=(0, 4. / 64.),\n",
    "        ),\n",
    "        VT.RandomCrop((224, 224)),\n",
    "    ])\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        target_embeddings = ClipSingleton.encode_text(text).to(device)\n",
    "        target_embeddings = target_embeddings / target_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    target_dots = torch.ones(batch_size, 1).half().to(device)\n",
    "\n",
    "    def _pixels_for_clip(pixels):\n",
    "        pixels = pixels.to(device).half()\n",
    "        if pixels.shape[-3] != 3:\n",
    "            pixels = set_image_channels(pixels, 3)\n",
    "        #if pixels.shape[-2:] != (224, 224):\n",
    "        #    pixels = VF.resize(pixels, (224, 224), VT.InterpolationMode.BILINEAR, antialias=True)\n",
    "        return pixels\n",
    "    \n",
    "    def _clip_size(pixels):\n",
    "        if pixels.shape[-2:] != (224, 224):\n",
    "            pixels = VF.resize(pixels, (224, 224), VT.InterpolationMode.BILINEAR, antialias=True)\n",
    "        return pixels\n",
    "    \n",
    "    # find best start candidate\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pixel_batch = []\n",
    "        code_batch = []\n",
    "        for i in tqdm(range(8), desc=\"find best seed\"):\n",
    "            pixel_model.reset()\n",
    "            pixels = _clip_size(transforms(_pixels_for_clip(pixel_model())))\n",
    "            pixel_batch.append(pixels.unsqueeze(0))\n",
    "            code_batch.append(pixel_model.code)\n",
    "         \n",
    "        image_embeddings = ClipSingleton.encode_image(torch.concat(pixel_batch), requires_grad=True)\n",
    "        image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        dots = image_embeddings @ target_embeddings.T\n",
    "        dot_ids = dots.flatten(0).argsort()\n",
    "        \n",
    "        pixel_model.code[:] = code_batch[dot_ids[-1]]\n",
    "        \n",
    "        # show example transforms\n",
    "        display(VF.to_pil_image(make_grid([\n",
    "            _clip_size(transforms(_pixels_for_clip(pixel_model().cpu())))\n",
    "            for _ in range(8)\n",
    "        ])))\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # train\n",
    "    \n",
    "    pixel_history = []\n",
    "    try:\n",
    "        for it in tqdm(range(iters)):\n",
    "            pixel_batch = []\n",
    "            pixels = _pixels_for_clip(pixel_model())\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                pixel_batch.append(_clip_size(transforms(pixels)).unsqueeze(0))\n",
    "            pixel_batch = torch.concat(pixel_batch)\n",
    "            \n",
    "            image_embeddings = ClipSingleton.encode_image(pixel_batch, requires_grad=True)\n",
    "            image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            dots = image_embeddings @ target_embeddings.T\n",
    "\n",
    "            loss = F.l1_loss(dots, target_dots)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if it % 2 == 0:\n",
    "                pixel_history.append(pixels.cpu().detach())\n",
    "\n",
    "            if len(pixel_history) >= 10:\n",
    "                print(float(loss))\n",
    "                #display(VF.to_pil_image(pixels.detach().to(\"cpu\")))\n",
    "                display(VF.to_pil_image(make_grid(pixel_history, nrow=len(pixel_history))))\n",
    "                display(VF.to_pil_image(RandomWangMap((8, 40), overlap=5)(pixel_history[-1])))\n",
    "                pixel_history.clear()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    result = pixel_model().detach().to(\"cpu\")\n",
    "    #filename = \"\".join(\n",
    "    return result\n",
    "        \n",
    "\n",
    "# dalle_decoder.default_shape = (128, 128)\n",
    "VF.to_pil_image(visualize(\n",
    "    #PixelModel((3, 224, 224)),\n",
    "    #PixelModel((3, 32*9, 32*9)),\n",
    "    #DecoderModel(dalle_decoder, 128, std=.5),\n",
    "    #DecoderModelRGB(dalle_decoder, 128, std=.2),\n",
    "    DecoderModelRGBHxW(dalle_decoder, 128, (4, 4), std=.5),\n",
    "    #\"a house in the woods\",\n",
    "    #\"deep in the woods\",\n",
    "    #\"blood splattered wall\",\n",
    "    #\"cracked stone surface\",\n",
    "    #\"close-up of a smiling face\",\n",
    "    #\"yellow square on blue background\",\n",
    "    #\"lava river between black rock\",\n",
    "    #\"the sphere of planet earth in front of a black background\",\n",
    "    #\"checkerboard texture\",\n",
    "    #\"moss and stone\",\n",
    "    #\"stars in the sky\",\n",
    "    #\"deep space photography\",\n",
    "    #\"evil spiderweb\",\n",
    "    #\"weapons of mass destruction\",\n",
    "    #\"organic cell structures\",\n",
    "    #\"small dots on black background\",\n",
    "    #\"Bob Dobbs\",\n",
    "    #\"underwater cobble texture\",\n",
    "    #\"top-down view of a river between meadows\",\n",
    "    #\"pile of guts\",\n",
    "    #\"stone texture\",\n",
    "    #\"top-down view of a city maze\",\n",
    "    #\"knotted ropes\",\n",
    "    #\"an unfriendly stone texture\",\n",
    "    #\"ocean waves\",\n",
    "    #\"typewriter printing\",\n",
    "    #\"cables and wires\",\n",
    "    #\"80's car racing game, bird's eye view\",\n",
    "    \"role playing game outdoor map\",\n",
    "    #\"cracks in the marble floor\",\n",
    "    #\"mountain tops raising from the mist\",\n",
    "    #\"penrose tiling\",\n",
    "    #\"2d side-scroller\",\n",
    "    #\"love & peace pattern\",\n",
    "    #\"friendly pattern\",\n",
    "    batch_size=1,\n",
    "    lr=1.,\n",
    "    iters=1000,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d8597-dead-41ff-ab3b-fbc9c5bf17c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    img = DecoderModel(dalle_decoder, 128, std=.3)()\n",
    "    #img = DecoderModelHxW(dalle_decoder, 128, (4, 4), std=.3).cuda()()\n",
    "    print(img.shape)\n",
    "    img = RandomWangMap((5, 5), probability=1, overlap=0, num_colors=2)(img)\n",
    "    print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14fc553-bf2b-41cc-a5f3-f7e4709ee107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44239689-64f7-4079-9d34-fa066a28b394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fb2a16-84c4-4e96-87e0-8927b854788a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb30fa5-0cc5-4ab7-bcb1-95010fca06ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_wang(\n",
    "    pixel_model: nn.Module,\n",
    "    text: str,\n",
    "    iters: int = 100,\n",
    "    batch_size: int = 1,\n",
    "    lr: float = 10.,\n",
    "    device=\"auto\",\n",
    "):    \n",
    "    device = to_torch_device(device)\n",
    "    \n",
    "    #optimizer = torch.optim.Adadelta(pixel_model.parameters(), lr=lr * 100.)\n",
    "    optimizer = torch.optim.Adam(pixel_model.parameters(), lr=lr * .1)\n",
    "\n",
    "    \n",
    "    transforms = VT.Compose([\n",
    "        #VT.RandomErasing(),\n",
    "        #VT.Pad(30, padding_mode=\"reflect\"),\n",
    "        RandomWangMap((30, 30), probability=1),\n",
    "        VT.RandomAffine(\n",
    "            degrees=35.,\n",
    "            scale=(.3, 1.),\n",
    "            #translate=(0, 4. / 64.),\n",
    "        ),\n",
    "        VT.RandomCrop((224, 224)),\n",
    "    ])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        target_embeddings = ClipSingleton.encode_text(text).to(device)\n",
    "        target_embeddings = target_embeddings / target_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    target_dots = torch.ones(batch_size, 1).half().to(device)\n",
    "\n",
    "    def _pixels_for_clip(pixels):\n",
    "        pixels = pixels.to(device).half()\n",
    "        if pixels.shape[-3] != 3:\n",
    "            pixels = set_image_channels(pixels, 3)\n",
    "        #if pixels.shape[-2:] != (224, 224):\n",
    "        #    pixels = VF.resize(pixels, (224, 224), VT.InterpolationMode.BILINEAR, antialias=True)\n",
    "        return pixels\n",
    "    \n",
    "    def _clip_size(pixels):\n",
    "        if pixels.shape[-2:] != (224, 224):\n",
    "            pixels = VF.resize(pixels, (224, 224), VT.InterpolationMode.BILINEAR, antialias=True)\n",
    "        return pixels\n",
    "    \n",
    "    # find best start candidate\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        wang_template = VF.to_tensor(PIL.Image.open(\"/home/bergi/prog/python/thegame/thegame/assets/cr31/path.png\"))[:3] * 150\n",
    "        wang_template = VF.resize(wang_template, (32, 32), VF.InterpolationMode.BILINEAR, antialias=True)\n",
    "        code = model.encoder(wang_template.unsqueeze(0)).squeeze(0)\n",
    "        pixel_model.code[:] = code\n",
    "    \n",
    "    display(VF.to_pil_image(pixel_model()))\n",
    "    \n",
    "    # train\n",
    "    \n",
    "    pixel_history = []\n",
    "    try:\n",
    "        for it in tqdm(range(iters)):\n",
    "\n",
    "            pixel_batch = []\n",
    "            pixels = _pixels_for_clip(pixel_model())\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                pixel_batch.append(_clip_size(transforms(pixels)).unsqueeze(0))\n",
    "            pixel_batch = torch.concat(pixel_batch)\n",
    "\n",
    "            image_embeddings = ClipSingleton.encode_image(pixel_batch, requires_grad=True)\n",
    "            image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            dots = image_embeddings @ target_embeddings.T\n",
    "\n",
    "            loss = F.l1_loss(dots, target_dots)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if it % 2 == 0:\n",
    "                pixel_history.append(pixels.to(\"cpu\").detach())\n",
    "\n",
    "            if len(pixel_history) >= 10:\n",
    "                print(float(loss))\n",
    "                #display(VF.to_pil_image(pixels.detach().to(\"cpu\")))\n",
    "                display(VF.to_pil_image(make_grid(pixel_history, nrow=len(pixel_history))))\n",
    "                display(VF.to_pil_image(RandomWangMap((8, 40))(pixel_history[-1])))\n",
    "                pixel_history.clear()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    result = pixel_model().detach().to(\"cpu\")\n",
    "    #filename = \"\".join(\n",
    "    return result\n",
    "        \n",
    "\n",
    "VF.to_pil_image(visualize_wang(\n",
    "    #PixelModel((3, 224, 224)).to(\"cuda\"),\n",
    "    DecoderModel(dalle_decoder, 128, std=.1),\n",
    "    #DecoderModelHxW(dalle_decoder, 128, (4, 4), std=0.5),\n",
    "    #\"a house in the woods\",\n",
    "    #\"deep in the woods\",\n",
    "    #\"blood splattered wall\",\n",
    "    #\"cracked stone surface\",\n",
    "    #\"close-up of a smiling face\",\n",
    "    #\"yellow square on blue background\",\n",
    "    #\"lava river between black rock\",\n",
    "    #\"the sphere of planet earth in front of a black background\",\n",
    "    #\"checkerboard texture\",\n",
    "    #\"moss and stone\",\n",
    "    #\"stars in the sky\",\n",
    "    #\"deep space photography\",\n",
    "    #\"evil spiderweb\",\n",
    "    #\"weapons of mass destruction\",\n",
    "    #\"organic structures\",\n",
    "    #\"Bob Dobbs\",\n",
    "    \"underwater cobble texture\",\n",
    "    #\"top-down view of a river between meadows\",\n",
    "    #\"pile of guts\",\n",
    "    #\"stone texture\",\n",
    "    #\"top-down view of a city maze\",\n",
    "    #\"knotted ropes\",\n",
    "    #\"an unfriendly stone texture\",\n",
    "    #\"love & peace pattern\",\n",
    "    #\"friendly pattern\",\n",
    "    batch_size=2,\n",
    "    lr=1.5,\n",
    "    iters=1000,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ced827-d57b-4fd6-bff3-43cffcab88f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f41d5-5948-4d05-9fb6-1b38c5dbf955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e64267-659d-49d3-bc0e-feedacca8b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2bebc6-71fd-47e7-abc4-bd1a98497359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5751a4d5-f427-43d9-82f3-d46142c492bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VT.Pad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249eb18-f292-441d-9996-cafb0cae780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
