{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1845bb39-6321-49fa-a467-da6a469f3ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union, Dict\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset, RandomSampler\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "import torchaudio.transforms as AT\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display, Audio\n",
    "import torchaudio\n",
    "from torchaudio.io import StreamReader\n",
    "\n",
    "from src.datasets import *\n",
    "from src.algo import GreedyLibrary\n",
    "from src.util.image import *\n",
    "from src.util import to_torch_device, iter_batches\n",
    "from src.patchdb import PatchDB, PatchDBIndex\n",
    "from src.models.encoder import *\n",
    "from src.util.audio import *\n",
    "from src.util.files import *\n",
    "from src.util.embedding import *\n",
    "from scripts.train_vae_audio import VAEAudioConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af627096-742c-4224-aeac-c1c0cd026ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def play(audio, shape=(200, 1024), normalize=False):\n",
    "    if normalize:\n",
    "        audio_max = audio.abs().max()\n",
    "        if audio_max:\n",
    "            audio = audio / audio_max\n",
    "    display(Audio(audio.clamp(-1, 1), rate=SAMPLERATE, normalize=False))\n",
    "    display(plot_audio(audio, shape))\n",
    "    \n",
    "def read_stream(stream, seconds: float = 3.):\n",
    "    chunks = []\n",
    "    max_samples = int(seconds * SAMPLERATE)\n",
    "    num_samples = 0\n",
    "    for chunk, in stream:\n",
    "        chunks.append(chunk.mean(-1))\n",
    "        num_samples += chunks[-1].shape[-1]\n",
    "        if num_samples >= max_samples:\n",
    "            break\n",
    "    return torch.concat(chunks)[:max_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a55b404-9d7e-47f8-a675-e6263d2a2fb2",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d53272a-de7c-47bd-9f84-4bcd20e2825c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SLICE_SIZE = 1024# * 4\n",
    "LATENT_SIZE = 128\n",
    "#SAMPLERATE = 22_050\n",
    "SAMPLERATE = 44_100\n",
    "    \n",
    "#model = VAEAudioConv(slice_size=SLICE_SIZE, latent_dims=LATENT_SIZE, channels=[1, 16, 24, 32])\n",
    "model = VAEAudioConv(slice_size=SLICE_SIZE, latent_dims=LATENT_SIZE, channels=[1, 16, 16, 16], kernel_size=7)\n",
    "#model = VAEAudioConv(slice_size=SLICE_SIZE, latent_dims=LATENT_SIZE, channels=[1, 16, 16, 16], kernel_size=15)\n",
    "data = torch.load(\"../checkpoints/audio-vae-4/snapshot.pt\")\n",
    "print(\"steps: {:,}\".format(data[\"num_input_steps\"]))\n",
    "model.load_state_dict(data[\"state_dict\"])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3a59d9-9ddd-4124-9fa8-a15693fdf79d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# load audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74428d0-e626-4fe6-918b-4a36cc740639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"/home/bergi/Music/BR80-backup/ROLAND/LIVEREC/LIVE0033-2023-09-10-poesnek.WAV\"\n",
    "#filename = \"/home/bergi/Music/Aphex Twin/Acoustica Alarm Will Sound Performs Aphex Twin/01 Cock_Ver 10.mp3\"\n",
    "filename = \"/home/bergi/Music/Ray Kurzweil The Age of Spiritual Machines/(audiobook) Ray Kurzweil - The Age of Spiritual Machines - 1 of 4.mp3\"\n",
    "filename = \"/home/bergi/Music/Scirocco/07 Bebop.mp3\"\n",
    "\n",
    "reader = StreamReader(filename)\n",
    "reader.add_basic_audio_stream(4096, sample_rate=SAMPLERATE)\n",
    "\n",
    "reader.seek(1)\n",
    "play(read_stream(reader.stream(), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de847eeb-6e25-4709-9fbe-0f26dddf806f",
   "metadata": {},
   "source": [
    "# encode audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f4c9a-4e26-4bff-ad00-299c30e3d69b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode_audio(\n",
    "        stream, \n",
    "        sub_sample: float = 1.1, \n",
    "        seconds: float = 3.,\n",
    "        noise: float = 0.,\n",
    "):\n",
    "    num_samples = int(seconds * SAMPLERATE)\n",
    "    repro = torch.zeros(num_samples)\n",
    "    repro_sum = torch.zeros(num_samples)\n",
    "    window = torch.hamming_window(SLICE_SIZE)\n",
    "    \n",
    "    embedding_batches = []\n",
    "    pos_batches = []\n",
    "    sample_count = 0\n",
    "    for slice_batch, pos_batch in iter_batches(\n",
    "        tqdm(iter_audio_slices(\n",
    "            stream, \n",
    "            slice_size=SLICE_SIZE, \n",
    "            with_position=True, \n",
    "            stride=int(SLICE_SIZE / sub_sample)\n",
    "        )), \n",
    "        batch_size=128\n",
    "    ):\n",
    "        slice_batch = slice_batch.mean(-1).unsqueeze(-2)\n",
    "        embedding_batch = model.encoder(slice_batch)\n",
    "        embedding_batches.append(embedding_batch)\n",
    "        pos_batches.append(torch.Tensor(pos_batch).type(torch.int64))\n",
    "        sample_count += slice_batch.shape[0] * slice_batch.shape[-1]\n",
    "        if sample_count >= num_samples:\n",
    "            break\n",
    "            \n",
    "    return (\n",
    "        torch.concat(embedding_batches)[:num_samples], \n",
    "        torch.concat(pos_batches)[:num_samples],\n",
    "    )\n",
    "reader.seek(1)\n",
    "slices, positions = encode_audio(reader.stream(), seconds=10)\n",
    "slices = slices.permute(1, 0)\n",
    "if 1:\n",
    "    tsne = TSNE(1, perplexity=10)\n",
    "    o = torch.Tensor(tsne.fit_transform(slices)).squeeze(-1).argsort()\n",
    "    slices = slices[o]\n",
    "slices = (slices * 10.).clamp(-1, 1)\n",
    "img = signed_to_image(slices.unsqueeze(0))\n",
    "img = VF.resize(img, [s * 3 for s in img.shape[-2:]], VF.InterpolationMode.NEAREST)\n",
    "VF.to_pil_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d01fd5-05ac-4073-8cc1-01b058c5ef9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1927c174-db9a-4541-b973-18cb8117bcbf",
   "metadata": {},
   "source": [
    "# reconstruct audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821bbe14-a7d7-402b-aec6-3bd6a490c749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def reconstruct_audio(\n",
    "        stream, \n",
    "        sub_sample: float = 1.1, \n",
    "        seconds: float = 3.,\n",
    "        noise: float = 0.,\n",
    "        echo: float = 0.,\n",
    "        transform: Optional[Callable] = None,\n",
    "        samplerate: int = SAMPLERATE,\n",
    "        slice_size: int = SLICE_SIZE,\n",
    "):\n",
    "    num_samples = int(seconds * samplerate)\n",
    "    repro = torch.zeros(num_samples)\n",
    "    repro_sum = torch.zeros(num_samples)\n",
    "    window = torch.hamming_window(slice_size)\n",
    "    \n",
    "    last_embedding = None\n",
    "    for slice_batch, pos_batch in iter_batches(\n",
    "        tqdm(iter_audio_slices(\n",
    "            stream, \n",
    "            slice_size=slice_size, \n",
    "            with_position=True, \n",
    "            stride=int(slice_size / sub_sample)\n",
    "        )), \n",
    "        batch_size=128\n",
    "    ):\n",
    "        slice_batch = slice_batch.mean(-1).unsqueeze(-2)\n",
    "        embedding_batch = model.encoder(slice_batch)\n",
    "        if noise:\n",
    "            embedding_batch = embedding_batch + noise * torch.randn_like(embedding_batch)\n",
    "        if transform:\n",
    "            embedding_batch = transform(embedding_batch)\n",
    "        \n",
    "        if echo:\n",
    "            for i, embedding in enumerate(embedding_batch):\n",
    "                if last_embedding is None:\n",
    "                    last_embedding = embedding\n",
    "                else:\n",
    "                    embedding_batch[i] = embedding_batch[i] * (1. - echo) + echo * last_embedding\n",
    "                last_embedding = embedding_batch[i]\n",
    "            \n",
    "        repro_batch = model.decoder(embedding_batch)\n",
    "        \n",
    "        do_break = False\n",
    "        for repro_slice, pos in zip(repro_batch, pos_batch):\n",
    "            if pos + slice_size >= num_samples:\n",
    "                do_break = True\n",
    "                break\n",
    "            \n",
    "            repro[pos: pos+slice_size] += window * repro_slice[0]\n",
    "            repro_sum[pos: pos+slice_size] += window\n",
    "            \n",
    "        if do_break:\n",
    "            break\n",
    "    \n",
    "    mask = repro_sum > 0\n",
    "    repro[mask] = repro[mask] / repro_sum[mask]\n",
    "    \n",
    "    return repro#.clamp(-1, 1)\n",
    "\n",
    "reader.seek(1)\n",
    "play(reconstruct_audio(\n",
    "    reader.stream(), seconds=10, noise=0.00,\n",
    "    #sub_sample=4.,\n",
    "    #echo=.9,\n",
    "    #transform=lambda emb: torch.concat([emb[:, -1:], emb[:, :-1]], dim=-1), \n",
    "    #transform=lambda emb: emb.permute(1, 0)\n",
    "    #transform=lambda emb: emb.clamp(-1, 0) + emb.clamp(0, 1) * 10# + .0001 * torch.linspace(0, 10, emb.shape[-1]) \n",
    "), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a57f64-0412-4c98-9819-832bbaaf0229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2df2b774-2a47-453b-97fb-e40250d6f58d",
   "metadata": {},
   "source": [
    "# convert to EncoderConv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc35104-bd50-4d00-b299-f4b378cb6d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6592b394-961e-4269-8709-20b61e59efd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enc = EncoderConv1d(\n",
    "    shape=(1, SLICE_SIZE),\n",
    "    kernel_size=model.encoder.encoder[0].layers[0].kernel_size[0],\n",
    "    channels=model.encoder.encoder[0].channels[1:],\n",
    "    code_size=LATENT_SIZE,\n",
    ")\n",
    "dec = EncoderConv1d(\n",
    "    shape=(1, SLICE_SIZE),\n",
    "    kernel_size=model.encoder.encoder[0].layers[0].kernel_size[0],\n",
    "    channels=model.encoder.encoder[0].channels[1:],\n",
    "    code_size=LATENT_SIZE,\n",
    "    reverse=True,\n",
    ")\n",
    "with torch.no_grad():\n",
    "    enc.linear.weight[:] = model.encoder.linear_mu.weight\n",
    "    enc.linear.bias[:] = model.encoder.linear_mu.bias\n",
    "    for i in range(0, len(model.encoder.encoder[0].layers), 2):\n",
    "        enc.convolution.layers[i].weight[:] = model.encoder.encoder[0].layers[i].weight\n",
    "        enc.convolution.layers[i].bias[:] = model.encoder.encoder[0].layers[i].bias\n",
    "    \n",
    "    dec.linear.weight[:] = model.decoder[0].weight\n",
    "    dec.linear.bias[:] = model.decoder[0].bias\n",
    "    for i in range(0, len(model.decoder[2].layers), 2):\n",
    "        dec.convolution.layers[i].weight[:] = model.decoder[2].layers[i].weight\n",
    "        dec.convolution.layers[i].bias[:] = model.decoder[2].layers[i].bias\n",
    "    \n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d62dcdf-5b57-4e47-a47a-6b82f50e62c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    reader.seek(31)\n",
    "    for slice in zip(iter_audio_slices(reader.stream(), SLICE_SIZE), \"abc\"):\n",
    "        slice = slice[0].permute(1, 0)\n",
    "    \n",
    "    emb1 = model.encoder(slice.unsqueeze(0))[0]\n",
    "    emb2 = enc(slice.unsqueeze(0))[0]\n",
    "\n",
    "    slice_repro = dec(emb2.unsqueeze(0))[0]\n",
    "    display(px.line(pd.DataFrame({\"org\": slice[0], \"repro\": slice_repro[0]})))\n",
    "    \n",
    "    display(px.line(pd.DataFrame({\"vae\": emb1, \"enc\": emb2})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2275aba-f1aa-42c8-9686-36847d509567",
   "metadata": {},
   "source": [
    "# save encoder1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c62e54c-a3e2-4b50-b6a5-74242401beef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls ../models/encoder1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f8811-9232-413d-aae3-2f77abd55fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = f\"../models/encoder1d/conv-1x{SLICE_SIZE}-{LATENT_SIZE}.pt\"\n",
    "torch.save(enc.state_dict(), filename)\n",
    "filename = f\"../models/encoder1d/conv-1x{SLICE_SIZE}-{LATENT_SIZE}-decoder.pt\"\n",
    "torch.save(dec.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9374f2-950a-4318-9281-b82fb121cc53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e646edc4-bef5-4846-9fa1-d229dd18c305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04761f87-073a-4012-91dd-97baa1af9b20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader.seek(1)\n",
    "audio = read_stream(reader.stream(), 3.)\n",
    "\n",
    "speccer = AT.MelSpectrogram(\n",
    "    sample_rate=SAMPLERATE,\n",
    "    n_fft=2048,\n",
    "    n_mels=200,\n",
    "    #f_max=1000,\n",
    "    power=.5,\n",
    "    #normalized=True,\n",
    ")\n",
    "spec = speccer(audio)\n",
    "spec[spec == 0] = torch.nan\n",
    "px.imshow(spec, height=800, aspect=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42da7c8f-e283-4f32-94c9-23288a78eaed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AT.MelSpectrogram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94135a9-08f8-40f6-9c0a-07b6e6a51d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d48d11-94db-4dad-8c7c-9fe2a24c27d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee0474-3aad-4e91-a40f-909e9c20512a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
