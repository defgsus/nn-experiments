{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fec0f7-dda6-4a63-a429-df76cf32fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de7e29-8068-4b07-8976-b03fd5de17df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            kernel_size: int = 3,\n",
    "            padding: Optional[int] = None,\n",
    "            activation: Union[None, str, Callable] = None,\n",
    "            dropout: float = 0.,\n",
    "            residual: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._act = activation\n",
    "        self._residual = residual\n",
    "        \n",
    "        if padding is None:\n",
    "            padding = int(math.floor(kernel_size / 2))\n",
    "        \n",
    "        self.q = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.k = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.v = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.act = activation_to_callable(activation)\n",
    "        if dropout > 0:\n",
    "            self.dropout = nn.Dropout2d(dropout)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        msg = f\"residual={self._residual}\"\n",
    "        msg = f\"{msg}, activation={repr(self._act)}\"\n",
    "        return msg\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "\n",
    "        attn = q @ k\n",
    "        attn = F.softmax(attn.view(B, -1, H * W), dim=-1).view(B, -1, H, W)\n",
    "        if hasattr(self, \"dropout\"):\n",
    "            attn = self.dropout(attn)\n",
    "            \n",
    "        y = attn @ v\n",
    "\n",
    "        if self.act is not None:\n",
    "            y = self.act(y)\n",
    "    \n",
    "        return y\n",
    "\n",
    "    \n",
    "\n",
    "m = AttentionConvBlock(3, 10, activation=\"gelu\")\n",
    "print(f\"params: {num_module_parameters(m):,}\")\n",
    "inp = torch.rand(1, 3, 16, 16)\n",
    "outp = m(inp)\n",
    "print(f\"{inp.shape} -> {outp.shape}\")\n",
    "display(m)\n",
    "display(VF.to_pil_image(outp[0, :3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8655df57-3492-4cbb-a41c-b380f1350c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            kernel_size: int = 3,\n",
    "            padding: Optional[int] = None,\n",
    "            activation: Union[None, str, Callable] = None,\n",
    "            dropout: float = 0.,\n",
    "            residual: bool = False,\n",
    "            norm: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._act = activation\n",
    "        self._residual = residual\n",
    "\n",
    "        if padding is None:\n",
    "            padding = int(math.floor(kernel_size / 2))\n",
    "\n",
    "        self.q = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.k = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.v = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.attn_conv3 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "        self.attn_conv5 = nn.Conv2d(out_channels, out_channels, 5, padding=2)\n",
    "        self.act = activation_to_callable(activation)\n",
    "        self.norm = normalization_to_module(norm, out_channels)\n",
    "\n",
    "        if dropout > 0:\n",
    "            self.dropout = nn.Dropout2d(dropout)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"residual={self._residual}, activation={repr(self._act)}\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "        residual = x\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        q = F.relu(self.q(x))\n",
    "        k = F.relu(self.k(x))\n",
    "        v = self.v(x)\n",
    "\n",
    "        attn = q @ k\n",
    "        if hasattr(self, \"dropout\"):\n",
    "            attn = self.dropout(attn)\n",
    "\n",
    "        attn = attn + self.attn_conv3(attn) + self.attn_conv5(attn)\n",
    "        \n",
    "        y = attn @ v\n",
    "\n",
    "        if self.act is not None:\n",
    "            y = self.act(y)\n",
    "\n",
    "        if self._residual:\n",
    "            y = y + residual\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "m = AttentionConvBlock(3, 10, activation=\"gelu\")\n",
    "print(f\"params: {num_module_parameters(m):,}\")\n",
    "inp = torch.rand(1, 3, 16, 16)\n",
    "outp = m(inp)\n",
    "print(f\"{inp.shape} -> {outp.shape}\")\n",
    "display(m)\n",
    "display(VF.to_pil_image(outp[0, :3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21461cf-9e62-41ee-84f9-d196835556f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dDepth(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            *args, **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.depth_conv = nn.Conv2d(in_channels, out_channels, *args, **kwargs)\n",
    "        self.point_conv = nn.Conv2d(out_channels, out_channels, 1)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.depth_conv(x)\n",
    "        y = self.point_conv(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "    \n",
    "\n",
    "m = Conv2dDepth(3, 10, 3, 1)\n",
    "print(f\"params: {num_module_parameters(m):,}\")\n",
    "inp = torch.rand(1, 3, 16, 16)\n",
    "outp = m(inp)\n",
    "print(f\"{inp.shape} -> {outp.shape}\")\n",
    "display(m)\n",
    "display(VF.to_pil_image(outp[0, :3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdbdcc5-fadc-4b01-a97e-a831b13de249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafdb4bb-da3b-416a-aefe-d5dc9f95d32b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8450b0c4-b9ca-4633-a88f-2b6b188cad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(v):\n",
    "    display((v * 100).to(torch.int))\n",
    "    \n",
    "v = torch.rand(1, 2, 3, 3, generator=torch.Generator().manual_seed(23))\n",
    "show(v)\n",
    "show(F.softmax(v, dim=-1))\n",
    "show(F.softmax(v.view(1, 2, 9), dim=-1).view(1, 2, 3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9226f5-bb45-4268-959d-4a49326dee9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676bac0d-d64b-46a8-b0d1-ce3a62069652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_module_weights(module: nn.Module, max_magnitude: float):\n",
    "    with torch.no_grad():\n",
    "        for param in module.parameters():\n",
    "            param[:] = param.clamp(-max_magnitude, max_magnitude)\n",
    "            print(param.max(), param.shape)\n",
    "\n",
    "clip_module_weights(m, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6100be64-f6c3-4432-b2b2-03d00a04525e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a4b85-c7b1-4402-a693-dc378348d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 28\n",
    "while size > 4 and (size // 2) % 2 == 0:\n",
    "    print(size)\n",
    "    size //= 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ccf39-069f-4d11-b4fd-3aba418828d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469f4919-ece2-4ff1-a764-e53e49e31726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69198dee-0354-42ca-8832-55afa5e6bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.efficientvit.ops import LiteMLA\n",
    "\n",
    "m = LiteMLA(16, 32)\n",
    "print(f\"params: {num_module_parameters(m):,}\")\n",
    "inp = torch.rand(1, 16, 16, 16)\n",
    "outp = m(inp)\n",
    "print(f\"{inp.shape} -> {outp.shape}\")\n",
    "display(m)\n",
    "display(VF.to_pil_image(outp[0, :3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46faacd6-1b1d-4a4d-a23f-4ecaf507fb85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
