{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d7805-5073-4601-8a5c-4cc209a4f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_notebook import *\n",
    "from src.train.experiment import load_experiment_trainer\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e938945c-8b94-4b44-afaa-0510d7f7b177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(ds, count=16*16):\n",
    "    batch = next(iter(DataLoader(ds, batch_size=count)))\n",
    "    if isinstance(batch, (tuple, list)):\n",
    "        images = batch[0]\n",
    "        for b in batch[1:]:\n",
    "            if isinstance(b, torch.Tensor) and b.shape[-3:] == images.shape[-3:]:\n",
    "                images = torch.cat([images, b], dim=0)\n",
    "    else:\n",
    "        images = batch\n",
    "        \n",
    "    display(VF.to_pil_image(make_grid(images, nrow=int(math.sqrt(count)))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acca63f-10b1-4be5-85a2-472b57ebab55",
   "metadata": {},
   "source": [
    "## load trainer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4967e9-d93a-4791-a095-8854b8d3f972",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = load_experiment_trainer(\"../experiments/img2img/extrusion/extrusion-simple-adv.yml\", device=\"auto\")\n",
    "assert trainer.load_checkpoint(\"snapshot\")\n",
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e51fb8-204a-440f-bec8-14767907c63b",
   "metadata": {},
   "source": [
    "### create adversarial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314dbfcb-0b1a-44f3-adaa-e8c4538601b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adversarial(\n",
    "        model: nn.Module,\n",
    "        image: torch.Tensor,\n",
    "        max_adversarial_distance: float = 0.001,\n",
    "        #min_output_distance: float = 0.1,\n",
    "        steps: int = 1000,\n",
    "        lr: float = 0.0001,\n",
    "):\n",
    "    model.eval()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "        device = p.device\n",
    "\n",
    "    image = image[:, :1].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ref_image = model(image.repeat(1, 3, 1, 1))\n",
    "\n",
    "    display(VF.to_pil_image(make_grid(ref_image)))\n",
    "    \n",
    "    ad_image = nn.Parameter(image.clone())\n",
    "    \n",
    "    optimizer = torch.optim.AdamW([ad_image], lr=lr)\n",
    "\n",
    "    history = []\n",
    "    with tqdm(total=steps) as progress:\n",
    "        for step in range(steps):\n",
    "    \n",
    "            output = model(ad_image.repeat(1, 3, 1, 1).clamp(0, 1)).clamp(0, 1)\n",
    "    \n",
    "            output_distance = F.l1_loss(ref_image, output)\n",
    "            adversarial_distance = F.l1_loss(image, ad_image)\n",
    "    \n",
    "            loss = (\n",
    "                #(adversarial_distance - max_adversarial_distance).clamp_min(0)\n",
    "                #- 0.01 * output_distance\n",
    "                adversarial_distance - output_distance / 4\n",
    "            )\n",
    "    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            entry = {\"loss\": float(loss), \"od\": float(output_distance), \"ad\": float(adversarial_distance)}\n",
    "            history.append(entry)\n",
    "            progress.desc = \" \".join(f\"{k}={v}\" for k, v in entry.items())\n",
    "            progress.update()\n",
    "            \n",
    "    ad_image = ad_image.detach().cpu()\n",
    "    display(VF.to_pil_image(make_grid(image)))\n",
    "    display(VF.to_pil_image(make_grid(ad_image)))\n",
    "    display(VF.to_pil_image(make_grid(output)))\n",
    "    return ad_image, pd.DataFrame(history)\n",
    "    \n",
    "ad_image, hist = create_adversarial(\n",
    "    trainer.model,\n",
    "    next(iter(trainer.validation_loader))[1][:8],\n",
    "    steps=10_000,\n",
    ")\n",
    "df = hist.copy()\n",
    "df.index = pd.DatetimeIndex(df.index * 1_000_000_000)\n",
    "df.resample(\"50s\").mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca53589-f9db-4a1a-8976-5619a11c4bef",
   "metadata": {},
   "source": [
    "## train whole image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b95d54-0738-44b2-8f6d-b652f1cdbd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adversarial(\n",
    "        model: nn.Module,\n",
    "        image: torch.Tensor,\n",
    "        adversarial_distance_factor: float = 2.,\n",
    "        steps: int = 1000,\n",
    "        lr: float = 0.003,\n",
    "        batch_size: int = 8,\n",
    "        shape: Tuple[int, int] = (64, 64),\n",
    "):\n",
    "    model.eval()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "        device = p.device\n",
    "\n",
    "    C, H, W = image.shape\n",
    "    print(f\"image: {W}x{H}\")\n",
    "    image = image[:1]\n",
    "    ad_image = nn.Parameter(image.clone().to(device))\n",
    "    history = []\n",
    "\n",
    "    optimizer = torch.optim.AdamW([ad_image], lr=lr)\n",
    "    \n",
    "    #if not isinstance(adversarial_distance_factor, (list, tuple)):\n",
    "    #    adversarial_distance_factor = tuple(adversarial_distance_factor)\n",
    "\n",
    "    def yield_crops():\n",
    "        t = 0.\n",
    "        with tqdm(total=steps * batch_size) as progress:\n",
    "            px, py = 0, 0\n",
    "            for step in range(steps):\n",
    "                if step % 500 == 0:\n",
    "                    px += 512\n",
    "                    if px >= W:\n",
    "                        px = 0\n",
    "                        py += 512\n",
    "                t = step / (steps - 1)\n",
    "                batches = [[], []]\n",
    "                for i in range(batch_size):\n",
    "                    x = px + random.randrange(min(512, W - px) - shape[-1])\n",
    "                    y = py + random.randrange(min(512, H - py) - shape[-2])\n",
    "                    #x, y = 500 + min(x, 100), 500 + min(y, 100)\n",
    "                    batches[0].append(   image[:, y: y + shape[-2], x: x + shape[-1]].to(device))\n",
    "                    batches[1].append(ad_image[:, y: y + shape[-2], x: x + shape[-1]])\n",
    "                yield t, tuple(\n",
    "                    torch.concat([b.unsqueeze(0) for b in batch])\n",
    "                    for batch in batches\n",
    "                )\n",
    "\n",
    "                if history:\n",
    "                    progress.desc = \" \".join(f\"{k}={v}\" for k, v in history[-1].items())\n",
    "                \n",
    "                progress.update(batch_size)\n",
    "\n",
    "    try:\n",
    "        for t, (image_crops, ad_image_crops) in yield_crops():\n",
    "            with torch.no_grad():\n",
    "                ref_crops = model(image_crops.repeat(1, C, 1, 1)).clamp(0, 1)\n",
    "            output_crops = model(ad_image_crops.repeat(1, C, 1, 1).clamp(0, 1)).clamp(0, 1)\n",
    "    \n",
    "            output_distance = F.l1_loss(output_crops, ref_crops)\n",
    "\n",
    "            if callable(adversarial_distance_factor):\n",
    "                adf = adversarial_distance_factor(t)\n",
    "            else:\n",
    "                adf = adversarial_distance_factor\n",
    "            #adf = adversarial_distance_factor[0] * (1-t) + t * adversarial_distance_factor[1]\n",
    "            \n",
    "            (-output_distance / adf).backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            adversarial_distance = F.l1_loss(ad_image_crops, image_crops)\n",
    "    \n",
    "            (adversarial_distance).backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            entry = {\"out-d\": float(output_distance), \"adv-d\": float(adversarial_distance)}\n",
    "            history.append(entry)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                ad_image[:] = ad_image.clamp(0, 1)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ad_image[:] = ad_image.clamp(0, 1)\n",
    "\n",
    "    ad_image = ad_image.detach().cpu().repeat(C, 1, 1)\n",
    "    display(VF.to_pil_image(make_grid(image_crops)))\n",
    "    display(VF.to_pil_image(make_grid(ad_image_crops)))\n",
    "    display(VF.to_pil_image(make_grid(output_crops)))\n",
    "    display(VF.to_pil_image(make_grid(ref_crops)))\n",
    "    display(VF.to_pil_image(signed_to_image(make_grid(ref_crops - output_crops))))\n",
    "    torch.cuda.empty_cache()\n",
    "    return ad_image, pd.DataFrame(history)\n",
    "\n",
    "def mix(a, b, t):\n",
    "    return a * (1-t) + t * b;\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "ad_image, hist = create_adversarial(\n",
    "    trainer.model,\n",
    "    VF.to_tensor(PIL.Image.open(\"../datasets/extrusion/train/source/005.png\"))\n",
    "        #[:, 500:700, 500:700]\n",
    "    ,\n",
    "    steps=4000, \n",
    "    adversarial_distance_factor=lambda t: mix(1, 2, min(1, t*4)),\n",
    ")\n",
    "df = hist.copy()\n",
    "df.index = pd.DatetimeIndex(df.index * 1_000_000_000)\n",
    "display(df.resample(\"50s\").mean().plot())\n",
    "display(VF.to_pil_image(ad_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4413130f-910b-4c25-815c-93b05ee94d05",
   "metadata": {},
   "source": [
    "# whole image in strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce34569-9ba8-4af8-9f6c-6ddfc06836a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adversarial(\n",
    "        model: nn.Module,\n",
    "        image: torch.Tensor,\n",
    "        adversarial_distance_factor: float = 2.,\n",
    "        steps: int = 1000,\n",
    "        lr: float = 0.003,\n",
    "        batch_size: int = 32,\n",
    "        shape: Tuple[int, int] = (64, 64),\n",
    "):\n",
    "    model.eval()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "        device = p.device\n",
    "\n",
    "    C, H, W = image.shape\n",
    "    print(f\"image: {W}x{H}\")\n",
    "    image = image[:1]\n",
    "    ad_image = nn.Parameter(image.clone().to(device))\n",
    "    history = []\n",
    "\n",
    "    optimizer = torch.optim.AdamW([ad_image], lr=lr)\n",
    "    \n",
    "    #if not isinstance(adversarial_distance_factor, (list, tuple)):\n",
    "    #    adversarial_distance_factor = tuple(adversarial_distance_factor)\n",
    "\n",
    "    def yield_crops():\n",
    "        t = 0.\n",
    "        count = math.prod(image.shape[-2:]) / math.prod(shape[-2:])  \n",
    "        with tqdm(total=steps * count) as progress:\n",
    "            for step in range(steps):\n",
    "                \n",
    "                for image_crops, positions in iter_image_patches(\n",
    "                        image, shape, stride=shape, batch_size=batch_size, with_pos=True,\n",
    "                        random_offset=[s//3 for s in shape],\n",
    "                ):\n",
    "                    \n",
    "                    ad_image_crops = torch.concat([\n",
    "                        ad_image[:, y: y + shape[-2], x: x + shape[-1]].unsqueeze(0)\n",
    "                        for y, x in positions\n",
    "                    ])\n",
    "                    \n",
    "                    yield image_crops.to(device), ad_image_crops\n",
    "                    progress.update(image_crops.shape[0])\n",
    "                    \n",
    "                    if history:\n",
    "                        progress.desc = \" \".join(f\"{k}={v}\" for k, v in history[-1].items())\n",
    "                \n",
    "\n",
    "    try:\n",
    "        for image_crops, ad_image_crops in yield_crops():\n",
    "            with torch.no_grad():\n",
    "                ref_crops = model(image_crops.repeat(1, C, 1, 1)).clamp(0, 1)\n",
    "            output_crops = model(ad_image_crops.repeat(1, C, 1, 1).clamp(0, 1)).clamp(0, 1)\n",
    "    \n",
    "            output_distance = F.l1_loss(output_crops, ref_crops)\n",
    "\n",
    "            (-output_distance / adversarial_distance_factor).backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            adversarial_distance = F.l1_loss(ad_image_crops, image_crops)\n",
    "    \n",
    "            (adversarial_distance).backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            entry = {\"out-d\": float(output_distance), \"adv-d\": float(adversarial_distance)}\n",
    "            history.append(entry)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                ad_image[:] = ad_image.clamp(0, 1)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ad_image[:] = ad_image.clamp(0, 1)\n",
    "\n",
    "    ad_image = ad_image.detach().cpu().repeat(C, 1, 1)\n",
    "    display(VF.to_pil_image(make_grid(image_crops)))\n",
    "    display(VF.to_pil_image(make_grid(ad_image_crops)))\n",
    "    display(VF.to_pil_image(make_grid(output_crops)))\n",
    "    display(VF.to_pil_image(make_grid(ref_crops)))\n",
    "    display(VF.to_pil_image(signed_to_image(make_grid(ref_crops - output_crops))))\n",
    "    torch.cuda.empty_cache()\n",
    "    return ad_image, pd.DataFrame(history)\n",
    "\n",
    "def mix(a, b, t):\n",
    "    return a * (1-t) + t * b;\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "ad_image, hist = create_adversarial(\n",
    "    trainer.model,\n",
    "    VF.to_tensor(PIL.Image.open(\"../datasets/extrusion/train/source/005.png\"))\n",
    "        #[:, 500:700, 500:700]\n",
    "    ,\n",
    "    steps=2000, \n",
    "    #adversarial_distance_factor=lambda t: mix(1, 2, min(1, t*4)),\n",
    ")\n",
    "df = hist.copy()\n",
    "df.index = pd.DatetimeIndex(df.index * 1_000_000_000)\n",
    "display(df.resample(\"50s\").mean().plot())\n",
    "display(VF.to_pil_image(ad_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ae141-a56e-407f-b6be-1f2e6f08eb04",
   "metadata": {},
   "source": [
    "# image crops with repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a07f87-2391-438c-beae-5557fa20658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adversarial(\n",
    "        model: nn.Module,\n",
    "        image: torch.Tensor,\n",
    "        adversarial_distance_factor: float = 2.,\n",
    "        steps: int = 1000,\n",
    "        lr: float = 0.0003,\n",
    "        batch_size: int = 8,\n",
    "        repeats: int = 10,\n",
    "        shape: Tuple[int, int] = (64, 64),\n",
    "):\n",
    "    model.eval()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "        device = p.device\n",
    "\n",
    "    C, H, W = image.shape\n",
    "    print(f\"image: {W}x{H}\")\n",
    "    image = image[:1]\n",
    "    ad_image = nn.Parameter(image.clone().to(device))\n",
    "    history = []\n",
    "\n",
    "    optimizer = torch.optim.AdamW([ad_image], lr=lr)\n",
    "\n",
    "    def yield_crops():\n",
    "        with tqdm(total=steps * batch_size * repeats) as progress:\n",
    "            for step in range(steps):\n",
    "                batches = [[], []]\n",
    "                positions = []\n",
    "                for i in range(batch_size): \n",
    "                    x = random.randrange(W - shape[-1])\n",
    "                    y = random.randrange(H - shape[-2])\n",
    "                    #x = random.randrange(10)\n",
    "                    #y = random.randrange(10)\n",
    "                    batches[0].append(   image[:, y: y + shape[-2], x: x + shape[-1]].to(device))\n",
    "                    batches[1].append(ad_image[:, y: y + shape[-2], x: x + shape[-1]])\n",
    "                    positions.append((x, y))\n",
    "                yield ( \n",
    "                    positions,\n",
    "                    tuple(torch.concat([b.unsqueeze(0) for b in batch])\n",
    "                      for batch in batches)\n",
    "                )\n",
    "                if history:\n",
    "                    progress.desc = \" \".join(f\"{k}={v}\" for k, v in history[-1].items())\n",
    "                \n",
    "                progress.update(batch_size * repeats)\n",
    "\n",
    "    try:\n",
    "        for positions, (image_crops, ad_image_crops) in yield_crops():\n",
    "            with torch.no_grad():\n",
    "                ref_crops = model(image_crops.repeat(1, C, 1, 1)).clamp(0, 1)\n",
    "    \n",
    "            for j in range(repeats):\n",
    "                output_crops = model(ad_image_crops.repeat(1, C, 1, 1).clamp(0, 1)).clamp(0, 1)\n",
    "        \n",
    "                output_distance = F.l1_loss(output_crops, ref_crops)\n",
    "                adversarial_distance = F.l1_loss(ad_image_crops, image_crops)\n",
    "    \n",
    "                loss = adversarial_distance - output_distance / adversarial_distance_factor\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                entry = {\"out-d\": float(output_distance), \"adv-d\": float(adversarial_distance)}\n",
    "                history.append(entry)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                ad_image[:] = ad_image.clamp(0, 1)\n",
    "                #for (x, y), image_crop, ad_image_crop in zip(positions, image_crops, ad_image_crops):\n",
    "                #    pad_ad_image_crop = image_crop.clone()\n",
    "                #    #p = 8\n",
    "                    #pad_ad_image_crop[:, p:-p, p:-p] = ad_image_crop[:, p:-p, p:-p]\n",
    "                #    ad_image[:, y: y + shape[-2], x: x + shape[-1]] = pad_ad_image_crop\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "            \n",
    "    ad_image = ad_image.detach().cpu().repeat(C, 1, 1)\n",
    "    display(VF.to_pil_image(make_grid(image_crops)))\n",
    "    display(VF.to_pil_image(make_grid(ad_image_crops)))\n",
    "    display(VF.to_pil_image(make_grid(output_crops)))\n",
    "    display(VF.to_pil_image(make_grid(ref_crops)))\n",
    "    display(VF.to_pil_image(signed_to_image(make_grid(ref_crops - output_crops))))\n",
    "    torch.cuda.empty_cache()\n",
    "    return ad_image, pd.DataFrame(history)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "ad_image, hist = create_adversarial(\n",
    "    trainer.model,\n",
    "    VF.to_tensor(PIL.Image.open(\"../datasets/extrusion/train/source/004.png\")), #[:, 500:600, 500:600],\n",
    "    steps=20000, \n",
    ")\n",
    "df = hist.copy()\n",
    "df.index = pd.DatetimeIndex(df.index * 1_000_000_000)\n",
    "display(df.resample(\"100s\").mean().plot())\n",
    "display(VF.to_pil_image(ad_image))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
