{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ea0a0-5a76-4cf8-856e-1933e0231839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union, Dict\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset, RandomSampler\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "import torchaudio.transforms as AT\n",
    "import torchaudio.functional as AF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display, Audio\n",
    "import torchaudio\n",
    "from torchaudio.io import StreamReader\n",
    "\n",
    "from src.datasets import *\n",
    "from src.algo import GreedyLibrary\n",
    "from src.util.image import *\n",
    "from src.util import to_torch_device, iter_batches\n",
    "from src.patchdb import PatchDB, PatchDBIndex\n",
    "from src.models.encoder import *\n",
    "from src.models.cnn import *\n",
    "from src.models.util import *\n",
    "from src.models.transform import *\n",
    "from src.util.audio import *\n",
    "from src.util.files import *\n",
    "from src.util.embedding import *\n",
    "from scripts import datasets\n",
    "\n",
    "def resize(img, scale: float, mode: VF.InterpolationMode = VF.InterpolationMode.NEAREST):\n",
    "    return VF.resize(img, [max(1, int(s * scale)) for s in img.shape[-2:]], mode, antialias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6478cdf-bcb1-4080-8a2e-e2995f84a07d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_vec(*shape):\n",
    "    n = math.prod(shape)\n",
    "    return torch.linspace(0, n - 1, n).view(shape)\n",
    "\n",
    "get_vec(1, 3, 10, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df35d5-c020-4f57-8af8-840d11f601ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def space_to_depth(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.cat(\n",
    "        [\n",
    "            x[..., ::2, ::2], \n",
    "            x[..., 1::2, ::2], \n",
    "            x[..., ::2, 1::2], \n",
    "            x[..., 1::2, 1::2]\n",
    "        ], \n",
    "        dim=1\n",
    "    )\n",
    "    \n",
    "batch = get_vec(1, 2, 4, 6)\n",
    "print(batch.shape)\n",
    "print(batch)\n",
    "d = space_to_depth(batch)\n",
    "print(d.shape)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b23157-8afd-464b-ab57-d6946a514b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def depth_to_space(x: torch.Tensor) -> torch.Tensor:\n",
    "    nc = x.shape[-3] // 2\n",
    "    s = torch.stack([\n",
    "        d[..., :nc, :, :], \n",
    "        d[..., nc:, :, :],\n",
    "       # d[..., 1:nc+1, :, :]\n",
    "    ], dim=-1).view(-1, nc, d.shape[-2], d.shape[-1] * 2)\n",
    "    nc //= 2\n",
    "    s = torch.stack([\n",
    "        s[..., :nc, :, :], \n",
    "        s[..., nc:, :, :], \n",
    "    ], dim=-2).view(-1, nc, d.shape[-2] * 2, d.shape[-1] * 2)\n",
    "    return s\n",
    "\n",
    "rd = depth_to_space(d)\n",
    "print(rd.shape)\n",
    "rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00208750-113b-44de-8a7c-72a76eeb4db2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvTest(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(4 * 3, 4, 5)\n",
    "        self.conv2 = nn.Conv2d(4 * 4, 3, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = space_to_depth(x)          # 12 x 32 x 32\n",
    "        x = self.conv1(x)              #  4 x 28 x 28\n",
    "        x = space_to_depth(x)          # 16 x 14 x 14\n",
    "        x = self.conv2(x)              #  3 x 10 x 10\n",
    "        return x\n",
    "\n",
    "conv = ConvTest()\n",
    "image = get_vec(1, 3, 64, 64)\n",
    "print(image.shape)\n",
    "out = conv(image)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb4df5-f128-4c2a-b5b1-fd4b90ddae0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a87b173-7af2-4a63-8d77-1233fc037970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PrintShape(nn.Module):\n",
    "    def __init__(self, prefix: str = \"\"):\n",
    "        super().__init__()\n",
    "        self.prefix = prefix\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(self.prefix, x.shape)\n",
    "        return x\n",
    "    \n",
    "class Conv2dBlockLOCAL(Conv2dBlock):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            channels: Iterable[int],\n",
    "            kernel_size: Union[int, Iterable[int]] = 5,\n",
    "            stride: int = 1,\n",
    "            pool_kernel_size: int = 0,\n",
    "            pool_type: str = \"max\",  # \"max\", \"average\"\n",
    "            act_fn: Optional[nn.Module] = None,\n",
    "            act_last_layer: bool = True,\n",
    "            bias: bool = True,\n",
    "            transpose: bool = False,\n",
    "            batch_norm: bool = False,\n",
    "            space_to_depth: bool = False,\n",
    "    ):\n",
    "        super().__init__(channels, kernel_size, stride, pool_kernel_size, pool_type, act_fn, act_last_layer, bias, transpose, batch_norm, space_to_depth)\n",
    "        self.channels = list(channels)\n",
    "        assert len(self.channels) >= 2, f\"Got: {channels}\"\n",
    "            \n",
    "        num_layers = len(self.channels) - 1\n",
    "        if isinstance(kernel_size, int):\n",
    "            self.kernel_size = [kernel_size] * num_layers \n",
    "        else:\n",
    "            self.kernel_size = list(kernel_size)\n",
    "            if len(self.kernel_size) != num_layers:\n",
    "                raise ValueError(f\"Expected `kernel_size` to have {num_layers} elements, got {self.kernel_size}\")\n",
    "                \n",
    "        self._act_fn = act_fn\n",
    "\n",
    "        self.layers = nn.Sequential()\n",
    "\n",
    "        if batch_norm:\n",
    "            self.layers.append(\n",
    "                nn.BatchNorm2d(self.channels[0])\n",
    "            )\n",
    "\n",
    "        #if space_to_depth and transpose:\n",
    "        #    for i, ch in enumerate(self.channels):\n",
    "        #        if ch / 4 != ch // 4:\n",
    "        #            raise ValueError(f\"with 'space_to_depth' and 'transpose', channels must be divisible by 4, got {ch} at position {i}\")\n",
    "                    \n",
    "        in_channel_mult = 1\n",
    "        out_channel_mult = 1\n",
    "        for i, (in_channels, out_channels, kernel_size) in enumerate(zip(self.channels, self.channels[1:], self.kernel_size)):\n",
    "            \n",
    "            if space_to_depth and transpose:\n",
    "                self.layers.append(SpaceToDepth2d(transpose=transpose))\n",
    "                self.layers.append(PrintShape(\"before conv\"))\n",
    "                out_channel_mult = 1\n",
    "                if i < len(self.channels) - 2:\n",
    "                    out_channel_mult = 4\n",
    "                    \n",
    "            self.layers.append(\n",
    "                self._create_layer(\n",
    "                    in_channels=in_channels * in_channel_mult,\n",
    "                    out_channels=out_channels * out_channel_mult,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    bias=bias,\n",
    "                    transpose=transpose,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            if space_to_depth and not transpose and i < len(self.channels) - 1:\n",
    "                self.layers.append(PrintShape(\"after conv\"))\n",
    "                self.layers.append(SpaceToDepth2d(transpose=transpose))\n",
    "                in_channel_mult = 4\n",
    "\n",
    "            if pool_kernel_size and i == len(self.channels) - 2:\n",
    "                klass = {\n",
    "                    \"max\": nn.MaxPool2d,\n",
    "                    \"average\": nn.AvgPool2d,\n",
    "                }[pool_type]\n",
    "                self.layers.append(\n",
    "                    klass(pool_kernel_size)\n",
    "                )\n",
    "            if self._act_fn and (act_last_layer or i + 2 < len(self.channels)):\n",
    "                self.layers.append(act_fn)\n",
    "    \n",
    "CHANNELS = [3, 2, 4, 5]\n",
    "KERNEL_SIZE = [5, 5, 6]\n",
    "conv = Conv2dBlockLOCAL(channels=CHANNELS, kernel_size=KERNEL_SIZE, space_to_depth=True)\n",
    "print(conv)\n",
    "print(\"out:\", conv(image).shape)\n",
    "convt = Conv2dBlockLOCAL(channels=list(reversed(CHANNELS)), kernel_size=list(reversed(KERNEL_SIZE)), space_to_depth=True, transpose=True)\n",
    "print(convt)\n",
    "print(convt(conv(image)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eba5f6-b472-4012-8aeb-ba0a79a22ed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "32*3*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f42eb5-7603-44a2-9ce8-7bcb3bb2dc1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c1 = Conv2dBlock([3, 1], 5)\n",
    "c2 = Conv2dBlock([1, 3], 5, transpose=True)\n",
    "i1 = torch.rand(1, 3, 64, 64)\n",
    "i2 = c1(i1)\n",
    "i3 = c2(i2)\n",
    "print(f\"{i1.shape} -> {i2.shape} -> {i3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651948a9-0237-489d-b295-77cf572243ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65922505-c4ca-4f5b-b2b9-114a89efb18c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = TransformDataset(\n",
    "    TensorDataset(torch.load(\"../datasets/kali-uint8-64x64.pt\")),\n",
    "    dtype=torch.float,\n",
    "    multiply=1./255.,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd57220-c7d6-42c4-9f17-038649f72ed0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sobel(nn.Module):\n",
    "    def __init__(self, kernel_size: int = 5, sigma: float = 5.):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def forward(self, x):\n",
    "        blurred = VF.gaussian_blur(x, [self.kernel_size, self.kernel_size], [self.sigma, self.sigma])\n",
    "        return (x - blurred).clamp_min(0)\n",
    "    \n",
    "image = ds[23][0]\n",
    "display(VF.to_pil_image(image))\n",
    "display(VF.to_pil_image(Sobel()(image)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac80151-abf6-4d67-892d-5aeece256abb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VF.gaussian_blur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b276d3-8ecc-4a24-822b-a7b7fbbf25d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923aeadf-08f7-4610-bd90-db88592bb1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
