{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da61208-be78-451a-b4ea-3cb53ebe16b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import *\n",
    "from src.util import *\n",
    "from src.algo import *\n",
    "from src.models.decoder import *\n",
    "from src.models.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2847a02-a00c-409b-870d-888443383c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf99a9-e451-4803-944b-b773ed8f4ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2c23fc-7619-40cb-b73b-c8e8b04439b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SHAPE = (3, 64, 64)\n",
    "dataset = TensorDataset(torch.load(f\"../datasets/kali-uint8-{SHAPE[-2]}x{SHAPE[-1]}.pt\")[:1000])\n",
    "dataset = TransformDataset(dataset, dtype=torch.float, multiply=1./255.)\n",
    "print(len(dataset))\n",
    "VF.to_pil_image(make_grid_labeled(\n",
    "    [i[0] for i, _ in zip(dataset, range(8*8))]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d71711e-763d-4481-9a19-05de614e47dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55e3261-beeb-49a7-bde7-5f794dcf6d91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.channels = [SHAPE[0], 50, 50, 20]\n",
    "        self.layers = nn.Sequential()\n",
    "        for i, (channels, next_channels) in enumerate(zip(self.channels, self.channels[1:])):\n",
    "            self.layers.append(nn.Conv2d(\n",
    "                in_channels=channels,\n",
    "                out_channels=next_channels,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "            ))\n",
    "        self.layers.append(nn.MaxPool2d(\n",
    "            kernel_size=11,\n",
    "            return_indices=False,\n",
    "        ))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x).flatten(1)\n",
    "\n",
    "encoder = Encoder()\n",
    "print(\"params:\", num_module_parameters(encoder))\n",
    "print(\"output:\", encoder(dataset[0][0].unsqueeze(0)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa2512-1927-4abe-beb8-8011eb077192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sinus(nn.Module):\n",
    "    def __init__(self, size: int, freq_scale: float = 3.):\n",
    "        super().__init__()\n",
    "        self.freq = nn.Parameter(torch.randn(size) * freq_scale)\n",
    "        self.phase = nn.Parameter(torch.randn(size) * 3.)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        freq = self.freq\n",
    "        phase = self.phase\n",
    "        return torch.sin(x * freq + phase)\n",
    "\n",
    "class Decoder(FreescaleImageModule):\n",
    "\n",
    "    def __init__(self, code_size: int = 100):\n",
    "        super().__init__(num_in=code_size)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(code_size + 2, code_size),\n",
    "            Sinus(code_size, 30),\n",
    "            nn.Linear(code_size, code_size),\n",
    "            Sinus(code_size, 20),\n",
    "            nn.Linear(code_size, 3),\n",
    "            nn.Linear(3, 3),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward_state(self, x: torch.Tensor, shape: Tuple[int, int, int]) -> torch.Tensor:\n",
    "        return self.layers(x)\n",
    "\n",
    "    \n",
    "decoder = Decoder(100)\n",
    "print(f\"params: {num_module_parameters(decoder):,}\")\n",
    "#print(\"output:\", decoder(torch.randn(2, 1), SHAPE).shape)\n",
    "code = torch.randn(1, 100)\n",
    "display(VF.to_pil_image(VF.resize(decoder(code, (3, 32, 32))[0], (256, 256), VF.InterpolationMode.NEAREST)))\n",
    "display(VF.to_pil_image(VF.resize(decoder(code, (3, 128, 128))[0], (256, 256), VF.InterpolationMode.NEAREST)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe125ec-38e9-4108-8cb8-dbd44b99db7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualLinearBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_hidden: int,\n",
    "            num_layers: int,\n",
    "            batch_norm: bool = True,\n",
    "            concat: bool = False,\n",
    "            activation: Union[str, Callable, nn.Module] = \"relu6\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.do_concat = concat\n",
    "        self.layers = nn.Sequential(OrderedDict([\n",
    "            *(\n",
    "                ((\"norm\", nn.BatchNorm1d(num_hidden)), ) if batch_norm else tuple()\n",
    "            ),\n",
    "            *(\n",
    "                (f\"layer_{i + 1}\", nn.Sequential(OrderedDict([\n",
    "                    (\"linear\", nn.Linear(num_hidden, num_hidden)),\n",
    "                    (\"act\", activation_to_module(activation)),\n",
    "                ])))\n",
    "                for i in range(num_layers)\n",
    "            )\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.do_concat:\n",
    "            return torch.concat([x, self.layers(x)], dim=-1)\n",
    "        else:\n",
    "            return x + self.layers(x)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"concat=True\" if self.do_concat else \"\"\n",
    "\n",
    "    \n",
    "class ImageManifoldDecoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_input_channels: int,\n",
    "            num_output_channels: int = 3,\n",
    "            num_hidden: int = 256,\n",
    "            num_blocks: int = 2,\n",
    "            num_layers_per_block: int = 2,\n",
    "            concat_residual: Union[bool, Iterable[bool]] = False,\n",
    "            pos_embedding_freqs: Iterable[float] = (7, 17),\n",
    "            batch_norm: bool = True,\n",
    "            default_shape: Optional[Tuple[int, int]] = None,\n",
    "            activation: Union[str, Callable, nn.Module] = \"gelu\",\n",
    "            activation_out: Union[str, Callable, nn.Module] = \"sigmoid\",\n",
    "            cross_attention: bool = True,\n",
    "            cross_attention_heads: int = 8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_input_channels = num_input_channels\n",
    "        self.default_shape = default_shape\n",
    "        self.num_output_channels = num_output_channels\n",
    "        self.pos_embedding_freqs = tuple(pos_embedding_freqs)\n",
    "        # x, y, sin-x, sin-y, cos-x, cos-y, ...\n",
    "        self.pos_embedding_size = (len(self.pos_embedding_freqs) * 2 + 1) * 2\n",
    "        if isinstance(concat_residual, bool):\n",
    "            self.concat_residual = (concat_residual, ) * num_blocks\n",
    "        else:\n",
    "            self.concat_residual = tuple(concat_residual)\n",
    "            if len(concat_residual) != num_blocks:\n",
    "                raise ValueError(f\"len(concat_residual) must be {num_blocks}, got {len(self.concat_residual)}\")\n",
    "\n",
    "        hidden_sizes = [num_hidden]\n",
    "        hs = num_hidden\n",
    "        for i, concat in enumerate(self.concat_residual):\n",
    "            if concat:\n",
    "                hs *= 2\n",
    "            hidden_sizes.append(hs)\n",
    "\n",
    "        if not cross_attention:\n",
    "            self.pos_to_color = nn.Sequential()\n",
    "            self.pos_to_color.add_module(\"linear_in\", nn.Linear(num_input_channels + self.pos_embedding_size, hidden_sizes[0]))\n",
    "            self.pos_to_color.add_module(\"act_in\", activation_to_module(activation))\n",
    "        else:\n",
    "            print(hidden_sizes)\n",
    "            self.upscale_pos = nn.Linear(self.pos_embedding_size, self.num_input_channels)\n",
    "            self.cross_atn = nn.MultiheadAttention(self.num_input_channels, num_heads=cross_attention_heads)\n",
    "            self.proj = nn.Linear(self.num_input_channels, hidden_sizes[0])\n",
    "            self.pos_to_color = nn.Sequential()\n",
    "            \n",
    "        self.pos_to_color.add_module(\"resblocks\", nn.Sequential(OrderedDict([\n",
    "            (\n",
    "                f\"resblock_{i+1}\",\n",
    "                ResidualLinearBlock(\n",
    "                    num_hidden=hs,\n",
    "                    num_layers=num_layers_per_block,\n",
    "                    batch_norm=batch_norm,\n",
    "                    concat=concat,\n",
    "                    activation=activation,\n",
    "                )\n",
    "            )\n",
    "            for i, (concat, hs) in enumerate(zip(self.concat_residual, hidden_sizes))\n",
    "        ])))\n",
    "        self.pos_to_color.add_module(\"linear_out\", nn.Linear(hidden_sizes[-1], num_output_channels))\n",
    "        self.pos_to_color.add_module(\"act_out\", activation_to_module(activation_out))\n",
    "        \n",
    "        self._cur_space = None\n",
    "        self._cur_space_shape = None\n",
    "\n",
    "    def extra_repr(self):\n",
    "        args = [\n",
    "            f\"pos_embedding_freqs={self.pos_embedding_freqs}\",\n",
    "            f\"concat_residual={self.concat_residual}\",\n",
    "        ]\n",
    "        if self.default_shape is not None:\n",
    "            args.append(f\"default_shape={self.default_shape}\")\n",
    "        return \", \".join(args)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, shape: Optional[Tuple[int, int]] = None) -> torch.Tensor:\n",
    "        if x.ndim not in (1, 2):\n",
    "            raise ValueError(f\"Expecting ndim 1 or 2, got {x.shape}\")\n",
    "\n",
    "        if x.ndim == 2:\n",
    "            return torch.concat([\n",
    "                self.forward(x_i, shape).unsqueeze(0)\n",
    "                for x_i in x\n",
    "            ])\n",
    "\n",
    "        space, shape = self.get_pos_embedding(shape)\n",
    "        input_codes = x.unsqueeze(0).expand(space.shape[0], x.shape[-1])\n",
    "        \n",
    "        if getattr(self, \"cross_atn\", None) is None:\n",
    "            codes = torch.concat([\n",
    "                input_codes,\n",
    "                space\n",
    "            ], dim=1)\n",
    "            color = self.pos_to_color(codes)\n",
    "        else:\n",
    "            embedding = self.upscale_pos(space)\n",
    "            print(x.shape, input_codes.shape, embedding.shape)\n",
    "            codes, code_weights = self.cross_atn(\n",
    "                query=embedding,\n",
    "                key=input_codes,\n",
    "                value=input_codes,\n",
    "            )\n",
    "            codes = self.proj(codes)\n",
    "            color = self.pos_to_color(codes)\n",
    "            \n",
    "        return color.permute(1, 0).view(self.num_output_channels, *shape)\n",
    "\n",
    "    def get_pos_embedding(self, shape: Optional[Tuple[int, int]] = None):\n",
    "        if shape is None:\n",
    "            shape = self.default_shape\n",
    "        if shape is None:\n",
    "            raise ValueError(\"Must either define `default_shape` or `shape`\")\n",
    "\n",
    "        if shape != self._cur_space_shape:\n",
    "            space = Space2d(shape=(2, *shape)).space().to(self.pos_to_color[-2].weight)\n",
    "            space = space.permute(1, 2, 0).view(-1, 2)\n",
    "            space = torch.concat([\n",
    "                space,\n",
    "                *(\n",
    "                    (space * freq).sin()\n",
    "                    for freq in self.pos_embedding_freqs\n",
    "                ),\n",
    "                *(\n",
    "                    (space * freq).cos()\n",
    "                    for freq in self.pos_embedding_freqs\n",
    "                )\n",
    "            ], 1)\n",
    "            self._cur_space = space.to(self.pos_to_color[-2].weight)\n",
    "            self._cur_space_shape = shape\n",
    "\n",
    "        return self._cur_space, shape\n",
    "\n",
    "    def weight_images(self, **kwargs):\n",
    "        images = []\n",
    "        for i, p in enumerate(self.parameters()):\n",
    "            if p.ndim == 2 and any(s > 1 for s in p.shape):\n",
    "                images.append(p)\n",
    "\n",
    "        return images\n",
    "    \n",
    "    \n",
    "    \n",
    "CODE_SIZE = 128\n",
    "decoder = ImageManifoldDecoder(\n",
    "    CODE_SIZE, num_blocks=8, num_layers_per_block=2, default_shape=SHAPE[-2:],\n",
    "    concat_residual=[False, True] * 4, num_hidden=64, cross_attention=True,\n",
    ")\n",
    "print(f\"params {num_module_parameters(decoder):,}\")\n",
    "if 1:\n",
    "    with torch.no_grad():\n",
    "        images = decoder(torch.randn(8, CODE_SIZE), (64, 64))\n",
    "        print(images.shape)\n",
    "        #print(images)\n",
    "        display(VF.to_pil_image(make_grid(images, normalize=True)))\n",
    "decoder#.weight_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ac5e7-c187-4326-9e5d-628efad9b194",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn.MultiheadAttention.forward?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4df58-dd17-4168-86b4-8faac0b2bce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    data = torch.randn(64, decoder.num_input_channels)\n",
    "    start_time = time.time()\n",
    "    for _ in tqdm(range(5)):\n",
    "        decoder(data)\n",
    "    seconds = time.time() - start_time\n",
    "    print(f\"inference rate {64 * 5 / seconds:,.2f}/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f5e0bf-c592-42d9-ad12-a1cc5b3e95f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# training test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648425e9-a2b1-421f-afd5-c739b11fd59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "def train_test(\n",
    "    decoder: nn.Module,\n",
    "    codes: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    iters: int = 10000,\n",
    "    device=\"auto\",\n",
    "    lr=0.0001,\n",
    "):    \n",
    "    device = to_torch_device(device)\n",
    "    print(device)\n",
    "    \n",
    "    decoder = decoder.to(device)\n",
    "    targets = targets.to(device)\n",
    "    codes = codes.to(device)\n",
    "    \n",
    "    #optimizer = torch.optim.Adadelta(decoder.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "    #optimizer = torch.optim.AdamW(decoder.parameters(), lr=lr)\n",
    "    #optimizer = torch.optim.RMSprop(decoder.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    last_print_time = time.time()\n",
    "    last_print_it = 0\n",
    "    try:\n",
    "        for it in tqdm(range(iters)):\n",
    "\n",
    "            pixels = decoder(codes).clamp(0, 1)\n",
    "            \n",
    "            loss = F.l1_loss(pixels, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #torch.nn.utils.clip_grad_value_(decoder.parameters(), .00000001)\n",
    "            grad = 0\n",
    "            #for i, p in enumerate(decoder.parameters()):\n",
    "            #    grad += float(p.grad.abs().mean())\n",
    "            #grad /= i + 1\n",
    "            #print(grad, end=\", \")\n",
    "            optimizer.step()\n",
    "            \n",
    "            cur_time = time.time()\n",
    "            if cur_time - last_print_time > 10 and it - last_print_it >= 300:\n",
    "                last_print_time = cur_time\n",
    "                last_print_it = it\n",
    "                print(f\"train loss {float(loss)}\")\n",
    "                display(VF.to_pil_image(make_grid(\n",
    "                    torch.concat([pixels, targets]), \n",
    "                    nrow=len(pixels))))\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    \n",
    "decoder = ImageManifoldDecoder(128, num_blocks=8, num_layers_per_block=2, num_hidden=256, default_shape=SHAPE[-2:])\n",
    "print(f\"params {num_module_parameters(decoder):,}\")\n",
    "\n",
    "train_test(\n",
    "    decoder,\n",
    "    torch.randn(8, 128),\n",
    "    torch.concat([dataset[16 + i][0].unsqueeze(0) for i in range(8)]),\n",
    "    lr=.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c31eb1-b6e8-4ab0-a5a6-0e1ed7d26464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    display(VF.to_pil_image(make_grid(decoder(torch.randn(8*4, CODE_SIZE).cuda()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3cbe31-0be7-421f-950b-2a7c75da16d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder = ImageManifoldDecoder(128, num_blocks=2, num_layers_per_block=2, num_hidden=256, default_shape=SHAPE[-2:])\n",
    "print(f\"params {num_module_parameters(decoder):,}\")\n",
    "\n",
    "train_test(\n",
    "    decoder,\n",
    "    torch.randn(8, 128),\n",
    "    torch.concat([dataset[16 + i][0].unsqueeze(0) for i in range(8)]),\n",
    "    lr=.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f542965-68eb-45e6-82cf-c06e61a1de9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder = ImageManifoldDecoder(128, num_blocks=4, num_layers_per_block=3, num_hidden=dden=256, default_shape=SHAPE[-2:])\n",
    "print(f\"params {num_module_parameters(decoder):,}\")\n",
    "\n",
    "train_test(\n",
    "    decoder,\n",
    "    torch.randn(8, 128),\n",
    "    torch.concat([dataset[16 + i][0].unsqueeze(0) for i in range(8)]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e703dad-3534-49ed-acc1-4de831f008de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeacfe5-b79f-49cf-bb39-3c091f2a26d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79bb2521-25fe-4617-8b58-de4045f8739b",
   "metadata": {},
   "source": [
    "# encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aeed32-4eda-44eb-89a5-6b819f60ac98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageManifoldEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_output_channels: int,\n",
    "            num_input_channels: int = 3,\n",
    "            num_hidden: int = 256,\n",
    "            num_blocks: int = 2,\n",
    "            num_layers_per_block: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_input_channels = num_input_channels\n",
    "        self.num_output_channels = num_output_channels\n",
    "        self.color_to_pos = nn.Sequential(OrderedDict([\n",
    "            (\"linear_in\", nn.Linear(num_input_channels + 2, num_hidden)),\n",
    "            (\"act_in\", nn.GELU()),\n",
    "            (\"resblocks\", nn.Sequential(OrderedDict([\n",
    "                (f\"resblock_{i+1}\", ResidualLinearBlock(num_hidden=num_hidden, num_layers=num_layers_per_block))\n",
    "                for i in range(num_blocks)\n",
    "            ]))),\n",
    "            (\"linear_out\", nn.Linear(num_hidden, num_output_channels)),\n",
    "            (\"act_out\", nn.Sigmoid()),\n",
    "        ]))\n",
    "        self._cur_space = None\n",
    "        self._cur_space_shape = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.ndim not in (3, 4):\n",
    "            raise ValueError(f\"Expecting ndim 3 or 4, got {x.shape}\")\n",
    "\n",
    "        if x.ndim == 4:\n",
    "            return torch.concat([\n",
    "                self.forward(x_i, shape).unsqueeze(0)\n",
    "                for x_i in x\n",
    "            ])\n",
    "\n",
    "        space = self.get_space(x.shape[-2:])\n",
    "        codes = torch.concat([x, space], dim=0).flatten(1).permute(1, 0)\n",
    "        return self.color_to_pos(codes).permute(1, 0).mean(-1)\n",
    "\n",
    "    def get_space(self, shape: Tuple[int, int] = None):\n",
    "        if shape != self._cur_space_shape:\n",
    "            space = Space2d(shape=(2, *shape)).space().to(self.color_to_pos[0].weight)\n",
    "            self._cur_space = space\n",
    "            self._cur_space_shape = shape\n",
    "\n",
    "        return self._cur_space\n",
    "\n",
    "    def weight_images(self, **kwargs):\n",
    "        images = []\n",
    "        for i, p in enumerate(self.parameters()):\n",
    "            if p.ndim == 2:\n",
    "                images.append(p)\n",
    "\n",
    "        return images\n",
    "\n",
    "encoder = ImageManifoldEncoder(128)\n",
    "out = encoder(torch.randn(3, 64, 64))\n",
    "print(\"out\", out.shape)\n",
    "print(out)\n",
    "encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd62b99-8499-4c1f-a8bf-376ed9bfda03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3c9d0c-eae9-403f-989d-ef9d8b9b166e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62133148-b588-4e07-a841-bd9880c8d8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e7d78-00dd-4ab6-a43a-235f264064c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87cbde8-b9a2-435d-9b57-1acd334348e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bdf852-6d20-476a-b060-6b503cbbe02c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state = torch.load(\"../checkpoints/ae-manifold-7/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b4294c-0391-4c2b-85aa-74d4cf357ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder = ImageManifoldDecoder(\n",
    "    128, 1, num_blocks=8, num_layers_per_block=2, num_hidden=256, default_shape=(64, 64),\n",
    ")\n",
    "decoder.load_state_dict({\n",
    "    key[8:]: value\n",
    "    for key, value in state[\"state_dict\"].items()\n",
    "    if key.startswith(\"decoder.\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1190b8-19e8-431f-802f-a5832f944aaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    display(VF.to_pil_image(make_grid(decoder(.5 * torch.randn(8*2, CODE_SIZE)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e50780-a622-4d97-be64-eb45a5311ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc0878-9006-4ed3-a0a8-c688f211679e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98d1076a-3252-4979-bc72-01444a33a511",
   "metadata": {},
   "source": [
    "# combine several decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee8a471-0eaa-483a-a66c-96a80c7fc725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderEnsemble(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            *decoders: nn.Module,\n",
    "            weights: Union[None, Iterable[float], torch.Tensor] = None,\n",
    "            train_weights: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.decoders = nn.ModuleDict({\n",
    "            f\"decoder_{i + 1}\": decoder\n",
    "            for i, decoder in enumerate(decoders)\n",
    "        })\n",
    "        \n",
    "        if weights is None:\n",
    "            weights = torch.ones(len(self.decoders)) / len(self.decoders)\n",
    "        elif isinstance(weights, torch.Tensor):\n",
    "            pass\n",
    "        else:\n",
    "            weights = torch.Tensor(weights)\n",
    "        self.weights = nn.Parameter(weights, requires_grad=train_weights)\n",
    "        \n",
    "    def forward(self, *args, **kwargs):\n",
    "        output_sum = None\n",
    "        for i, decoder in enumerate(self.decoders.values()):\n",
    "            output = decoder(*args, **kwargs) * self.weights[i]\n",
    "            \n",
    "            if output_sum is None:\n",
    "                output_sum = output\n",
    "            else:\n",
    "                output_sum = output_sum + output\n",
    "                \n",
    "        return output_sum\n",
    "    \n",
    "dec = DecoderEnsemble(\n",
    "    ImageManifoldDecoder(num_input_channels=100, num_output_channels=1, default_shape=(64, 64), num_hidden=32),\n",
    "    ImageManifoldDecoder(num_input_channels=100, num_output_channels=1, default_shape=(64, 64), num_hidden=256)\n",
    ")\n",
    "output = dec(torch.rand(8, 100))\n",
    "print(output.shape)\n",
    "VF.to_pil_image(make_grid(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9607dac9-d4b1-4198-a50b-cd8f44a50b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
