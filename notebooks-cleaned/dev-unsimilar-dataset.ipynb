{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d9249-e9ca-4efe-81af-dbf534c86518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union, Dict\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset, RandomSampler\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.algo import GreedyLibrary\n",
    "from src.util.image import *\n",
    "from src.util import to_torch_device\n",
    "from src.patchdb import PatchDB\n",
    "from src.models.encoder import EncoderConv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417e63d-de1b-4986-9c62-a0a5f3d91ee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_samples(ds, count=30*30, nrow=30, batch_size=100):\n",
    "    cur_count = 0\n",
    "    batches = []\n",
    "    try:\n",
    "        for batch in DataLoader(ds, batch_size=batch_size):\n",
    "            if isinstance(batch, (list, tuple)):\n",
    "                batch = batch[0]\n",
    "            batches.append(batch)\n",
    "            cur_count += batch.shape[0]\n",
    "            if cur_count >= count:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    if not batches:\n",
    "        return \"nothin'\"\n",
    "    batch = torch.concat(batches)[:count]\n",
    "        \n",
    "    return VF.to_pil_image(make_grid(batch, nrow=nrow))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b4225-e755-454b-91a0-308fdaf1a6e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SHAPE = [1, 32, 32]\n",
    "\n",
    "def _stride(shape: Tuple[int, int]):\n",
    "    # print(shape)\n",
    "    size = min(shape)\n",
    "    if size <= 512:\n",
    "        return 5\n",
    "    \n",
    "    return SHAPE[-2:]\n",
    "\n",
    "ds_crop = make_image_patch_dataset(\n",
    "    path=\"~/Pictures/photos\", recursive=True,\n",
    "    shape=SHAPE,\n",
    "    scales=[1./12., 1./6, 1./3, 1.],\n",
    "    stride=_stride,\n",
    "    interleave_images=4,\n",
    "    #image_shuffle=5,\n",
    "    #transforms=[lambda x: VF.resize(x, tuple(s // 6 for s in x.shape[-2:]))], stride=5,\n",
    "    with_pos=True,\n",
    "    with_filename=True,\n",
    ")\n",
    "#ds_crop = IterableImageFilterDataset(ds_crop, ImageFilter(min_std=0.03)) \n",
    "\n",
    "plot_samples(ds_crop, count=30*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6301f3-99e4-40c3-8d30-f20d76cb1519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d994a9a-d23e-4177-b519-28bc12afbaab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DissimilarImageIterableDatasetLOCAL(IterableDataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset: Union[IterableDataset, Dataset],\n",
    "            max_similarity: float = .9,\n",
    "            max_age: Optional[int] = None,\n",
    "            encoder: Union[str, torch.nn.Module, Callable[[torch.Tensor], torch.Tensor]] = \"flatten\",\n",
    "            batch_size: int = 10,\n",
    "            verbose: bool = False,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.max_similarity = float(max_similarity)\n",
    "        self.max_age = max_age\n",
    "        self.encoder = encoder\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.verbose = bool(verbose)\n",
    "        self.features: Optional[torch.Tensor] = None\n",
    "        self._ages: Optional[List[int]] = None\n",
    "        self._age = 0\n",
    "\n",
    "    def __iter__(self) -> Generator[Union[torch.Tensor, Tuple[torch.Tensor, ...]], None, None]:\n",
    "        self.features = None\n",
    "        self._ages = None\n",
    "        self._age = 0\n",
    "        self._num_passed = 0\n",
    "        image_batch = []\n",
    "        tuple_batch = []\n",
    "\n",
    "        def _process(data):\n",
    "\n",
    "            is_tuple = isinstance(data, (tuple, list))\n",
    "            if is_tuple:\n",
    "                image_batch.append(data[0])\n",
    "                tuple_batch.append(data[1:])\n",
    "            else:\n",
    "                image_batch.append(data)\n",
    "                tuple_batch.append(None)\n",
    "\n",
    "            if len(image_batch) >= self.batch_size:\n",
    "                yield from self._process_batch(image_batch, tuple_batch)\n",
    "                image_batch.clear()\n",
    "                tuple_batch.clear()\n",
    "\n",
    "            self._age += 1\n",
    "\n",
    "            self._drop_old_features()\n",
    "\n",
    "        if not self.verbose:\n",
    "            for data in self.dataset:\n",
    "                yield from _process(data)\n",
    "\n",
    "        else:\n",
    "            from tqdm import tqdm\n",
    "\n",
    "            try:\n",
    "                total = len(self.dataset)\n",
    "            except:\n",
    "                total = None\n",
    "\n",
    "            with tqdm(total=total) as progress:\n",
    "                for data in self.dataset:\n",
    "                    yield from _process(data)\n",
    "\n",
    "                    progress.desc = (\n",
    "                        f\"filtering unsimilar images (features={self.features.shape[0] if self.features is not None else 0}\"\n",
    "                        f\", passed={self._num_passed})\"\n",
    "                    )\n",
    "                    progress.update(1)\n",
    "\n",
    "        if image_batch:\n",
    "            yield from self._process_batch(image_batch, tuple_batch)\n",
    "\n",
    "    def _process_batch(self, image_batch, tuple_batch):\n",
    "        image_batch = torch.concat([i.unsqueeze(0) for i in image_batch])\n",
    "        feature_batch = self._encode(image_batch)\n",
    "\n",
    "        # store first image feature\n",
    "        if self.features is None:\n",
    "            if tuple_batch[0]:\n",
    "                yield image_batch[0], *tuple_batch[0]\n",
    "            else:\n",
    "                yield image_batch[0]\n",
    "\n",
    "            self._num_passed += 1\n",
    "            self.features = feature_batch[0].unsqueeze(0)\n",
    "            self._ages = [self._age]\n",
    "            image_batch = image_batch[1:]\n",
    "            tuple_batch = tuple_batch[1:]\n",
    "            feature_batch = feature_batch[1:]\n",
    "\n",
    "        similarities = self._highest_similarities(feature_batch)\n",
    "\n",
    "        features_to_add = None\n",
    "        for image, tup, feature, similarity in zip(image_batch, tuple_batch, feature_batch, similarities):\n",
    "\n",
    "            if features_to_add is not None:\n",
    "                # get highest similarity with stored features and new features from batch\n",
    "                similarity2 = self._highest_similarities(feature.unsqueeze(0), features_to_add)\n",
    "                similarity = torch.max(similarity, similarity2)\n",
    "\n",
    "            if similarity <= self.max_similarity:\n",
    "                if tup:\n",
    "                    yield image, *tup\n",
    "                else:\n",
    "                    yield image\n",
    "\n",
    "                self._num_passed += 1\n",
    "                self._ages.append(self._age)\n",
    "                if features_to_add is None:\n",
    "                    features_to_add = feature.unsqueeze(0)\n",
    "                else:\n",
    "                    features_to_add = torch.concat([features_to_add, feature.unsqueeze(0)])\n",
    "\n",
    "        if features_to_add is not None:\n",
    "            self.features = torch.concat([self.features, features_to_add])\n",
    "\n",
    "    def _drop_old_features(self):\n",
    "        if self.max_age is not None and self._ages:\n",
    "            idx = None\n",
    "            for i, age in enumerate(self._ages):\n",
    "                if self._age - age > self.max_age:\n",
    "                    idx = i + 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            if idx is not None and idx < len(self._ages):\n",
    "                self.features = self.features[idx:]\n",
    "                self._ages = self._ages[idx:]\n",
    "\n",
    "    def _highest_similarities(self, feature_batch: torch.Tensor, features: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        sim = feature_batch @ (features if features is not None else self.features).T\n",
    "        return sim.max(dim=1)[0]\n",
    "\n",
    "    def _encode(self, image_batch: torch.Tensor) -> torch.Tensor:\n",
    "        if isinstance(self.encoder, str):\n",
    "\n",
    "            if self.encoder == \"flatten\":\n",
    "                feature_batch = image_batch.flatten(1)\n",
    "\n",
    "            elif self.encoder.startswith(\"clip\"):\n",
    "                from src.models.clip import ClipSingleton\n",
    "                feature_batch = ClipSingleton.encode_image(image_batch)\n",
    "\n",
    "            elif self.encoder.startswith(\"encoderconv:\"):\n",
    "                from src.models.encoder import EncoderConv2d\n",
    "                if not hasattr(self, \"_encoderconv\"):\n",
    "                    self._encoderconv = EncoderConv2d.from_torch(self.encoder.split(\":\", 1)[-1])\n",
    "                feature_batch = self._encoderconv.encode_image(image_batch)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported encoder '{self.encoder}', expected 'flatten', 'clip'\")\n",
    "\n",
    "        elif callable(self.encoder):\n",
    "            feature_batch = self.encoder(image_batch)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported encoder type {type(self.encoder).__name__}, expected str or callable\")\n",
    "\n",
    "        # feature_batch = feature_batch - feature_batch.mean(dim=1, keepdim=True)\n",
    "        return feature_batch / torch.norm(feature_batch, dim=1, keepdim=True)\n",
    " \n",
    "    \n",
    "ds_unique = DissimilarImageIterableDataset(ds_crop, max_similarity=0.999, max_age=200_000, verbose=True)\n",
    "plot_samples(ds_unique, count=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560118c5-753f-45a3-98fe-0728a2c65176",
   "metadata": {},
   "source": [
    "# build PatchDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68dd18-d68e-4d67-8fac-8f7150c9b96d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder = EncoderConv2d.from_torch(\"../models/encoderconv/encoder-1x32x32-128-photo-5.pt\", device=\"cpu\")\n",
    "encoder.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684cb4df-e6eb-4b1f-904b-ad89a04233ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = PatchDB(\"../db/photos-1x32x32.patchdb\")\n",
    "\n",
    "db.clear()\n",
    "\n",
    "count = 0\n",
    "last_count = 0\n",
    "try:\n",
    "    with db:\n",
    "        for patches, positions, filenames in DataLoader(ds_unique, batch_size=64):\n",
    "            embeddings = encoder(patches)\n",
    "            embeddings = embeddings / embeddings.norm(dim=1, keepdim=True)\n",
    "            # embeddings = torch.round(embeddings, decimals=5)\n",
    "            for embedding, pos, filename in zip(embeddings, positions, filenames):\n",
    "                rect = pos.tolist() + list(patches[0].shape[-2:])\n",
    "                db.add_patch(filename, rect, embedding)\n",
    "                count += 1\n",
    "\n",
    "            if count - last_count > 50_000:\n",
    "                last_count = count\n",
    "                db.flush()\n",
    "                print(f\"{db.size_bytes():,} bytes\")\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042b684d-901a-4612-a1cf-cbbcb85ba1c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "with gzip.open(db.filename, \"rt\") as fp:\n",
    "    print(fp.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51383e5-2e33-4d2c-896f-f2675591708c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "math.prod(embedding.shape) * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d5a7b7-3b53-43fb-8d32-3e5840f4ac2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "a = embedding.detach().numpy()\n",
    "print(len(base64.b64encode(a.data)))\n",
    "len(json.dumps(a.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33591e5d-d8ea-4134-a057-3aef0e343af3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e2c6b-bde9-45ae-a959-e5d71aca36e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_unique = ds_crop#DissimilarImageIterableDataset(ds_crop, max_similarity=.99, max_age=200_000, verbose=False)\n",
    "ds_unique = DissimilarImageIterableDataset(ds_unique, max_similarity=.4, max_age=200_000, verbose=True, \n",
    "                                           encoder=\"encoderconv:../models/encoderconv/encoder-1x32x32-128-small-photo-3.pt\")\n",
    "plot_samples(ds_unique, count=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5510e4a2-6ffa-4cf0-8607-a6859f3b994f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "32*32\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
