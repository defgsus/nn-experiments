{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3a1ae4-d2dc-4f09-a9f6-3c4ee653ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Optional, Union\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util import *\n",
    "from src.util.image import * \n",
    "from src.algo import Space2d, IFS\n",
    "from src.datasets import *\n",
    "from src.models.util import *\n",
    "from src.models.cnn import *\n",
    "from src.models.encoder import *\n",
    "from src.models.decoder import *\n",
    "from src.models.transform import *\n",
    "from src.util.embedding import *\n",
    "\n",
    "def resize(img, scale: float, aa: bool = False):\n",
    "    return VF.resize(img, [max(1, int(s * scale)) for s in img.shape[-2:]], VF.InterpolationMode.BILINEAR if aa else VF.InterpolationMode.NEAREST, antialias=aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d0077c-e25c-4d72-a89b-01b811aef54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymmetricLinear(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            activation: Union[None, str, nn.Module] = None,     \n",
    "            #space_to_depth: bool = False,\n",
    "            #dropout: float = 0.,\n",
    "            #batch_norm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = activation_to_callable(activation)\n",
    "        self.linear = nn.Linear(in_channels, out_channels)\n",
    "        self.bias_out = nn.Parameter(\n",
    "            torch.randn(in_channels) * self.linear.bias.std()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, transpose: bool = False):\n",
    "        if not transpose:\n",
    "            y = self.linear(x)\n",
    "            if self.activation:\n",
    "                y = self.activation(y)\n",
    "        else:\n",
    "            y = F.linear(x, self.linear.weight.T, self.bias_out)\n",
    "            if self.activation:\n",
    "                y = self.activation(y)\n",
    "        return y\n",
    "\n",
    "model = SymmetricLinear(3, 16)\n",
    "print(f\"params: {num_module_parameters(model):,}\")\n",
    "print(model)\n",
    "print(model(torch.rand(1, 3)).shape)\n",
    "print(model(torch.rand(1, 16), transpose=True).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb0b3a-533e-4015-a062-ef131f7d8e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymmetricConv2d(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            kernel_size: int = 3,\n",
    "            stride: int = 1,\n",
    "            groups: int = 1,\n",
    "            activation: Union[None, str, nn.Module] = \"leaky_relu\",     \n",
    "            #space_to_depth: bool = False,\n",
    "            #dropout: float = 0.,\n",
    "            #batch_norm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.groups = groups\n",
    "        self.activation = activation_to_callable(activation)\n",
    "        self.conv_in = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "        self.conv_out = nn.ConvTranspose2d(out_channels, in_channels, kernel_size, stride, output_padding=stride - 1)\n",
    "        # make weights symmetric\n",
    "        self.conv_out.weight = self.conv_in.weight\n",
    "        #self.conv_out.bias = nn.Parameter(torch.randn(in_channels)\n",
    "\n",
    "    def forward(self, x, transpose: bool = False):\n",
    "        if not transpose:\n",
    "            y = self.conv_in(x)\n",
    "            if self.activation:\n",
    "                y = self.activation(y)\n",
    "        else:\n",
    "            y = self.conv_out(x)\n",
    "            if self.activation:\n",
    "                y = self.activation(y)\n",
    "        return y\n",
    "\n",
    "model = SymmetricConv2d(3, 16)\n",
    "print(f\"params: {num_module_parameters(model):,}\")\n",
    "print(model)\n",
    "print(model(torch.rand(1, 3, 10, 10)).shape)\n",
    "print(model(torch.rand(1, 16, 8, 8), transpose=True).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b160a-49bc-40b7-987c-223c915dba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedSymmetricAutoencoderConv2d(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            shape: Tuple[int, int, int],\n",
    "            code_size: int,\n",
    "            kernel_size: Union[int, Iterable[int]] = 3,\n",
    "            stride: Union[int, Iterable[int]] = 1,\n",
    "            groups: int = 1,\n",
    "            channels: Iterable[int] = (16, 32),\n",
    "            activation: Union[None, str, nn.Module] = \"leaky_relu\",\n",
    "            space_to_depth: bool = False,\n",
    "            dropout: float = 0.,\n",
    "            batch_norm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.shape = tuple(shape)\n",
    "        self.code_size = code_size\n",
    "        self.channels = tuple(channels)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.groups = groups\n",
    "        self.activation = activation_to_callable(activation)\n",
    "        self.layer_index = len(self.channels) + 1\n",
    "        \n",
    "        channels = [self.shape[0], *self.channels]\n",
    "        self.layers = nn.Sequential()\n",
    "        for idx, ch in enumerate(channels[:-1]):\n",
    "            ch_in = ch\n",
    "            ch_out = channels[idx + 1]\n",
    "            if isinstance(self.kernel_size, int):\n",
    "                ks = self.kernel_size\n",
    "            else:\n",
    "                ks = self.kernel_size[idx]\n",
    "            if isinstance(self.stride, int):\n",
    "                stride = self.stride\n",
    "            else:\n",
    "                stride = self.stride[idx]\n",
    "\n",
    "            self.layers.add_module(f\"conv_{idx + 1}\", SymmetricConv2d(\n",
    "                ch_in, ch_out, \n",
    "                kernel_size=kernel_size, \n",
    "                stride=stride, \n",
    "                activation=activation,\n",
    "            ))\n",
    "            \n",
    "        self.conv_shapes = []\n",
    "        with torch.no_grad():\n",
    "            data = torch.zeros(1, *self.shape)\n",
    "            for conv in self.layers:\n",
    "                data = conv(data)\n",
    "                self.conv_shapes.append(data.shape[-3:])\n",
    "        # print(self.conv_shapes)\n",
    "\n",
    "        self.layers.add_module(\"linear\", SymmetricLinear(\n",
    "            math.prod(self.conv_shapes[-1]), self.code_size,\n",
    "        ))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.encode(x)\n",
    "        # print(f\"code: {y.shape}\")\n",
    "        x = self.decode(y)\n",
    "        return x\n",
    "\n",
    "    def encode(self, x):\n",
    "        for idx, layer in zip(range(self.layer_index + 1), self.layers):\n",
    "            if idx == len(self.layers) - 1:\n",
    "                x = x.flatten(-3)\n",
    "            x = layer(x)\n",
    "            # print(\"Y\", x.shape)\n",
    "        if self.layer_index < len(self.layers) - 1:\n",
    "            x = x.flatten(-3)\n",
    "            \n",
    "        return x\n",
    "        \n",
    "    def decode(self, x):\n",
    "        if self.layer_index < len(self.conv_shapes) - 1:\n",
    "            shape = self.conv_shapes[self.layer_index]\n",
    "            x = x.view(*x.shape[:-1], *shape)\n",
    "            \n",
    "        for idx, layer in reversed(list(zip(range(self.layer_index + 1), self.layers))):\n",
    "            if idx == len(self.conv_shapes) - 1:\n",
    "                shape = self.conv_shapes[idx]\n",
    "                x = x.view(*x.shape[:-1], *shape)\n",
    "            x = layer(x, transpose=True)        \n",
    "        return x\n",
    "        \n",
    "model = StackedSymmetricAutoencoderConv2d(\n",
    "    (3, 64, 64), 100, \n",
    "    channels=(32, ) * 25,\n",
    "    #channels=(16, 16, 16, 16, 16, 16, 16), \n",
    "    #stride=  ( 1,  1,  2,  1,  2,  1,  1),\n",
    ")\n",
    "print(f\"params: {num_module_parameters(model):,}\")\n",
    "#print(model)\n",
    "#print(\"X\", model.forward(torch.rand(2, 3, 64, 64)).shape)\n",
    "\n",
    "for layer_index in range(len(model.layers)):\n",
    "    model.layer_index = layer_index\n",
    "    encoded = model.encode(torch.rand(2, 3, 64, 64))\n",
    "    output = model.decode(encoded)\n",
    "    print(f\"idx={layer_index}, {encoded.shape}, {output.shape}\")\n",
    "\n",
    "#print(output.shape)\n",
    "display(VF.to_pil_image(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13a0bb-e6bc-45aa-bdd3-2bcfa3af3440",
   "metadata": {},
   "outputs": [],
   "source": [
    "1_000_000 / 47_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2758d3-66ca-46c0-8860-50188d021eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ff436-bad8-4331-b12f-038902cd0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in (\n",
    "    b\"tbg-forum@web.de\",\n",
    "    b\"forum@tbg2024\",\n",
    "    b\"smtp.web.de\",\n",
    "):\n",
    "    print(base64.b64encode(k))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
