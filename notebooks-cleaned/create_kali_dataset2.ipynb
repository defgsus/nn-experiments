{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ed52b-36ad-43d0-99d3-236d6bb06640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import *\n",
    "from src.util import ImageFilter\n",
    "from src.algo import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e45f85-0ba8-4fec-ba81-8493cc9e008e",
   "metadata": {},
   "source": [
    "## p = abs(p) / dot(p, p) - v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb97dbd-779a-49c7-8e55-26a581220b9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = kali2d(\n",
    "    Space2d(\n",
    "        shape=(3, 128, 128), \n",
    "        offset=torch.Tensor([0.7,0,0]), \n",
    "        scale=.01,\n",
    "    ), \n",
    "    param=torch.Tensor([.75, .75, .75]),\n",
    "    iterations=21,\n",
    "    #out_weights=torch.rand((3, 3)),\n",
    "    accumulate=\"min\",\n",
    "    aa=10,\n",
    ")\n",
    "#img = VF.resize(img, [512, 512], interpolation=VF.InterpolationMode.BICUBIC)\n",
    "img = VF.resize(img, [512, 512], interpolation=VF.InterpolationMode.NEAREST)\n",
    "VF.to_pil_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552290e-59ed-46b3-a14c-bb0040777c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def make_grid_labeled(\n",
    "    tensor: Union[torch.Tensor, List[torch.Tensor]],\n",
    "    labels: Union[bool, Iterable[str]] = True,\n",
    "    nrow: int = 8,\n",
    "    padding: int = 2,\n",
    "    normalize: bool = False,\n",
    "    value_range: Optional[Tuple[int, int]] = None,\n",
    "    scale_each: bool = False,\n",
    "    pad_value: float = 0.0,\n",
    "    return_pil: bool = False,\n",
    "    **kwargs,\n",
    ") -> torch.Tensor:\n",
    "    grid = make_grid(\n",
    "        tensor=tensor, nrow=nrow, padding=padding, value_range=value_range, \n",
    "        scale_each=scale_each, pad_value=pad_value, \n",
    "        **kwargs,\n",
    "    )\n",
    "    \n",
    "    if labels:\n",
    "        if isinstance(tensor, (list, tuple)):\n",
    "            num_images = len(tensor)\n",
    "            shape = tensor[0].shape\n",
    "        else:\n",
    "            assert tensor.ndim == 4, f\"make_grid_labeled() only supports [N, C, H, W] shape, got '{tensor.shape}'\"\n",
    "            num_images = tensor.shape[0]\n",
    "            shape = tensor.shape[1:]\n",
    "\n",
    "        if labels is True:\n",
    "            labels = [str(i) for i in range(num_images)]\n",
    "        else:\n",
    "            labels = [str(i) for i in labels]\n",
    "        \n",
    "        grid_pil = VF.to_pil_image(grid)\n",
    "        draw = PIL.ImageDraw.ImageDraw(grid_pil)\n",
    "        \n",
    "        for idx, label in enumerate(labels):\n",
    "            x = padding + ((idx % nrow) * (shape[-1] + padding))\n",
    "            y = padding + ((idx // nrow) * (shape[-2] + padding))\n",
    "            draw.text((x-1, y), label, fill=(0, 0, 0))\n",
    "            draw.text((x+1, y), label, fill=(0, 0, 0))\n",
    "            draw.text((x, y-1), label, fill=(0, 0, 0))\n",
    "            draw.text((x, y+1), label, fill=(0, 0, 0))\n",
    "            draw.text((x, y), label, fill=(256, 256, 256))\n",
    "        \n",
    "        if return_pil:\n",
    "            return grid_pil\n",
    "        \n",
    "        grid = VF.to_tensor(grid_pil)\n",
    "        \n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b292f118-e065-4c79-afa7-bfc3b8df4739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_samples(\n",
    "        iterable, \n",
    "        total: int = 16, \n",
    "        nrow: int = 16, \n",
    "        return_image: bool = False, \n",
    "        show_compression_ratio: bool = False,\n",
    "        label: Optional[Callable] = None,\n",
    "):\n",
    "    samples = []\n",
    "    labels = []\n",
    "    f = ImageFilter()\n",
    "    try:\n",
    "        for image in tqdm(iterable, total=total):\n",
    "            samples.append(image)\n",
    "            if show_compression_ratio:\n",
    "                labels.append(round(f.calc_compression_ratio(image), 3))\n",
    "            elif label is not None:\n",
    "                labels.append(label(image))\n",
    "                \n",
    "            if len(samples) >= total:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    if labels:\n",
    "        image = VF.to_pil_image(make_grid_labeled(samples, nrow=nrow, labels=labels))\n",
    "    else:\n",
    "        image = VF.to_pil_image(make_grid(samples, nrow=nrow))\n",
    "    if return_image:\n",
    "        return image\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae915d9-f6cd-4cff-b64a-4f22f148fed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = Kali2dDataset(\n",
    "    (3, 64, 64), aa=2, size=1_000_000, \n",
    "    #accumulation_modes=[\"submin\", \"alternate\"],\n",
    "    #min_iterations=17,\n",
    "    #min_scale=0.01, max_scale=2.,\n",
    "    #min_offset=-2., max_offset=2.\n",
    ")\n",
    "\n",
    "plot_samples(dataset, total=16*16, show_compression_ratio=True)\n",
    "#VF.to_pil_image(make_grid(\n",
    "#    [dataset[i] for i in range(16*16)],\n",
    "#    nrow=16\n",
    "#))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e019a-e986-4b27-9f3c-e8cf9ecf8821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SHAPE = (3, 64, 64)\n",
    "datasets = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a3aed-923f-43b1-abed-7b302ac3058f",
   "metadata": {},
   "source": [
    "# dataset #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51d1759-24a0-4a35-acd4-2a8818d93d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_iter_1 = Kali2dFilteredIterableDataset(\n",
    "    SHAPE, aa=2, size=1_000_000_000, \n",
    "    accumulation_modes=[\"min\", \"max\"],\n",
    "    min_iterations=17,\n",
    "    min_scale=0.01, max_scale=2.,\n",
    "    min_offset=-2., max_offset=2.,\n",
    "    filter_shape=SHAPE[1:],\n",
    "    filter=ImageFilter(\n",
    "        #min_mean=.2,\n",
    "        max_mean=.3,\n",
    "        #min_std=.4,\n",
    "        #max_std=.3,\n",
    "        #min_compression_ratio=.5,\n",
    "        #max_compression_ratio=.9,\n",
    "        #min_scaled_compression_ratio=.7,\n",
    "        #scaled_compression_shape=(16, 16),\n",
    "        min_blurred_compression_ratio=0.32,\n",
    "        #blurred_compression_sigma=10.,\n",
    "        #blurred_compression_kernel_size=[21, 21],\n",
    "        #compression_format=\"png\",\n",
    "    )\n",
    ")\n",
    "plot_samples(ds_iter_1, total=16*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1957c-3682-41c9-a1c6-d03c942607a0",
   "metadata": {},
   "source": [
    "# dataset #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d225cf1-7d2a-4428-abe4-f274920989b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def std_test(image: torch.Tensor) -> bool:\n",
    "    std = torch.std(image.reshape(image.shape[0], -1), dim=1)\n",
    "    return torch.all(std >= .25)\n",
    "\n",
    "ds_iter_2 = Kali2dFilteredIterableDataset(\n",
    "    SHAPE, aa=8, size=1_000_000_000, \n",
    "    #accumulation_modes=[\"min\", \"max\"],\n",
    "    min_iterations=17,\n",
    "    min_scale=0.01, max_scale=2.,\n",
    "    min_offset=-2., max_offset=2.,\n",
    "    filter_shape=(12, 12),\n",
    "    #filter_aa=8,\n",
    "    seed=42,\n",
    "    filter=ImageFilter(\n",
    "        min_mean=.1,\n",
    "        max_mean=.5,\n",
    "        #min_std=.4,\n",
    "        callable=std_test,\n",
    "        #max_std=.3,\n",
    "        #min_compression_ratio=.9,\n",
    "        #max_compression_ratio=.9,\n",
    "        #min_scaled_compression_ratio=.7,\n",
    "        #scaled_compression_shape=(16, 16),\n",
    "        min_blurred_compression_ratio=.42,\n",
    "        #min_blurred_compression_ratio=.32,\n",
    "        #blurred_compression_sigma=10.,\n",
    "        blurred_compression_kernel_size=[21, 21],\n",
    "    )\n",
    ")\n",
    "plot_samples(ds_iter_2, total=16*4, \n",
    "             #label=lambda image: (torch.std(image.reshape(image.shape[0], -1), dim=1) * 10).to(torch.int).tolist()\n",
    "             #label=lambda image: torch.all(torch.std(image.reshape(image.shape[0], -1), dim=1) >= .2)\n",
    "             #label=lambda image: print(\"X\", torch.std(image.reshape(image.shape[0], -1), dim=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f18e1f-f116-454b-b667-a09ecd4ebba9",
   "metadata": {},
   "source": [
    "# dataset #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4d7c9-cb81-4d87-b075-e5e66b0e5932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_iter_3 = Kali2dFilteredIterableDataset(\n",
    "    SHAPE, aa=4, size=1_000_000_000, \n",
    "    #accumulation_modes=[\"min\", \"max\"],\n",
    "    #min_iterations=17,\n",
    "    #min_scale=0.01, max_scale=2.,\n",
    "    #min_offset=-2., max_offset=2.,\n",
    "    filter_shape=(12, 12),\n",
    "    seed=777,\n",
    "    filter=ImageFilter(\n",
    "        min_mean=.05,\n",
    "        max_mean=.4,\n",
    "        #min_std=.4,\n",
    "        #max_std=.3,\n",
    "        min_compression_ratio=.9,\n",
    "        #max_compression_ratio=.9,\n",
    "        #min_scaled_compression_ratio=.7,\n",
    "        #scaled_compression_shape=(16, 16),\n",
    "        #min_blurred_compression_ratio=.5,\n",
    "        #min_blurred_compression_ratio=.32,\n",
    "        #blurred_compression_sigma=10.,\n",
    "        #blurred_compression_kernel_size=[21, 21],\n",
    "    )\n",
    ")\n",
    "plot_samples(ds_iter_3, total=16*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff114a6-1f3b-48c0-be19-8fe7fbc55068",
   "metadata": {},
   "source": [
    "# dataset #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e560282-6846-4785-82ed-89542f2a944a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_iter_4 = Kali2dFilteredIterableDataset(\n",
    "    SHAPE, aa=10, size=1_000_000_000, \n",
    "    accumulation_modes=[\"max\"],\n",
    "    min_iterations=21,\n",
    "    min_scale=.5, max_scale=1,\n",
    "    min_offset=-0, max_offset=0,\n",
    "    filter_shape=(16, 16),\n",
    "    seed=45878,\n",
    "    filter=ImageFilter(\n",
    "        min_mean=.05,\n",
    "        max_mean=.4,\n",
    "        #min_std=.4,\n",
    "        #max_std=.3,\n",
    "        #min_compression_ratio=.9,\n",
    "        #max_compression_ratio=.9,\n",
    "        #min_scaled_compression_ratio=.7,\n",
    "        #scaled_compression_shape=(16, 16),\n",
    "        min_blurred_compression_ratio=.45,\n",
    "        #max_blurred_compression_ratio=.32,\n",
    "        #blurred_compression_sigma=10.,\n",
    "        #blurred_compression_kernel_size=[21, 21],\n",
    "    )\n",
    ")\n",
    "plot_samples(ds_iter_4, total=16*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfb331f-6a4d-4175-8716-6e1cd2a69e96",
   "metadata": {},
   "source": [
    "# dataset #5 !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe3f3d-d4b6-4e5a-9a04-98da4fb84f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_iter_5 = Kali2dFilteredIterableDataset(\n",
    "    SHAPE, aa=8, size=1_000_000_000, \n",
    "    accumulation_modes=[\"min\"],\n",
    "    min_iterations=21,\n",
    "    min_scale=.1, max_scale=.1,\n",
    "    min_offset=-.2, max_offset=.2,\n",
    "    filter_shape=(16, 16),\n",
    "    seed=339595,\n",
    "    filter=ImageFilter(\n",
    "        min_mean=.05,\n",
    "        max_mean=.4,\n",
    "        #min_std=.4,\n",
    "        #max_std=.3,\n",
    "        #min_compression_ratio=.9,\n",
    "        #max_compression_ratio=.9,\n",
    "        #min_scaled_compression_ratio=.7,\n",
    "        #scaled_compression_shape=(16, 16),\n",
    "        min_blurred_compression_ratio=.5,\n",
    "        #max_blurred_compression_ratio=.32,\n",
    "        #blurred_compression_sigma=10.,\n",
    "        #blurred_compression_kernel_size=[21, 21],\n",
    "    )\n",
    ")\n",
    "plot_samples(ds_iter_5, total=16*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc4e36d-0f65-44fa-a5d3-0d12992a4a6b",
   "metadata": {},
   "source": [
    "# datatset #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc016992-ab3b-4a3d-92ce-7aa745d2fdda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_iter_6 = Kali2dFilteredIterableDataset(\n",
    "    SHAPE, aa=4, size=1_000_000_000, \n",
    "    accumulation_modes=[\"min\"],\n",
    "    min_iterations=8,\n",
    "    max_iterations=19,\n",
    "    min_scale=.1, max_scale=.1,\n",
    "    min_offset=--.2, max_offset=.2,\n",
    "    filter_shape=(16, 16),\n",
    "    seed=77,\n",
    "    filter=ImageFilter(\n",
    "        min_mean=.15,\n",
    "        max_mean=.5,\n",
    "        min_std=.2,\n",
    "        max_std=.3,\n",
    "        #min_compression_ratio=.9,\n",
    "        #max_compression_ratio=.9,\n",
    "        #min_scaled_compression_ratio=.7,\n",
    "        #scaled_compression_shape=(16, 16),\n",
    "        min_blurred_compression_ratio=.5,\n",
    "        #max_blurred_compression_ratio=.32,\n",
    "        #blurred_compression_sigma=10.,\n",
    "        #blurred_compression_kernel_size=[21, 21],\n",
    "    )\n",
    ")\n",
    "plot_samples(ds_iter_6, total=16*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f5247a-b06e-4bd5-bd21-7efb4271d039",
   "metadata": {},
   "source": [
    "# dataset 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1594d01b-fe55-4cb4-afba-664044e7be81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_iter_7 = Kali2dFilteredIterableDataset(\n",
    "    SHAPE, aa=4, size=1_000_000_000, \n",
    "    accumulation_modes=[\"none\"],\n",
    "    min_iterations=8,\n",
    "    max_iterations=19,\n",
    "    min_scale=.1, max_scale=.1,\n",
    "    min_offset=--.2, max_offset=.2,\n",
    "    filter_shape=(32, 32),\n",
    "    seed=77733,\n",
    "    filter=ImageFilter(\n",
    "        #min_mean=.15,\n",
    "        max_mean=.5,\n",
    "        #min_std=.2,\n",
    "        max_std=.3,\n",
    "        #min_compression_ratio=.9,\n",
    "        max_compression_ratio=.95,\n",
    "        #min_scaled_compression_ratio=.7,\n",
    "        #scaled_compression_shape=(16, 16),\n",
    "        min_blurred_compression_ratio=.3,\n",
    "        #max_blurred_compression_ratio=.32,\n",
    "        #blurred_compression_sigma=10.,\n",
    "        #blurred_compression_kernel_size=[21, 21],\n",
    "    )\n",
    ")\n",
    "plot_samples(ds_iter_7, total=16*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f87dbf-e396-42d0-af57-2ff2f13e675e",
   "metadata": {},
   "source": [
    "# dataset 8 !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a1bf8-1abe-41b8-adfd-5dd7a391e785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_iter_8 = Kali2dFilteredIterableDataset(\n",
    "    SHAPE, aa=4, size=1_000_000_000, \n",
    "    #accumulation_modes=[\"mean\"],\n",
    "    min_iterations=8,\n",
    "    max_iterations=19,\n",
    "    min_scale=.05, max_scale=.1,\n",
    "    min_offset=1, max_offset=1.,\n",
    "    filter_shape=(16, 16),\n",
    "    seed=7445977,\n",
    "    filter=ImageFilter(\n",
    "        #min_mean=.15,\n",
    "        max_mean=.4,\n",
    "        #min_std=.2,\n",
    "        #max_std=.3,\n",
    "        min_compression_ratio=.7,\n",
    "        #max_compression_ratio=.9,\n",
    "        min_blurred_compression_ratio=.4,\n",
    "        #max_blurred_compression_ratio=.32,\n",
    "        #blurred_compression_sigma=10.,\n",
    "        blurred_compression_kernel_size=[15, 15],\n",
    "    )\n",
    ")\n",
    "plot_samples(ds_iter_8, total=16*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9440863-9104-43fa-abf1-e3642ebcfff3",
   "metadata": {},
   "source": [
    "# datatset 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe10b25-3309-4463-a194-fddbfb559861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_iter_9 = Kali2dFilteredIterableDataset(\n",
    "    SHAPE, aa=4, size=1_000_000_000, \n",
    "    accumulation_modes=[\"min\", \"max\"],\n",
    "    min_iterations=8,\n",
    "    max_iterations=19,\n",
    "    min_scale=.05, max_scale=.1,\n",
    "    min_offset=1, max_offset=1.,\n",
    "    filter_shape=(16, 16),\n",
    "    seed=777,\n",
    "    filter=ImageFilter(\n",
    "        #min_mean=.15,\n",
    "        max_mean=.4,\n",
    "        #min_std=.2,\n",
    "        #max_std=.3,\n",
    "        #min_compression_ratio=.0,\n",
    "        #max_compression_ratio=.9,\n",
    "        min_blurred_compression_ratio=.45,\n",
    "        #max_blurred_compression_ratio=.32,\n",
    "        #blurred_compression_sigma=10.,\n",
    "        blurred_compression_kernel_size=[15, 15],\n",
    "    )\n",
    ")\n",
    "plot_samples(ds_iter_9, total=16*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015c4d32-0043-4077-9497-91b1aff0e6a6",
   "metadata": {},
   "source": [
    "# dataset 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe0001-5001-4819-ba3f-d5386b520624",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_iter_10 = Kali2dFilteredIterableDataset(\n",
    "    SHAPE, aa=4, size=1_000_000_000, \n",
    "    accumulation_modes=[\"min\", \"max\"],\n",
    "    min_iterations=21,\n",
    "    max_iterations=51,\n",
    "    min_scale=.001, max_scale=.01,\n",
    "    min_offset=.5, max_offset=2.,\n",
    "    filter_shape=(16, 16),\n",
    "    seed=7696333,\n",
    "    filter=ImageFilter(\n",
    "        #min_mean=.15,\n",
    "        max_mean=.4,\n",
    "        #min_std=.2,\n",
    "        #max_std=.3,\n",
    "        min_compression_ratio=.9,\n",
    "        #max_compression_ratio=.9,\n",
    "        min_blurred_compression_ratio=.4,\n",
    "        #max_blurred_compression_ratio=.5,\n",
    "        blurred_compression_sigma=10.,\n",
    "        blurred_compression_kernel_size=[15, 15],\n",
    "        compression_format=\"png\",\n",
    "    )\n",
    ")\n",
    "plot_samples(ds_iter_10, total=16*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c199a-3400-4cd5-832e-29d14c7acff9",
   "metadata": {},
   "source": [
    "# dataset 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2becf95-f599-4462-a4d7-a602cf7b6ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_iter_11 = Kali2dFilteredIterableDataset(\n",
    "    SHAPE, aa=8, size=1_000_000_000, \n",
    "    accumulation_modes=[\"min\"],\n",
    "    min_iterations=21,\n",
    "    min_scale=.5, max_scale=1,\n",
    "    min_offset=0, max_offset=0,\n",
    "    filter_shape=(16, 16),\n",
    "    seed=93230,\n",
    "    filter=ImageFilter(\n",
    "        #min_mean=.05,\n",
    "        max_mean=.4,\n",
    "        #min_std=.4,\n",
    "        #max_std=.3,\n",
    "        min_compression_ratio=.8,\n",
    "            #max_compression_ratio=.9,\n",
    "        #min_scaled_compression_ratio=.7,\n",
    "        #scaled_compression_shape=(16, 16),\n",
    "        min_blurred_compression_ratio=.25,\n",
    "        #max_blurred_compression_ratio=.32,\n",
    "        blurred_compression_sigma=10.,\n",
    "        blurred_compression_kernel_size=[15, 15],\n",
    "    )\n",
    ")\n",
    "plot_samples(ds_iter_11, total=16*4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f6259-4c86-48cf-bf8a-53e96d0c3ab0",
   "metadata": {},
   "source": [
    "# dataset 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e60a10f-a531-4430-9bdb-95ceb433f247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_iter_12 = Kali2dFilteredIterableDataset(\n",
    "    SHAPE, aa=4, size=1_000_000_000, \n",
    "    #accumulation_modes=[\"mean\"],\n",
    "    min_iterations=31,\n",
    "    min_scale=.5, max_scale=.6,\n",
    "    min_offset=0, max_offset=0,\n",
    "    filter_shape=(16, 16),\n",
    "    seed=93237880,\n",
    "    filter=ImageFilter(\n",
    "        #min_mean=.05,\n",
    "        max_mean=.4,\n",
    "        #min_std=.4,\n",
    "        max_std=.3,\n",
    "        min_compression_ratio=.8,\n",
    "            #max_compression_ratio=.95,\n",
    "        #min_scaled_compression_ratio=.7,\n",
    "        #scaled_compression_shape=(16, 16),\n",
    "        min_blurred_compression_ratio=.25,\n",
    "        #max_blurred_compression_ratio=.32,\n",
    "        blurred_compression_sigma=10.,\n",
    "        blurred_compression_kernel_size=[15, 15],\n",
    "    )\n",
    ")\n",
    "plot_samples(ds_iter_12, total=16*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa143658-a73a-4495-b175-2d28a2337a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b8cc1-8c16-404b-9b28-0620337ae6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.interleave import InterleaveIterableDataset\n",
    "interleaved_dataset = InterleaveIterableDataset(\n",
    "    datasets=[\n",
    "        ds_iter_1, ds_iter_2, ds_iter_3, ds_iter_4, ds_iter_5, ds_iter_6, ds_iter_7, ds_iter_8, ds_iter_9, ds_iter_10, ds_iter_11\n",
    "    ],\n",
    "    counts=[1, 1, 1, 1, 2, 1, 1, 4, 1, 1, 1],\n",
    "    shuffle_datasets=True,\n",
    ")\n",
    "plot_samples(interleaved_dataset, total=16*4, nrow=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8cadb-fab9-48c0-b906-11ad58bff99f",
   "metadata": {},
   "source": [
    "## store 64x64 samples image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0518f15a-f090-41cc-a46d-eebaef495f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = plot_samples(interleaved_dataset, total=64*64, nrow=64, return_image=True)\n",
    "img.save(\"/home/bergi/Pictures/kali-special.png\")\n",
    "img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39aefc7-ac40-4c28-b297-031812be29c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = PIL.Image.open(\"/home/bergi/Pictures/kali-interleaved.png\")\n",
    "t = VF.pil_to_tensor(img)\n",
    "t.shape[2]/64#reshape(3, 68*68, -1).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5dbe2-3c7b-46f9-83d3-99827c59dd22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff88c6ac-0bbd-446d-aaa3-694bdebc1160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79f6920-5849-4c83-9d56-483773f9f02d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PIL.Image.registered_extensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1810b-7d96-48c4-95b2-82e5bd69db0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7446aa0-a591-4fde-8165-d0e34d8eac4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f689d83-06df-4a73-bb10-314591162f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc7f9f2-ce3f-4045-a782-a96fa36f7f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c76813-e131-4e04-bd72-96e55df6be0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def store_dataset(\n",
    "        images: Iterable,\n",
    "        output_filename=f\"../datasets/kali-uint8-{SHAPE[-2]}x{SHAPE[-1]}.pt\",\n",
    "        max_megabyte=2_048,\n",
    "):\n",
    "    tensor_batch = []\n",
    "    tensor_size = 0\n",
    "    last_print_size = 0\n",
    "    try:\n",
    "        for image in tqdm(images):\n",
    "\n",
    "            image = (image * 255).to(torch.uint8)\n",
    "\n",
    "            if len(image.shape) < 4:\n",
    "                image = image.unsqueeze(0)\n",
    "            tensor_batch.append(image.clamp(0, 255))\n",
    "            tensor_size += math.prod(image.shape)\n",
    "\n",
    "            if tensor_size - last_print_size > 1024 * 1024 * 50:\n",
    "                last_print_size = tensor_size\n",
    "\n",
    "                print(f\"size: {tensor_size:,}\")\n",
    "\n",
    "            if tensor_size >= max_megabyte * 1024 * 1024:\n",
    "                break\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    tensor_batch = torch.cat(tensor_batch)\n",
    "    torch.save(tensor_batch, output_filename)\n",
    "\n",
    "store_dataset(interleaved_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ac0e3-ac61-44e8-9214-b8da311cdbc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c39360-643f-409f-9707-8b142300f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TensorDataset(torch.load(f\"../datasets/kali-uint8-{SHAPE[-2]}x{SHAPE[-1]}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75290a4b-136d-4ad4-9827-cc860d21bbb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, shuffle=True, batch_size=24*24)\n",
    "for batch in dl:\n",
    "    batch = batch[0]\n",
    "    img = VF.to_pil_image(make_grid(batch, nrow=24))\n",
    "    break\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c1f44-9eb9-4911-bf43-682fe4db7a01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d19e4e1-db5a-44f5-941a-f51b9b91f0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be144306-7aa5-4cd3-b7d1-5b726b80c2eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(VF.to_pil_image(Kali2dDataset((3, 16, 16))[111]))\n",
    "display(VF.to_pil_image(Kali2dDataset((3, 32, 32))[111]))\n",
    "display(VF.to_pil_image(Kali2dDataset((3, 64, 64))[111]))\n",
    "display(VF.to_pil_image(Kali2dDataset((3, 256, 256), aa=10)[111]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fbe2cd-667f-4a89-9f4c-152a3db0e95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e807d15e-2ea8-476c-9225-2c1fff56013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Kali2dDataset((3, 128, 128))[221]\n",
    "VF.to_pil_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111ccbc5-b0b2-484d-ad22-0813ebe4e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def pca_error(img: torch.Tensor, n_components: int = 1) -> float:\n",
    "    h = img.shape[-2]\n",
    "    pca = PCA(n_components=n_components)\n",
    "    data = img.permute(1, 2, 0).reshape(h, -1)\n",
    "    pca.fit(data)\n",
    "    f = pca.transform(data)\n",
    "    r_data = torch.Tensor(pca.inverse_transform(f))\n",
    "    r_img = r_data.reshape(h, img.shape[-1], 3).permute(2, 0, 1)\n",
    "    #return ((img - r_img).abs().sum() / math.prod(img.shape))\n",
    "    #d = (img-r_img).abs()\n",
    "    #return VF.to_pil_image(d / d.max())\n",
    "    return VF.to_pil_image(r_img)\n",
    "    \n",
    "pca_error(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
