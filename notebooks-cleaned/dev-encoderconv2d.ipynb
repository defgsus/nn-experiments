{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339552d7-378f-495e-b241-933909db31db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union, Dict\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset, RandomSampler\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import *\n",
    "from src.util import to_torch_device\n",
    "from src.models.cnn import *\n",
    "from src.models.encoder import EncoderConv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f56398-ebd5-406b-a088-4e088f59708b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderConv2dLOCAL(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            shape: Tuple[int, int, int],\n",
    "            kernel_size: int = 5,\n",
    "            channels: Iterable[int] = (16, 32),\n",
    "            code_size: int = 1024,\n",
    "            act_fn: Optional[nn.Module] = nn.ReLU(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.shape = tuple(shape)\n",
    "        self.channels = tuple(channels)\n",
    "        self.kernel_size = int(kernel_size)\n",
    "        self.code_size = int(code_size)\n",
    "        # self.act_fn = act_fn\n",
    "        \n",
    "        channels = [self.shape[0], *self.channels]\n",
    "        self.convolution = Conv2dBlock(\n",
    "            channels=channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            act_fn=act_fn,\n",
    "        )\n",
    "        encoded_shape = self.convolution.get_output_shape(shape)\n",
    "        self.linear = nn.Linear(math.prod(encoded_shape), self.code_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(self.convolution(x).flatten(1))\n",
    "\n",
    "    def get_extra_state(self):\n",
    "        return {\n",
    "            \"shape\": self.shape,\n",
    "            \"kernel_size\": self.kernel_size,\n",
    "            \"channels\": self.channels,\n",
    "            \"code_size\": self.code_size,\n",
    "            \"act_fn\": self.convolution._act_fn,\n",
    "        }\n",
    "    \n",
    "    def set_extra_state(self, state):\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def from_torch(cls, f):\n",
    "        if isinstance(f, (dict, OrderedDict)):\n",
    "            data = f\n",
    "        else:\n",
    "            data = torch.load(f)\n",
    "        \n",
    "        extra = data[\"_extra_state\"]\n",
    "        model = cls(\n",
    "            shape=extra[\"shape\"],\n",
    "            kernel_size=extra[\"kernel_size\"],\n",
    "            channels=extra[\"channels\"],\n",
    "            code_size=extra[\"code_size\"],\n",
    "            act_fn=extra[\"act_fn\"],\n",
    "        )\n",
    "        model.load_state_dict(data)\n",
    "        return model\n",
    "    \n",
    "enc = EncoderConv2d((1, 32, 32), kernel_size=7)\n",
    "enc(torch.ones(1, 1, 32, 32))\n",
    "dict(enc.state_dict())\n",
    "EncoderConv2d.from_torch(enc.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591894de-ffa5-4058-9502-0446d215a55f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts.train_autoencoder_vae import VariationalAutoencoderConv\n",
    "model = VariationalAutoencoderConv((1, 32, 32), channels=[16, 24, 32], kernel_size=5, latent_dims=128)\n",
    "data = torch.load(\"../checkpoints/vae-all1/snapshot.pt\")\n",
    "print(\"{:,} steps\".format(data[\"num_input_steps\"]))\n",
    "model.load_state_dict(data[\"state_dict\"])\n",
    "model = model.encoder\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e543d77d-f973-471b-aaae-e87adc2521ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enc = EncoderConv2d(\n",
    "    shape=(1, 32, 32),\n",
    "    channels=[16, 24, 32],\n",
    "    kernel_size=5,\n",
    "    code_size=128,\n",
    ")\n",
    "if 1:\n",
    "    with torch.no_grad():\n",
    "        for i in range(3):\n",
    "            enc.convolution.layers[i * 2].weight[:] = model.encoder[0].layers[i * 2].weight\n",
    "            enc.convolution.layers[i * 2].bias[:] = model.encoder[0].layers[i * 2].bias\n",
    "        enc.linear.weight[:] = model.linear_mu.weight\n",
    "        enc.linear.bias[:] = model.linear_mu.bias\n",
    "enc#.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45efa871-425e-479a-a800-42d1d74fd20b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fig = px.line(torch.concat([\n",
    "        model.forward(torch.ones(1, *enc.shape), random=False),\n",
    "        enc(torch.ones(1, *enc.shape)),\n",
    "    ]).T, title=\"check that model-copying worked\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c605ff-a96f-4edc-89c8-e238c33b0457",
   "metadata": {},
   "source": [
    "### save encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61311867-eeca-4ae8-8113-ed0df0c2725b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -l ../models/encoder2d/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38220158-85a1-477c-9d5a-d21a69b86aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(enc.state_dict(), \"../models/encoder2d/conv-1x32x32-128-all1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d7feaf-f782-4599-a1da-8d7d5751eef5",
   "metadata": {},
   "source": [
    "# load encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f44a28-d96f-47ae-a89f-35e74895148f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder = EncoderConv2d.from_torch(\"../models/encoderconv/encoder-1x32x32-128-photo-5.pt\", device=\"cpu\")\n",
    "encoder.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab609d0-ad15-4d01-9e8d-0e3fd7209645",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = make_image_patch_dataset(\n",
    "    encoder.shape, \"~/Pictures/photos\", recursive=True, \n",
    "    interleave_images=4, patch_shuffle=100_000,\n",
    "    scales=[1., 1./6.],\n",
    ")\n",
    "ds = ImageFilterIterableDataset(ds, filter=ImageFilter(min_mean=.1))\n",
    "ds = DissimilarImageIterableDataset(ds, max_similarity=.9, max_age=100_000, verbose=True)\n",
    "patches = next(iter(DataLoader(ds, batch_size=32*32)))\n",
    "VF.to_pil_image(make_grid_labeled(patches[:32*32], nrow=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d93e44-ec1c-4842-8867-734fcc63e91b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    features = torch.round(encoder(patches.to(encoder.device)).cpu(), decimals=5)\n",
    "    features /= features.norm(dim=1, keepdim=True)\n",
    "\n",
    "#df = pd.DataFrame(model(patches).detach().numpy())\n",
    "px.line(pd.DataFrame(features[:50]).T.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21564ecd-d4bc-4923-9e1b-6bbfd3db283b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sim = features[:100] @ features[:100].T\n",
    "px.imshow(sim, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0615156f-57ab-46dc-9b16-c458ac5d0a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = (sim * (1. - torch.diag(torch.Tensor([1] * sim.shape[0])))).argmax(dim=1)\n",
    "values, _ = (sim * (1. - torch.diag(torch.Tensor([1] * sim.shape[0])))).max(dim=1)\n",
    "grid = []\n",
    "grid_labels = []\n",
    "for i, (label, value) in enumerate(zip(labels, values)):\n",
    "    if value > .5:\n",
    "        grid.append(patches[i])\n",
    "        grid.append(patches[label])\n",
    "        grid_labels.append(\"\")\n",
    "        grid_labels.append(f\"{float(value):.3f}\")\n",
    "VF.to_pil_image(make_grid_labeled(grid, labels=grid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064aa342-7352-40dc-b363-d902b4e6f3eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "big_sim = features @ features.T\n",
    "values, labels = big_sim.sort(dim=1, descending=True)\n",
    "grid = []\n",
    "grid_labels = []\n",
    "for i, (label_row, value_row) in enumerate(zip(labels[:50], values[:50])):\n",
    "    for l, v in zip(label_row[:30], value_row[:30]):\n",
    "        grid.append(patches[l])\n",
    "        grid_labels.append(f\"{float(v):.3f}\")\n",
    "VF.to_pil_image(make_grid_labeled(grid, nrow=30, labels=grid_labels))\n",
    "VF.to_pil_image(make_grid(grid, nrow=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44af2fb2-4fa8-491b-a883-78230a8161c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed9077-565a-4d01-b296-cf66a0637873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc2ba3-1381-4c79-8f5b-3e4d3d210a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4847893-ce6a-4b23-9538-c1a70561b55f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flat_features = patches.flatten(1)\n",
    "flat_features = flat_features / flat_features.norm(dim=1, keepdim=True)\n",
    "big_sim = flat_features @ flat_features.T\n",
    "values, labels = big_sim.sort(dim=1, descending=True)\n",
    "grid = []\n",
    "grid_labels = []\n",
    "for i, (label_row, value_row) in enumerate(zip(labels[:50], values[:50])):\n",
    "    for l, v in zip(label_row[:30], value_row[:30]):\n",
    "        grid.append(patches[l])\n",
    "        grid_labels.append(f\"{float(v):.3f}\")\n",
    "#VF.to_pil_image(make_grid_labeled(grid, nrow=30, labels=grid_labels))\n",
    "VF.to_pil_image(make_grid(grid, nrow=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1500def4-6596-4d7b-b393-5a2318672b78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "F.hinge_embedding_loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8cfd57-cd98-4de9-a0ef-e159d5e31f5c",
   "metadata": {},
   "source": [
    "# feature vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f43a036-cb97-4274-af83-5d88aa94d93a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RandomRotation(nn.Module):\n",
    "    def __init__(self, degree: float = 10., random_center: float = 1.):\n",
    "        super().__init__()\n",
    "        self.degree = degree\n",
    "        self.random_center = random_center\n",
    "        \n",
    "    def forward(self, x):\n",
    "        degree = (torch.rand(1).item() * 2. - 1.) * self.degree\n",
    "        center = (torch.rand(2) - .5) * self.random_center + .5\n",
    "        center = [\n",
    "            max(0, min(x.shape[-2] - 1, int(center[0] * x.shape[-2]))),\n",
    "            max(0, min(x.shape[-1] - 1, int(center[1] * x.shape[-1])))\n",
    "        ]\n",
    "        return VF.rotate(x, angle=degree, center=center)\n",
    "        \n",
    "def feature_visualization(\n",
    "    encoder: EncoderConv2d,\n",
    "    target: torch.Tensor,\n",
    "    shape: Optional[Tuple[int, int]] = None,\n",
    "    std: float = .1,\n",
    "    mean: float = .5,\n",
    "    num_iter: int = 10,\n",
    "    batch_size: int = 5,\n",
    "    lr: float = 1.,\n",
    "):  \n",
    "    target = target.unsqueeze(0).expand(batch_size, -1).to(encoder.device)\n",
    "    pixel_shape = encoder.shape\n",
    "    if shape:\n",
    "        pixel_shape = (encoder.shape[0], *shape)\n",
    "\n",
    "    run_again = True\n",
    "    while run_again:\n",
    "        run_again = False\n",
    "        \n",
    "        pixels = nn.Parameter(torch.rand(pixel_shape).to(encoder.device) * std + mean)\n",
    "\n",
    "        optimizer = torch.optim.Adadelta([pixels], lr=lr * 5.)\n",
    "        optimizer = torch.optim.Adamax([pixels], lr=lr * .04)\n",
    "\n",
    "        augmentations = [\n",
    "            #VT.Pad(2, padding_mode=\"reflect\"),\n",
    "            #VT.RandomAffine(15, (.3, .3), scale=(.9, 1.1)),\n",
    "            #RandomRotation(4, 1),\n",
    "            VT.RandomPerspective(1, .6),\n",
    "        ]\n",
    "        if shape:\n",
    "            augmentations.append(VT.RandomCrop(encoder.shape[-2:]))\n",
    "\n",
    "        for itr in range(num_iter):\n",
    "\n",
    "            with torch.no_grad():\n",
    "                mix = .35\n",
    "                pixels[:] = pixels * (1.-mix) + mix * VF.gaussian_blur(pixels, 5, 5)\n",
    "\n",
    "            pixel_batch = []\n",
    "            for batch_idx in range(batch_size):\n",
    "                aug_pixels = pixels\n",
    "                for aug in augmentations:\n",
    "                    aug_pixels = aug(aug_pixels)\n",
    "\n",
    "                pixel_batch.append(aug_pixels.unsqueeze(0))\n",
    "\n",
    "            pixel_batch = torch.concat(pixel_batch)\n",
    "\n",
    "            output = encoder(pixel_batch)\n",
    "            if torch.any(torch.isnan(output)):\n",
    "                run_again = True\n",
    "                print(\"NaN eNcOuNtErEd\")\n",
    "                break\n",
    "\n",
    "            #loss = F.l1_loss(target, output)\n",
    "            loss = F.mse_loss(target, output)\n",
    "            #loss = -F.cosine_similarity(target, output, dim=1).mean()\n",
    "            #loss = F.soft_margin_loss(output, target)\n",
    "\n",
    "            encoder.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    return pixels.detach().clamp(0, 1).cpu()\n",
    "\n",
    "img = feature_visualization(encoder, features[12])\n",
    "img = feature_visualization(encoder, features[12], shape=(40, 40), num_iter=35, lr=0.8, batch_size=1)\n",
    "    \n",
    "VF.to_pil_image(VF.resize(img, [s * 4 for s in img.shape[-2:]], interpolation=VF.InterpolationMode.NEAREST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b87989-8172-4c4a-beaf-029de5ed3df2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3a3c27-9ab1-4764-858b-29fc51813829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "for i in tqdm(range(4*4)):\n",
    "    target = features[i] \n",
    "    #target = torch.randn_like(target) * target.std() + target.mean()\n",
    "    images.append(VF.resize(patches[i], (64, 64)))\n",
    "    images.append(feature_visualization(encoder, target, shape=(64, 64)))\n",
    "    \n",
    "img = make_grid(images, nrow=4)\n",
    "VF.to_pil_image(VF.resize(img, [s * 4 for s in img.shape[-2:]], interpolation=VF.InterpolationMode.NEAREST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454acca4-c06d-462f-a1f7-62dba2378f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_window(shape: Tuple[int, int]):\n",
    "    return (\n",
    "          torch.hamming_window(shape[-1], periodic=True).unsqueeze(0).expand(shape[-2], -1)\n",
    "        * torch.hamming_window(shape[-2], periodic=True).unsqueeze(0).expand(shape[-1], -1).T\n",
    "    )\n",
    "#px.imshow(get_window((10, 15)))\n",
    "\n",
    "def reconstruct_image(\n",
    "        encoder: EncoderConv2d, \n",
    "        original: torch.Tensor, \n",
    "        sub_sample: float = 1, \n",
    "        noise: float = 0.,\n",
    "        patch_shape: Optional[Tuple[int, int]] = None,\n",
    "        num_iter: int = 10,\n",
    "        lr: float = 1.,\n",
    "):\n",
    "    _patch_shape = encoder.shape[-2:]\n",
    "    _scale = [1, 1]\n",
    "    if patch_shape:\n",
    "        _patch_shape = (\n",
    "            int(original.shape[-2] / encoder.shape[-2] * patch_shape[-2]),\n",
    "            int(original.shape[-1] / encoder.shape[-1] * patch_shape[-1]),\n",
    "        )\n",
    "        _scale = [\n",
    "            1. / encoder.shape[-2] * patch_shape[-2],\n",
    "            1. / encoder.shape[-1] * patch_shape[-1],\n",
    "        ]\n",
    "    recon = torch.zeros(encoder.shape[0], *_patch_shape)\n",
    "    recon_sum = torch.zeros(encoder.shape[0], *_patch_shape)\n",
    "    window = get_window(patch_shape or encoder.shape[-2:])\n",
    "\n",
    "    try:\n",
    "        patches = []\n",
    "        positions = []\n",
    "        for patch, pos in iter_image_patches(\n",
    "            original, shape=encoder.shape[-2:],\n",
    "            stride=(int(s / sub_sample) for s in encoder.shape[-2:]),\n",
    "            with_pos=True,\n",
    "        ):\n",
    "            pos = [int(p) for p in pos]\n",
    "            if patch.shape[0] != encoder.shape[0]:\n",
    "                for chan in range(patch.shape[0]):\n",
    "                    patches.append(patch[chan].unsqueeze(0).unsqueeze(0))\n",
    "                    positions.append([chan] + pos)\n",
    "            else:\n",
    "                patches.append(patch.unsqueeze(0))\n",
    "                positions.append([slice(0, patch.shape[0])] + pos)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            features = encoder(torch.concat(patches).to(encoder.device))\n",
    "            if noise:\n",
    "                features = features + noise * torch.randn_like(features)\n",
    "        \n",
    "        for feature, pos in tqdm(zip(features, positions), total=len(positions)):\n",
    "            chan, pos = pos[0], pos[1:]\n",
    "            patch_recon = feature_visualization(encoder, feature, shape=patch_shape, lr=lr, num_iter=num_iter)\n",
    "            s1 = chan\n",
    "            s2 = slice(int(pos[0] * _scale[0]), int(pos[0] * _scale[0]) + patch_recon.shape[-2])\n",
    "            s3 = slice(int(pos[1] * _scale[1]), int(pos[1] * _scale[1]) + patch_recon.shape[-1])\n",
    "            recon[s1, s2, s3] = recon[s1, s2, s3] + patch_recon * window\n",
    "            recon_sum[s1, s2, s3] = recon_sum[s1, s2, s3] + window\n",
    "            #recon[chan, pos[0]: pos[0] + patch.shape[-2], pos[1]: pos[1] + patch.shape[-1]] = patch_recon \n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    mask = recon_sum > 0\n",
    "    recon[mask] = recon[mask] / recon_sum[mask]\n",
    "    return recon\n",
    "\n",
    "original = PIL.Image.open(\n",
    "    \"/home/bergi/Pictures/csv-turing.png\"\n",
    "    #\"/home/bergi/Pictures/__diverse/28580_1.jpg\"\n",
    "    #\"/home/bergi/Pictures/__diverse/merkel_sarkozy_g8_regierungOnline_Kuehler_CMS_small_620.jpeg\"\n",
    "   # \"/home/bergi/Pictures/__diverse/honecker.jpg\"\n",
    "    #\"/home/bergi/Pictures/__diverse/plakat01.jpg\"\n",
    "    #\"/home/bergi/Pictures/DWlZbQ5WsAQEzHT.jpg\"\n",
    "    #\"/home/bergi/Pictures/there_is_no_threat.jpeg\"\n",
    "    #\"/home/bergi/Pictures/diffusion/cthulhu-09.jpeg\"\n",
    ")\n",
    "original = VF.to_tensor(original)\n",
    "original = set_image_channels(original, 1)\n",
    "original = VF.resize(original, [s // 8 for s in original.shape[-2:]])\n",
    "display(VF.to_pil_image(original))\n",
    "\n",
    "img = reconstruct_image(encoder, original, sub_sample=5, noise=.000, patch_shape=(64, 64), num_iter=10, lr=10.)\n",
    "VF.to_pil_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ff594-da32-4026-a226-50c99693f00d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
