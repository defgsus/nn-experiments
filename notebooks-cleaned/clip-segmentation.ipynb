{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7b4895-13bb-4dc2-a008-ce51d315cba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "#from torchvision.transforms import v2\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import clip\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util import *\n",
    "from src.util.image import * \n",
    "from src.algo import *\n",
    "from src.algo.wangtiles import *\n",
    "from src.datasets.generative import *\n",
    "from src.models.cnn import *\n",
    "from src.models.transform import *\n",
    "from src.util.embedding import *\n",
    "from src.models.clip import ClipSingleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232e8aea-495c-4911-96b2-fea561ad75d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = VF.to_tensor(PIL.Image.open(\"/home/bergi/Pictures/clipig2/pixelart-cthulhu-02.png\"))\n",
    "#image = VF.to_tensor(PIL.Image.open(\"/home/bergi/Pictures/clipig2/cobblestone-tile.png\"))\n",
    "VF.to_pil_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f19d011-03bd-4ef5-8cbf-b0c1be097b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def clip_segmentation(\n",
    "    image: torch.Tensor,\n",
    "    prompt: str,\n",
    "    patch_size: Tuple[int, int],\n",
    "    batch_size: int = 32,\n",
    "):\n",
    "    def _get_grid(patch_size: Tuple[int, int]):\n",
    "        grid_shape = (image.shape[-2] // patch_size[-2], image.shape[-1] // patch_size[-1]) \n",
    "        num_total = math.prod(grid_shape)\n",
    "        \n",
    "        def _iter_batches():\n",
    "            image_batch = []\n",
    "            pos_batch = []\n",
    "            for patch, pos in tqdm(iter_image_patches(image, shape=patch_size, with_pos=True), total=num_total, disable=True):\n",
    "                if 0:\n",
    "                    masked_image = torch.zeros_like(image)\n",
    "                    masked_image[:, pos[0]:pos[0]+patch_size[0], pos[1]:pos[1]+patch_size[1]] = patch\n",
    "                else:\n",
    "                    masked_image = image + 0\n",
    "                    masked_image[:, pos[0]:pos[0]+patch_size[0], pos[1]:pos[1]+patch_size[1]] = 0\n",
    "                image_batch.append(masked_image.unsqueeze(0))\n",
    "                pos_batch.append(pos)\n",
    "                if len(image_batch) == batch_size:\n",
    "                    yield torch.concat(image_batch), pos_batch\n",
    "                    image_batch.clear()\n",
    "                    pos_batch.clear()\n",
    "            if image_batch:\n",
    "                yield torch.concat(image_batch), pos_batch\n",
    "    \n",
    "        target_features = ClipSingleton.encode_text(prompt, normalize=True)\n",
    "    \n",
    "        dot_batches = [] \n",
    "        for image_batch, pos_batch in _iter_batches():\n",
    "            # display(VF.to_pil_image(make_grid(image_batch)))\n",
    "            # break\n",
    "            feature_batch = ClipSingleton.encode_image(image_batch, normalize=True)\n",
    "    \n",
    "            dots = feature_batch @ target_features.T\n",
    "            dot_batches.append(dots)\n",
    "            \n",
    "        grid = torch.concat(dot_batches).view(grid_shape).cpu()\n",
    "\n",
    "        gridn = 1. - (grid - grid.min()) / (grid.max() - grid.min())\n",
    "        return VF.resize(gridn.unsqueeze(0), image.shape[-2:], VF.InterpolationMode.NEAREST, antialias=False)\n",
    "\n",
    "    grid_sum = torch.zeros_like(image)\n",
    "    for size in tqdm(range(30, 50)):\n",
    "        grid_sum += _get_grid((size, size))\n",
    "\n",
    "    grid_sum /= 20\n",
    "    return grid_sum\n",
    "    #display(px.imshow(grid))\n",
    "\n",
    "grid = clip_segmentation(image, \"cobblestone\", (50, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261560a4-84fb-46ee-9cbb-5d1f1d4142ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gridn = (grid - grid.min()) / (grid.max() - grid.min())\n",
    "#print(gridn.shape, image.shape[-2:])\n",
    "#gridn = VF.resize(gridn.unsqueeze(0), image.shape[-2:], VF.InterpolationMode.NEAREST, antialias=False)\n",
    "image_g = image + 0\n",
    "image_g *= grid\n",
    "VF.to_pil_image(image_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a5a34b-02dd-452a-87b8-4f1c6bf21f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def clip_segmentation_2(\n",
    "    image: torch.Tensor,\n",
    "    prompt: str,\n",
    "    #patch_size: Tuple[int, int],\n",
    "    trials: int = 100,\n",
    "    batch_size: int = 32,\n",
    "):\n",
    "    rng = random.Random(23)\n",
    "    def _iter_batches():\n",
    "        image_batch = [image.unsqueeze(0)]\n",
    "        pos_batch = [((0, 0), image.shape[-2:])]\n",
    "        for i in tqdm(range(trials)):\n",
    "            size = (rng.randrange(image.shape[-2] // 2), rng.randrange(image.shape[-1] // 2))\n",
    "            size = (rng.randint(20, 50), rng.randint(20, 50))\n",
    "            pos = (rng.randrange(image.shape[-2] - size[-2]), rng.randrange(image.shape[-1] - size[-1]))\n",
    "                   \n",
    "            masked_image = image + 0\n",
    "            masked_image[:, pos[0]:pos[0]+size[0], pos[1]:pos[1]+size[1]] = 0\n",
    "            image_batch.append(masked_image.unsqueeze(0))\n",
    "            pos_batch.append((pos, size))\n",
    "            if len(image_batch) == batch_size:\n",
    "                yield torch.concat(image_batch), pos_batch\n",
    "                image_batch.clear()\n",
    "                pos_batch.clear()\n",
    "        if image_batch:\n",
    "            yield torch.concat(image_batch), pos_batch\n",
    "\n",
    "    target_features = ClipSingleton.encode_text(prompt, normalize=True)\n",
    "\n",
    "    grid = torch.zeros_like(image[0])\n",
    "    grid_count = torch.zeros_like(image[0])\n",
    "    for image_batch, pos_batch in _iter_batches():\n",
    "        #display(VF.to_pil_image(make_grid(image_batch)))\n",
    "        #break\n",
    "        feature_batch = ClipSingleton.encode_image(image_batch, normalize=True)\n",
    "\n",
    "        dots = feature_batch @ target_features.T\n",
    "\n",
    "        for (pos, size), dot in zip(pos_batch, dots):\n",
    "            grid[pos[0]:pos[0]+size[0], pos[1]:pos[1]+size[1]] += float(dot)\n",
    "            grid_count[pos[0]:pos[0]+size[0], pos[1]:pos[1]+size[1]] += 1\n",
    "\n",
    "    mask = grid_count > 0\n",
    "    grid[mask] = grid[mask] / grid_count[mask]\n",
    "    #return grid\n",
    "    #print(grid.min(), grid.max(), grid)\n",
    "    return 1. - (grid - grid.min()) / (grid.max() - grid.min())\n",
    "\n",
    "grid = clip_segmentation_2(image, \"repetitive\", 1000)\n",
    "image_g = image + 0\n",
    "image_g *= grid\n",
    "VF.to_pil_image(make_grid([image, image_g, grid.unsqueeze(0).repeat(image.shape[0], 1, 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf7ba78-6d53-43bc-8e00-e387f0f111c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_g = image + 0\n",
    "image_g *= grid\n",
    "VF.to_pil_image(make_grid([image, image_g, grid.unsqueeze(0).repeat(image.shape[0], 1, 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0995200-1f87-4426-a120-b2adcafcbcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(px.imshow(grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bffea8-a003-4717-9d9b-38e140960076",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_g = image + 0\n",
    "image_g *= grid\n",
    "VF.to_pil_image(image_g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
