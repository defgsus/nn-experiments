{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74d853c-775d-4ee4-99ac-b0cd6f0e5871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67371681-7e36-47e9-ac7d-0f80555f7469",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = VF.to_tensor(PIL.Image.open(\n",
    "    \"/home/bergi/Pictures/__diverse/_1983018_orson_150.jpg\"\n",
    "    #\"/home/bergi/Pictures/__diverse/keinmenschistillegal.jpg\"\n",
    ").convert(\"RGB\"))\n",
    "display(VF.to_pil_image(image1))\n",
    "display(image1.shape)\n",
    "\n",
    "image2 = VF.to_tensor(PIL.Image.open(\n",
    "    \"/home/bergi/Pictures/__diverse/keinmenschistillegal.jpg\"\n",
    ").convert(\"RGB\"))\n",
    "display(VF.to_pil_image(image2))\n",
    "display(image2.shape)\n",
    "\n",
    "image3 = VF.to_tensor(PIL.Image.open(\n",
    "    \"../datasets/MarsMarken.png\"\n",
    ").convert(\"RGB\"))\n",
    "display(VF.to_pil_image(image3))\n",
    "display(image3.shape)\n",
    "\n",
    "image4 = VF.to_tensor(PIL.Image.open(\n",
    "    \"../datasets/pixilart.png\"\n",
    ").convert(\"RGB\"))\n",
    "display(VF.to_pil_image(image4))\n",
    "display(image4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7e2d21-a300-4c57-8699-def7273ab7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalized_mean_image(\n",
    "        target_images: torch.Tensor,  # B,C,H,W\n",
    "        perceptual_model: nn.Module,\n",
    "        steps: int = 20000,\n",
    "        batch_size: int = 32,\n",
    "        learnrate: float = 0.005,\n",
    "        loss_function: Callable = F.mse_loss,\n",
    "        device: str = \"auto\",\n",
    "        ret_image: bool = False,\n",
    "):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    device = to_torch_device(device)\n",
    "    if callable(getattr(perceptual_model, \"to\", None)):\n",
    "        perceptual_model.to(device)\n",
    "    \n",
    "    source_image = nn.Parameter(torch.zeros(target_images.shape[1:]).to(device))\n",
    "    optimizer = torch.optim.Adam([source_image], lr=learnrate)\n",
    "    \n",
    "    target_batch = target_images.repeat(batch_size // target_images.shape[0], 1, 1, 1)[:batch_size].to(device)\n",
    "    with torch.no_grad():\n",
    "        p_target_batch = perceptual_model(target_batch)\n",
    "\n",
    "    try:\n",
    "        with tqdm(total=steps) as progress:\n",
    "            for i in range(steps // batch_size):\n",
    "                progress.update(batch_size)\n",
    "    \n",
    "                source_batch = source_image.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n",
    "                p_source_batch = perceptual_model(source_batch)\n",
    "    \n",
    "                loss = loss_function(p_source_batch, p_target_batch)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                progress.set_postfix({\"loss\": float(loss)})\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    source_image = source_image.detach().cpu().clamp(0, 1)\n",
    "    if not ret_image:\n",
    "        display(VF.to_pil_image(source_image))\n",
    "    torch.cuda.empty_cache()\n",
    "    if ret_image:\n",
    "        return VF.to_pil_image(source_image)\n",
    "    \n",
    "def create_target_images(image: torch.Tensor, o: int = 2):\n",
    "    h, w = image.shape[-2:]\n",
    "    h -= o\n",
    "    w -= o\n",
    "    return torch.cat([\n",
    "        image[None, :, :h, :w],\n",
    "        image[None, :, o:h+o, :w],\n",
    "        image[None, :, o:h+o, o:w+o],\n",
    "        image[None, :, :h, o:w+o],\n",
    "    ])\n",
    "    \n",
    "image1_l1 = generalized_mean_image(\n",
    "    create_target_images(image1, o=3),\n",
    "    nn.Identity(),\n",
    "    loss_function=F.l1_loss,\n",
    "    ret_image=True,\n",
    ")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f673d85f-12d7-44e3-9a62-e419ca7d4fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generalized_mean_image(\n",
    "    create_target_images(image4),\n",
    "    nn.Identity(),\n",
    "    loss_function=F.mse_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba560f-273d-498a-a297-f66b623ac228",
   "metadata": {},
   "outputs": [],
   "source": [
    "generalized_mean_image(\n",
    "    create_target_images(image3),\n",
    "    nn.Sequential(\n",
    "        nn.Conv2d(3, 32, kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 32, kernel_size=1),\n",
    "        nn.ReLU(),\n",
    "    ),\n",
    "    loss_function=F.l1_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae323a-1882-4a67-9093-d4039d780ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generalized_mean_image(\n",
    "    create_target_images(image3),\n",
    "    nn.Sequential(\n",
    "        nn.Conv2d(3, 32, kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 32, kernel_size=1),\n",
    "        nn.ReLU(),\n",
    "    ),\n",
    "    loss_function=F.mse_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fd09e6-22e6-4f1a-904e-17468bdcc538",
   "metadata": {},
   "outputs": [],
   "source": [
    "generalized_mean_image(\n",
    "    create_target_images(image3),\n",
    "    nn.Sequential(\n",
    "        nn.Conv2d(3, 32, kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 32, kernel_size=1),\n",
    "        nn.ReLU(),\n",
    "        nn.AvgPool2d(32, 16),\n",
    "    ),\n",
    "    loss_function=F.l1_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db04a4fe-4750-4e73-8f07-12f83ad0826b",
   "metadata": {},
   "source": [
    "# special kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3269eded-f9be-48fb-bc13-0fc2cdb09127",
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptual_model = nn.Conv2d(3, 3, 3, padding=1)\n",
    "w = torch.Tensor([\n",
    "    [-1, 0, -1],\n",
    "    [0, 4, 0],\n",
    "    [-1, 0, -1],\n",
    "]) * math.sqrt(2) / 2\n",
    "with torch.no_grad():\n",
    "    perceptual_model.weight[:] = torch.cat([\n",
    "        (w.unsqueeze(0).repeat(3, 1, 1) * torch.Tensor([1, 0, 0])).unsqueeze(0),\n",
    "        (w.unsqueeze(0).repeat(3, 1, 1) * torch.Tensor([0, 1, 0])).unsqueeze(0),\n",
    "        (w.unsqueeze(0).repeat(3, 1, 1) * torch.Tensor([0, 0, 1])).unsqueeze(0),\n",
    "    ])\n",
    "#print(perceptual_model.weight)\n",
    "\n",
    "generalized_mean_image(\n",
    "    create_target_images(image2),\n",
    "    perceptual_model,\n",
    "    loss_function=F.l1_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a17a35-8def-4ec2-bbc3-e625f70e2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "from src.models.wavelet.util import create_wavelet_filter\n",
    "#pywt.wavelist()\n",
    "#wl = pywt.Wavelet(\"haar\")\n",
    "#wl.dec_hi\n",
    "create_wavelet_filter(\"haar\", 3, 3)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24a3fa-eb0d-4cbd-b54a-d7901831a507",
   "metadata": {},
   "source": [
    "# sobel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe286e-7a20-4235-b3c6-b9a8907d8a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "VF.gaussian_blur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10528b6-137e-4025-b998-946f23253154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sobel(x):\n",
    "    blur = VF.gaussian_blur(x, 5, 2.)\n",
    "    return x - blur * .5\n",
    "\n",
    "generalized_mean_image(\n",
    "    create_target_images(image4),\n",
    "    sobel,\n",
    "    loss_function=F.l1_loss,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b98160-5554-4d1f-bdc2-a1796fb7d679",
   "metadata": {},
   "source": [
    "# FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecea73d-f7f1-4443-a987-abe49cafc625",
   "metadata": {},
   "outputs": [],
   "source": [
    "generalized_mean_image(\n",
    "    create_target_images(image3),\n",
    "    #nn.Identity(), \n",
    "    torch.fft.fft2,\n",
    "    loss_function=F.l1_loss,\n",
    "    steps=100_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c802f-3951-4823-b55b-f272f17c24a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_func(x):\n",
    "    x = torch.fft.fft2(x)\n",
    "    return torch.concat([x.real, x.imag], dim=-3)\n",
    "    \n",
    "generalized_mean_image(\n",
    "    create_target_images(image3),\n",
    "    fft_func,\n",
    "    loss_function=F.mse_loss, #l1_loss,\n",
    "    steps=200_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7681c0-64e7-4c2c-93bc-a30590da2780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_func(x):\n",
    "    x = torch.fft.fft2(x)\n",
    "    return x[..., :10, :10]\n",
    "    \n",
    "generalized_mean_image(\n",
    "    create_target_images(image3),\n",
    "    fft_func,\n",
    "    loss_function=F.l1_loss,\n",
    "    steps=100_000,\n",
    ")\n",
    "display(image1_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7277724-e116-4d63-9aa0-807aa3af0d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_func(x):\n",
    "    x = torch.fft.fft2(x)\n",
    "    h, w = x.shape[-2:]\n",
    "    h, w = h * 2 // 3, w * 2 // 3\n",
    "    #x[..., h:, w:] = x[..., h:, w:] * 10\n",
    "    #x[..., :3, :3] = x[..., :3, :3] * 1000\n",
    "    return x\n",
    "    #f1 = x[..., :10, :10].flatten(-2)\n",
    "    #f2 = x[..., 10:, 10:].flatten(-2)\n",
    "    #return torch.concat([f1, f2], dim=-1)\n",
    "    \n",
    "generalized_mean_image(\n",
    "    create_target_images(image1),\n",
    "    fft_func,\n",
    "    loss_function=F.l1_loss,\n",
    "    steps=100_000,\n",
    ")\n",
    "display(image1_l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632a355-1f74-4c35-a145-a13cdd392f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.fft.fft2(image1)\n",
    "    #x = x[..., :30, :30] / 20\n",
    "    x[..., :, 1:20] = 0\n",
    "    #f2 = x[..., 10:, 10:].flatten(-2)\n",
    "    display(VF.to_pil_image(torch.fft.ifft2(x).real.clamp(0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b82e3-534c-41f7-beb5-f3d4b5514c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20c1bf-3e93-44f8-be1d-816e97471117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6862b8c-99e6-488c-93be-6fcf41d654d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f31d3-8fe1-458c-916f-de7b7ce4d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "CH = 16\n",
    "ACT = nn.GELU()\n",
    "KS = 1\n",
    "for KS in [1, 2, 3, 4, 5, 6, 7]:\n",
    "#for CH in [4, 8, 16, 32, 64, 128]:\n",
    "    print(f\"CH={CH}, KS={KS}, ACT={ACT}\")\n",
    "    grid = []\n",
    "    for i in range(4):\n",
    "        grid.append(VF.to_tensor(generalized_mean_image(\n",
    "            create_target_images(image1),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(3, CH, kernel_size=KS),#, dilation=3), \n",
    "                ACT,\n",
    "                nn.Conv2d(CH, CH, kernel_size=KS),#, dilation=5), \n",
    "                ACT,\n",
    "                nn.Conv2d(CH, CH, kernel_size=KS), \n",
    "                ACT,\n",
    "            ),\n",
    "            loss_function=F.l1_loss,\n",
    "            batch_size=16,\n",
    "            steps=6000,\n",
    "            ret_image=True,\n",
    "        )))\n",
    "    print(f\"CH={CH}, KS={KS}, ACT={ACT}\")\n",
    "    if len(grid) == 1:\n",
    "        display(VF.to_pil_image(grid[0]))\n",
    "    else:\n",
    "        display(VF.to_pil_image(make_grid(grid)))\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb12447-31f0-468e-99c3-232c150e33e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CH = 128\n",
    "ACT = nn.GELU()\n",
    "KS = 1\n",
    "for KS in [1, 3, 5, 7, 9]:\n",
    "#for CH in [4, 8, 16, 32, 64, 128]:\n",
    "    print(f\"CH={CH}, KS={KS}, ACT={ACT}\")\n",
    "    grid = []\n",
    "    for i in range(1):\n",
    "        grid.append(VF.to_tensor(generalized_mean_image(\n",
    "            create_target_images(image2),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(3, CH, kernel_size=KS, padding=(KS - 1) // 2),#, dilation=3), \n",
    "                #ACT,\n",
    "                #nn.Conv2d(CH, CH, kernel_size=KS),#, dilation=5), \n",
    "                #ACT,\n",
    "                #nn.Conv2d(CH, CH, kernel_size=KS), \n",
    "                #ACT,\n",
    "            ),\n",
    "            loss_function=F.l1_loss,\n",
    "            batch_size=16,\n",
    "            steps=6000,\n",
    "            ret_image=True,\n",
    "        )))\n",
    "    print(f\"CH={CH}, KS={KS}, ACT={ACT}\")\n",
    "    if len(grid) == 1:\n",
    "        display(VF.to_pil_image(grid[0]))\n",
    "    else:\n",
    "        display(VF.to_pil_image(make_grid(grid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a59e9-4601-47ed-9f19-ccc0835aa2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "CH = 128\n",
    "ACT = nn.ReLU()\n",
    "generalized_mean_image(\n",
    "    create_target_images(image1, o=3),\n",
    "    nn.Sequential(\n",
    "        nn.Conv2d(3, CH, kernel_size=3), \n",
    "        ACT,\n",
    "        nn.Conv2d(CH, CH, kernel_size=3), \n",
    "        ACT,\n",
    "        nn.Conv2d(CH, CH, kernel_size=3), \n",
    "        ACT,\n",
    "        nn.AvgPool2d(32, 16),\n",
    "        #nn.Conv2d(CH, CH, kernel_size=1), \n",
    "        #ACT,\n",
    "        #nn.Conv2d(CH, CH, kernel_size=1), \n",
    "        #ACT,\n",
    "        #nn.Conv2d(CH, CH, kernel_size=1), \n",
    "        #ACT,\n",
    "        #nn.Conv2d(CH, CH, kernel_size=1), \n",
    "        #ACT,\n",
    "    ),\n",
    "    loss_function=F.mse_loss,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106fe579-7f58-42ff-b41b-342e23846e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_func(x):\n",
    "    x = torch.fft.fft2(x)\n",
    "    return torch.concat([x.real, x.imag], dim=-3)\n",
    "    \n",
    "generalized_mean_image(\n",
    "    create_target_images(image2),\n",
    "    nn.Identity(),\n",
    "    loss_function=lambda s, t: F.huber_loss(s, t, delta=.4),\n",
    "    #loss_function=lambda s, t: -F.cosine_similarity(s, t).mean(),\n",
    "    #steps=200_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd5323-b2b8-46d0-8efa-71fba7b4d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "[n for n in dir(F) if \"loss\" in n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e6f9a8-f120-464d-a12d-a3f2eee8a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.smooth_l1_loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aaef51-5f7b-495c-b672-c9a9e612cf70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a426472f-6ecf-4226-a061-59bd72e6120d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c09e1-9955-4ecf-917e-4a46acfc2f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef58b32-3956-4d18-9e24-68bf6650b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "ds = torchvision.datasets.STL10(\n",
    "    root=Path(\"~/prog/data/datasets/\").expanduser(),\n",
    "    #download=True,\n",
    ")\n",
    "for _, (image, id) in zip(range(10), ds):\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0242a8e0-d9f7-4d48-9c7f-5c143e5ac154",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.pixel_unshuffle(VF.to_tensor(image), 3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf184d35-b35c-4dd9-8769-824e6f23bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "64*4**2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
