{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd67fe-b454-463a-a01f-0708f3fe54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import *\n",
    "from src.util import *\n",
    "from src.algo import *\n",
    "from src.models.decoder import *\n",
    "from src.models.transform import *\n",
    "from src.models.util import *\n",
    "\n",
    "def resize(img, scale: float, mode: VF.InterpolationMode = VF.InterpolationMode.NEAREST):\n",
    "    return VF.resize(img, [max(1, int(s * scale)) for s in img.shape[-2:]], mode, antialias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae3ef3c-78ad-408e-b28f-2c5ae677e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.datasets import all_image_patch_dataset\n",
    "ds = all_image_patch_dataset((3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee72ed-54a8-45b6-83f4-c3cbce75ee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = next(iter(DataLoader(ds, batch_size=64)))\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0437d391-0d6f-47ab-9a9e-ac2f0f459bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "VF.to_pil_image(make_grid(patches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c37ee2f-7633-424c-9a62-e4fee53f1297",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomGradientTransform(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            min_quantization: float = 0.1,\n",
    "            max_quantization: float = 0.3,\n",
    "            clamp_output: Union[bool, Tuple[float, float]] = (0., 1.),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.min_quantization = min_quantization\n",
    "        self.max_quantization = max_quantization\n",
    "        self.clamp_output = clamp_output\n",
    "        \n",
    "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        if self.clamp_output is True:\n",
    "            mi, ma = image.min(), image.max()\n",
    "        q = max(0.000001, random.uniform(self.min_quantization, self.max_quantization))\n",
    "        x = (image / q).round() * q\n",
    "\n",
    "        if self.clamp_output is True:\n",
    "            x = x.clamp(mi, ma)\n",
    "        elif self.clamp_output:\n",
    "            x = x.clamp(*self.clamp_output)\n",
    "            \n",
    "        return x\n",
    "\n",
    "VF.to_pil_image(make_grid(\n",
    "    RandomGradientTransform(min_quantization=.2, max_quantization=.4)(patches)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028e173-c14c-45a7-9a38-088b78d6647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = 3\n",
    "st = 2\n",
    "pool = nn.MaxPool2d(kernel_size=ks, stride=st, return_indices=True)\n",
    "input = torch.rand(1, 3, 32, 32)\n",
    "pooled, idx = pool(input)\n",
    "print(\"pooled:\", pooled.shape)\n",
    "pooled_conv = pooled\n",
    "pooled_conv = nn.Conv2d(3, 3, 3, padding=1, padding_mode=\"reflect\")(pooled)\n",
    "display(VF.to_pil_image(resize(make_grid(list(pooled) + list(pooled_conv), normalize=True, scale_each=True), 5)))\n",
    "#print(idx.shape)\n",
    "\n",
    "unpool = nn.MaxUnpool2d(kernel_size=ks, stride=st)\n",
    "output = unpool(pooled_conv, idx, output_size=input.shape[-2:])\n",
    "print(output.shape)\n",
    "VF.to_pil_image(resize(make_grid(list(input) + list(output)), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d68045-9eda-4eef-bf7a-c1c6a617551a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBDNConv(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            kernel_size: int,\n",
    "            stride: int,\n",
    "            padding: int,\n",
    "            activation: Union[None, str, Callable],\n",
    "            batch_norm: bool,\n",
    "            transposed: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = (nn.ConvTranspose2d if transposed else nn.Conv2d)(\n",
    "            in_channels, out_channels, kernel_size, stride=stride, padding=padding,\n",
    "        )\n",
    "        self.act = activation_to_module(activation)\n",
    "        if batch_norm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "            output_size: Union[None, Tuple[int, int]] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        if output_size is not None and tuple(x.shape[-2:]) != output_size:\n",
    "            x = F.pad(x, (0, output_size[-1] - x.shape[-1], 0, output_size[-2] - x.shape[-2]))\n",
    "\n",
    "        if self.act:\n",
    "            x = self.act(x)\n",
    "\n",
    "        if hasattr(self, \"bn\"):\n",
    "            x = self.bn(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class RBDNBranch(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            hidden_channels: int,\n",
    "            num_hidden_layers: int = 1,\n",
    "\n",
    "            conv_kernel_size: int = 3,\n",
    "            conv_stride: int = 1,\n",
    "            conv_padding: int = 0,\n",
    "            pool_kernel_size: int = 3,\n",
    "            pool_stride: int = 1,\n",
    "\n",
    "            hidden_kernel_size: int = 3,\n",
    "            hidden_stride: int = 1,\n",
    "            hidden_padding: int = 1,\n",
    "\n",
    "            batch_norm: bool = True,\n",
    "            batch_norm_last_layer: bool = False,\n",
    "            activation: Union[None, str, Callable] = \"relu\",\n",
    "            activation_last_layer: Union[None, str, Callable] = \"relu\",\n",
    "            sub_branch: Union[None, \"RBDNBranch\"] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        actual_hidden_channels = hidden_channels\n",
    "        if sub_branch is not None:\n",
    "            actual_hidden_channels += sub_branch.out_channels\n",
    "\n",
    "        self.conv_in = RBDNConv(\n",
    "            in_channels, hidden_channels, conv_kernel_size, stride=conv_stride, padding=conv_padding,\n",
    "            activation=activation, batch_norm=batch_norm\n",
    "        )\n",
    "\n",
    "        self.sub_branch = sub_branch\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=pool_kernel_size, stride=pool_stride, return_indices=True)\n",
    "\n",
    "        self.hidden = nn.Sequential()\n",
    "        for i in range(num_hidden_layers):\n",
    "            self.hidden.add_module(f\"conv_{i+1}\", RBDNConv(\n",
    "                actual_hidden_channels, actual_hidden_channels, hidden_kernel_size, stride=hidden_stride, padding=hidden_padding,\n",
    "                activation=activation, batch_norm=batch_norm\n",
    "            ))\n",
    "\n",
    "        self.unpool = nn.MaxUnpool2d(kernel_size=pool_kernel_size, stride=pool_stride)\n",
    "\n",
    "        self.conv_out = RBDNConv(\n",
    "            actual_hidden_channels, out_channels, conv_kernel_size, stride=conv_stride, padding=conv_padding,\n",
    "            activation=activation_last_layer, batch_norm=batch_norm_last_layer,\n",
    "            transposed=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv_in(input)\n",
    "\n",
    "        if self.sub_branch is not None:\n",
    "            sub_x = self.sub_branch(x)\n",
    "            x = torch.concat([x, sub_x], dim=-3)\n",
    "\n",
    "        unpooled_size = x.shape[-2:]\n",
    "        x, indices = self.pool(x)\n",
    "\n",
    "        # print(\"hidden:\", x.shape)\n",
    "        x = self.hidden(x)\n",
    "\n",
    "        x = self.unpool(x, indices, output_size=unpooled_size)\n",
    "        x = self.conv_out(x, output_size=input.shape[-2:])\n",
    "\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_inner_shape(self, shape: Tuple[int, int, int]) -> dict:\n",
    "        x = self.conv_in(torch.zeros(1, *shape))\n",
    "\n",
    "        if self.sub_branch is not None:\n",
    "            branch_shape = self.sub_branch.get_inner_shape(x.shape[-3:])\n",
    "            sub_x = self.sub_branch(x)\n",
    "            x = torch.concat([x, sub_x], dim=-3)\n",
    "        else:\n",
    "            branch_shape = None\n",
    "            \n",
    "        x, indices = self.pool(x)\n",
    "\n",
    "        ret = {\"shape\": x.shape[-3:], \"branch\": branch_shape}\n",
    "        return ret\n",
    "            \n",
    "        \n",
    "class RBDN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            hidden_channels: int,\n",
    "            num_branches: int,\n",
    "            num_hidden_layers: int = 1,\n",
    "\n",
    "            conv_kernel_size: int = 3,\n",
    "            conv_stride: int = 1,\n",
    "            conv_padding: int = 0,\n",
    "            pool_kernel_size: int = 3,\n",
    "            pool_stride: int = 1,\n",
    "\n",
    "            branch_conv_kernel_size: int = 3,\n",
    "            branch_conv_stride: int = 2,\n",
    "            branch_conv_padding: int = 1,\n",
    "            branch_pool_kernel_size: int = 3,\n",
    "            branch_pool_stride: int = 1,\n",
    "\n",
    "            hidden_kernel_size: int = 3,\n",
    "            hidden_stride: int = 1,\n",
    "            hidden_padding: int = 1,\n",
    "\n",
    "            batch_norm: bool = True,\n",
    "            batch_norm_last_layer: bool = False,\n",
    "            activation: Union[None, str, Callable] = \"relu\",\n",
    "            activaton_last_layer: Union[None, str, Callable] = \"sigmoid\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        branches = None\n",
    "        for i in range(num_branches):\n",
    "            branches = RBDNBranch(\n",
    "                hidden_channels, hidden_channels, hidden_channels,\n",
    "                conv_kernel_size=branch_conv_kernel_size, conv_stride=branch_conv_stride, conv_padding=branch_conv_padding,\n",
    "                pool_kernel_size=branch_pool_kernel_size, pool_stride=branch_pool_stride,\n",
    "                hidden_kernel_size=hidden_kernel_size, hidden_stride=hidden_stride, hidden_padding=hidden_padding,\n",
    "                activation=activation, batch_norm=batch_norm,\n",
    "                sub_branch=branches,\n",
    "            )\n",
    "\n",
    "        self.branches = RBDNBranch(\n",
    "            in_channels, out_channels, hidden_channels,\n",
    "            conv_kernel_size=conv_kernel_size, conv_stride=conv_stride, conv_padding=conv_padding,\n",
    "            pool_kernel_size=pool_kernel_size, pool_stride=pool_stride,\n",
    "            hidden_kernel_size=hidden_kernel_size, hidden_stride=hidden_stride, hidden_padding=hidden_padding,\n",
    "            activation=activation,\n",
    "            batch_norm=batch_norm,\n",
    "            num_hidden_layers=num_hidden_layers,\n",
    "            sub_branch=branches,\n",
    "            batch_norm_last_layer=batch_norm_last_layer,\n",
    "            activation_last_layer=activaton_last_layer,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.branches(x)\n",
    "\n",
    "    def get_inner_shape(self, shape: Tuple[int, int, int]) -> dict:\n",
    "        return self.branches.get_inner_shape(shape)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model = RBDN(\n",
    "        3, 3, 64,\n",
    "        num_branches=2,\n",
    "        conv_kernel_size=3,\n",
    "        conv_padding=1,\n",
    "        branch_conv_kernel_size=3,\n",
    "        branch_conv_padding=1,\n",
    "        branch_conv_stride=2,\n",
    "        num_hidden_layers=5,\n",
    "        #branch_pool_kernel_size=1,\n",
    "        #branch_pool_stride=1,\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    print(f\"params: {num_module_parameters(model):,}\")\n",
    "    \n",
    "    input = torch.randn(4, 3, 64, 64)\n",
    "    #input = torch.randn(1, 3, 128, 128)\n",
    "    output = model(input)\n",
    "    #output = model.branches.sub_branch(torch.rand(4, 64, 64, 64))[:, :3, ...]\n",
    "    \n",
    "    #output = ((output-.0) * 3).clamp(0, 1)\n",
    "    print(output.shape)\n",
    "    display(VF.to_pil_image(make_grid(list(input) + list(output), padding=0)))\n",
    "\n",
    "    print(json.dumps(model.get_inner_shape(input.shape[-3:]), indent=2))\n",
    "    \n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d693db-a897-4803-be86-1a6cdcf049ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab774fe-9a72-49b5-a705-3278ef03126c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc21f2-5e10-44fe-9be2-90eb3a0c3c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa84862-8a04-4255-a883-d8fcf838976c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6a41f-4be5-4eb0-942c-eb324206ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDenoiser(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            shape: Tuple[int, int, int],\n",
    "            channels: Iterable[int],\n",
    "            kernel_size: Union[int, Iterable[int]] = 3,\n",
    "            stride: Union[int, Iterable[int]] = 1,\n",
    "            batch_norm: bool = True,\n",
    "            activation: Union[None, str, Callable] = \"gelu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "        self._channels = [shape[0], *channels, shape[0]]\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_sizes = [kernel_size] * len(self._channels)\n",
    "        else:\n",
    "            kernel_sizes = list(kernel_size)\n",
    "            if len(kernel_sizes) != len(self._channels) - 1:\n",
    "                raise ValueError(f\"Expected kernel_size of length {len(self._channels) - 1}, got {len(kernel_sizes)}\")\n",
    "\n",
    "        if isinstance(stride, int):\n",
    "            strides = [stride] * len(self._channels)\n",
    "        else:\n",
    "            strides = list(stride)\n",
    "            if len(strides) != len(self._channels) - 1:\n",
    "                raise ValueError(f\"Expected stride of length {len(self._channels) - 1}, got {len(strides)}\")\n",
    "\n",
    "        self.encoder = nn.ModuleDict()\n",
    "        decoder_paddings = []\n",
    "        with torch.no_grad():\n",
    "            tmp_state = torch.zeros(1, *shape)\n",
    "\n",
    "            for i, (ch, ch_next, kernel_size, stride) in enumerate(zip(self._channels, self._channels[1:], kernel_sizes, strides)):\n",
    "                if batch_norm:\n",
    "                    self.encoder[f\"layer{i+1}_bn\"] = nn.BatchNorm2d(ch)\n",
    "                self.encoder[f\"layer{i+1}_conv\"] = nn.Conv2d(ch, ch_next, kernel_size, stride=stride)\n",
    "                if activation:\n",
    "                    self.encoder[f\"layer{i+1}_act\"] = activation_to_module(activation)\n",
    "\n",
    "                in_shape = tmp_state.shape[-2:]\n",
    "                tmp_state = self.encoder[f\"layer{i+1}_conv\"](tmp_state)\n",
    "                dec_shape = nn.ConvTranspose2d(ch_next, ch_next, kernel_size, stride=stride)(tmp_state).shape[-2:]\n",
    "                decoder_paddings.append(\n",
    "                    [s - ds for s, ds in zip(in_shape, dec_shape)]\n",
    "                )\n",
    "\n",
    "        channels = list(reversed(self._channels))\n",
    "        kernel_sizes = list(reversed(kernel_sizes))\n",
    "        strides = list(reversed(strides))\n",
    "        self.decoder = nn.ModuleDict()\n",
    "        for i, (ch, ch_next, kernel_size, stride, pad) in enumerate(\n",
    "                zip(channels, channels[1:], kernel_sizes, strides, list(reversed(decoder_paddings)))\n",
    "        ):\n",
    "            if batch_norm:\n",
    "                self.decoder[f\"layer{i+1}_bn\"] = nn.BatchNorm2d(ch)\n",
    "\n",
    "            self.decoder[f\"layer{i+1}_conv\"] = nn.ConvTranspose2d(\n",
    "                ch, ch_next, kernel_size, stride=stride, output_padding=pad,\n",
    "            )\n",
    "            if activation and i < len(channels) - 2:\n",
    "                self.decoder[f\"layer{i+1}_act\"] = activation_to_module(activation)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        state_history = []\n",
    "        state = x\n",
    "        for i in range(len(self._channels) - 1):\n",
    "            if f\"layer{i+1}_bn\" in self.encoder:\n",
    "                state = self.encoder[f\"layer{i+1}_bn\"](state)\n",
    "            state = self.encoder[f\"layer{i+1}_conv\"](state)\n",
    "            if f\"layer{i+1}_act\" in self.encoder:\n",
    "                state = self.encoder[f\"layer{i+1}_act\"](state)\n",
    "            state_history.append(state)\n",
    "            #print(\"ENC\", state.shape)\n",
    "\n",
    "        for i in range(len(self._channels) - 1):\n",
    "            if i > 0:\n",
    "                # print(\"DEC\", state.shape, state_history[-(i+1)].shape)\n",
    "                state = state + state_history[-(i+1)]\n",
    "            if f\"layer{i+1}_bn\" in self.decoder:\n",
    "                state = self.decoder[f\"layer{i+1}_bn\"](state)\n",
    "            state = self.decoder[f\"layer{i+1}_conv\"](state)\n",
    "            if f\"layer{i+1}_act\" in self.decoder:\n",
    "                state = self.decoder[f\"layer{i+1}_act\"](state)\n",
    "\n",
    "        return state\n",
    "\n",
    "model = ConvDenoiser(\n",
    "    (3, 64, 64), \n",
    "    [64, 128, 128, 128, 64],\n",
    "    stride=[1, 1, 2, 2, 2, 1],\n",
    ").eval()\n",
    "print(f\"params: {num_module_parameters(model):,}\")\n",
    "\n",
    "input = torch.randn(1, *model.shape)\n",
    "#input = torch.randn(1, 3, 128, 128)\n",
    "output = model(input)\n",
    "print(output.shape)\n",
    "display(VF.to_pil_image(make_grid([input[0], output[0]])))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b738bd1-cf7e-4420-acc0-2bacc1c2dc7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec78e03-efba-48be-8e9e-2477ab1a7a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f261801-11a6-473d-a869-6b7b99d6c566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be8a5f-9e2b-4a4e-8b96-278e16337594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.util.params import *\n",
    "\n",
    "def map_image_patches(\n",
    "        image: torch.Tensor,\n",
    "        function: Callable[[torch.Tensor], torch.Tensor],\n",
    "        patch_size: Union[int, Tuple[int, int]],\n",
    "        overlap: Union[int, Tuple[int, int]] = 0,\n",
    "        cut_away: Union[int, Tuple[int, int]] = 0,\n",
    "        batch_size: int = 64,\n",
    "        auto_pad: bool = True,\n",
    "        window: Union[bool, torch.Tensor, Callable] = False,\n",
    "        verbose: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Pass an image patch-wise through `function` with shape [batch_size, C, *patch_size]\n",
    "    and return processed image.\n",
    "    \"\"\"\n",
    "    if image.ndim != 3:\n",
    "        raise ValueError(f\"Expected image.ndim = 3, got {image.shape}\")\n",
    "\n",
    "    patch_size = param_make_tuple(patch_size, 2, \"patch_size\")\n",
    "    overlap = param_make_tuple(overlap, 2, \"overlap\")\n",
    "    cut_away = param_make_tuple(cut_away, 2, \"cut_away\")\n",
    "\n",
    "    for d in (-1, -2):\n",
    "        if cut_away[d]:\n",
    "            if cut_away[d] >= patch_size[d] // 2:\n",
    "                raise ValueError(f\"`cut_away` must be smaller than half the patch_size {patch_size}, got {cut_away}\")\n",
    "            if cut_away[d] * 2 + overlap[d] >= patch_size[d]:\n",
    "                raise ValueError(\n",
    "                    f\"2 * `cut_away` + `overlap` must be smaller than the patch_size {patch_size}\"\n",
    "                    f\", got cut_away={cut_away}, overlap={overlap}\"\n",
    "                )\n",
    "\n",
    "    for i in (-1, -2):\n",
    "        if overlap[i] >= patch_size[i]:\n",
    "            raise ValueError(f\"`overlap` must be smaller than the patch_size {patch_size}, got {overlap}\")\n",
    "\n",
    "    LEFT, RIGHT, TOP, BOTTOM = range(4)\n",
    "    padding = [0, 0, 0, 0]\n",
    "\n",
    "    is_cut_away = bool(any(cut_away))\n",
    "    if is_cut_away:\n",
    "        overlap = tuple(o + c * 2 for o, c in zip(overlap, cut_away))\n",
    "        padding[LEFT] = cut_away[-1]\n",
    "        padding[TOP] = cut_away[-2]\n",
    "        \n",
    "    stride = [patch_size[0] - overlap[0], patch_size[1] - overlap[1]]\n",
    "\n",
    "    if isinstance(window, torch.Tensor):\n",
    "        if window.shape != patch_size:\n",
    "            raise ValueError(\n",
    "                f\"`window` must match patch_size {patch_size}, got {window.shape}\"\n",
    "            )\n",
    "    elif window is True:\n",
    "        window = get_image_window(shape=patch_size)\n",
    "    elif callable(window):\n",
    "        window = get_image_window(shape=patch_size, window_function=window)\n",
    "    else:\n",
    "        window = None\n",
    "\n",
    "    if is_cut_away and window is not None:\n",
    "        window = window[cut_away[-2]: window.shape[-2] - cut_away[-2], cut_away[-1]: window.shape[-1] - cut_away[-1]]\n",
    "\n",
    "    image_shape = (image.shape[-2] + padding[TOP], image.shape[-1] + padding[LEFT])\n",
    "                   \n",
    "    if auto_pad:\n",
    "        for d, pad_pos in ((-1, RIGHT), (-2, BOTTOM)):\n",
    "            #grid_size = max(1, int(math.ceil((image_shape[d] + overlap[d] + stride[d] - 1) / stride[d])))\n",
    "            #recon_size = grid_size * stride[d] #+ overlap[d] #- cut_away[d] \n",
    "            # print(f\"X dim={d} grid{grid_size} * stride{stride[d]} + over{overlap[d]} = {recon_size}\")\n",
    "            #while recon_size < image_shape[d]:\n",
    "            #    grid_size += 1\n",
    "            #    recon_size = grid_size * stride[d] #+ overlap[d] #- cut_away[d]\n",
    "            grid_size = max(1, int(math.ceil(image_shape[d] / stride[d])))\n",
    "            while True:\n",
    "                needed_size = grid_size * stride[d] + overlap[d]\n",
    "                if needed_size >= image_shape[d]:\n",
    "                    break\n",
    "                print(f\"  INCREASED dim={d} needed={needed_size} image={image_shape[d]}\")\n",
    "                grid_size += 1\n",
    "                \n",
    "            if needed_size > image_shape[d]:\n",
    "                padding[pad_pos] = needed_size - image_shape[d]\n",
    "                print(f\"  needed > image, added padding {padding[pad_pos]}\")\n",
    "\n",
    "    # padding[BOTTOM] += 2\n",
    "    if any(padding):\n",
    "        image = F.pad(image, padding)\n",
    "\n",
    "    print(f\"stride={stride} grid={grid_size} image={image.shape[-2:]} pad={padding} overlap={overlap}\")\n",
    "\n",
    "    output = torch.zeros_like(image)\n",
    "    output_sum = torch.zeros_like(image[0])\n",
    "\n",
    "    for patch_batch, pos_batch in iter_image_patches(\n",
    "            image=image,\n",
    "            shape=patch_size,\n",
    "            stride=stride,\n",
    "            batch_size=batch_size,\n",
    "            with_pos=True,\n",
    "            verbose=verbose,\n",
    "    ):\n",
    "        patch_batch = function(patch_batch)\n",
    "        for patch, pos in zip(patch_batch, pos_batch):\n",
    "            ps = patch_size\n",
    "            if is_cut_away:\n",
    "                patch = patch[..., cut_away[-2]: patch.shape[-2] - cut_away[-2], cut_away[-1]: patch.shape[-1] - cut_away[-1]]\n",
    "                pos = (pos[0] + cut_away[0], pos[1] + cut_away[1])\n",
    "                ps = patch.shape[-2:]\n",
    "\n",
    "            add_pixel = 1.\n",
    "            if window is not None:\n",
    "                patch *= window\n",
    "                add_pixel = window\n",
    "\n",
    "            print(f\"ADDPATCH x={pos[-1]} y={pos[-2]} ps={ps} pixel={add_pixel}\")\n",
    "\n",
    "            output_sum[pos[-2]: pos[-2] + ps[-2], pos[-1]: pos[-1] + ps[-1]] += add_pixel\n",
    "            output[:, pos[-2]: pos[-2] + ps[-2], pos[-1]: pos[-1] + ps[-1]] += patch\n",
    "\n",
    "    display(VF.to_pil_image(resize(output_sum.unsqueeze(0) / output_sum.max(), 4)))\n",
    "    #print(output)\n",
    "    \n",
    "    mask = output_sum > 0\n",
    "    output[:, mask] /= output_sum[mask].unsqueeze(0)\n",
    "\n",
    "    if any(padding):\n",
    "        output = output[\n",
    "            ...,\n",
    "            padding[TOP]: output.shape[-2] - padding[BOTTOM],\n",
    "            padding[LEFT]: output.shape[-1] - padding[RIGHT],\n",
    "        ]\n",
    "\n",
    "    return output\n",
    "\n",
    "#  0123456789\n",
    "# .##.\n",
    "#   .##.\n",
    "#     .##.\n",
    "#       .##.\n",
    "\n",
    "#  0         1         2         3         4         5         6   4     7 2       8      |\n",
    "#  012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789\n",
    "#  ---...........................---                               |\n",
    "#                    ---...........................---             |\n",
    "#                                      ---...........................---\n",
    "#                                                        ---...........................---\n",
    "image = torch.ones(1, 64, 64) * .3\n",
    "output = map_image_patches(\n",
    "    image, patch_size=(32, 32), overlap=1, cut_away=5, #auto_pad=False,\n",
    "    function=lambda x: 1. - x,\n",
    ")\n",
    "print(output.mean())\n",
    "VF.to_pil_image(resize(make_grid([image, output, output]), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a7fc13-0fb5-4a74-b968-fcfccf56ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "(64 + 1) / 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62747b-c461-477b-b9f7-45efa5010f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "for i in range(4):\n",
    "    print(x + 32 - 2)\n",
    "    x += 17\n",
    "    #(80-2-10) / 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202eea3c-6952-4061-bb9a-37753de60b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d63654d-c7d9-4445-9d3f-f52a7287ec22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d8dd7-e132-4b76-82f6-a6bc8a44d141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c833af9f-0db1-4001-ae9d-3bf2852f2f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvDenoiser(\n",
    "    (3, 64, 64), [64, 128, 256],\n",
    "    stride=[1, 2, 2, 1],\n",
    "    kernel_size=3,\n",
    ")\n",
    "state = torch.load(\"../checkpoints/denoise/conv02-ks-3_chan-64,128,256_stride-1,2,2,1/best.pt\")\n",
    "print(f'inputs: {state[\"num_input_steps\"]:,}')\n",
    "model.load_state_dict(state[\"state_dict\"])\n",
    "print(f\"params: {num_module_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8169d22a-3c58-4adb-b24f-e326c1063548",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvDenoiser(\n",
    "    (3, 64, 64), \n",
    "    [64, 128, 128, 128, 64],\n",
    "    stride=[1, 1, 2, 2, 2, 1],\n",
    "    kernel_size=3,\n",
    ")\n",
    "state = torch.load(\"../checkpoints/denoise/conv04-ks-3_chan-64,128,128,128,64_stride-1,1,2,2,2,1/snapshot.pt\")\n",
    "print(f'inputs: {state[\"num_input_steps\"]:,}')\n",
    "model.load_state_dict(state[\"state_dict\"])\n",
    "print(f\"params: {num_module_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16540b91-062f-466f-a419-f7c210384ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.img2img import RBDN\n",
    "\n",
    "model = RBDN(\n",
    "    3, 3, 64,\n",
    "    num_branches=2,\n",
    "    num_hidden_layers=1,\n",
    "    \n",
    "    conv_kernel_size=9,\n",
    "    conv_padding=1,\n",
    "    \n",
    "    branch_conv_kernel_size=9,\n",
    "    branch_conv_padding=1,\n",
    "    branch_conv_stride=2,\n",
    ").eval()\n",
    "state = torch.load(\"../checkpoints/denoise/rbdn-noise-strong-ks-9_chans-64_layers-1_blayers-1_nbranch-2_bstride-2_pad-1/snapshot.pt\")\n",
    "print(f'inputs: {state[\"num_input_steps\"]:,}')\n",
    "model.load_state_dict(state[\"state_dict\"])\n",
    "print(f\"params: {num_module_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aba03c-66e2-4bc7-b74f-7f8689bfba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = VF.to_tensor(PIL.Image.open(\n",
    "    #\"/home/bergi/Pictures/there_is_no_threat.jpeg\"\n",
    "    \"/home/bergi/Pictures/__diverse/gordon_brown.jpg\",\n",
    "))\n",
    "\n",
    "VF.to_pil_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46692cbb-278b-49b0-9504-e9a6a94ec2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    part = 0+image#[:, 0:128, 0:128]\n",
    "    noise = torch.randn_like(part) \n",
    "    noise = noise[:1].repeat(3, 1, 1)\n",
    "    #noise = VF.gaussian_blur(noise, 5, 2)\n",
    "    part = (part + noise * .3).clamp(0, 1)\n",
    "    output = model(part.unsqueeze(0))[0]\n",
    "    display(VF.to_pil_image(make_grid([part, output])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2b44f4-3af7-49e8-8197-01f8dbedc6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def denoise(\n",
    "        image: torch.Tensor, \n",
    "        denoiser: nn.Module, \n",
    "        batch_size: int = 64,\n",
    "        overlap: Union[int, Tuple[int, int]] = 0,\n",
    "        auto_pad: bool = True,\n",
    "        verbose: bool = True,\n",
    "):\n",
    "    denoiser.eval()\n",
    "    return map_image_patches(\n",
    "        image=image, \n",
    "        function=lambda x: denoiser(x).clamp(0, 1),\n",
    "        patch_size=denoiser.shape[-2:],\n",
    "        batch_size=batch_size,\n",
    "        overlap=overlap,\n",
    "        auto_pad=auto_pad,\n",
    "        window=True if overlap else False,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "image2 = image\n",
    "#image2 = resize(image, .8)\n",
    "denoised = denoise(\n",
    "    image2 + 0.1 * torch.randn_like(image2),\n",
    "    model,\n",
    "    overlap=16,\n",
    ")\n",
    "for i in range(3):\n",
    "    denoised = denoise(denoised, model, overlap=12+i)\n",
    "display(VF.to_pil_image(denoised))\n",
    "display(VF.to_pil_image(signed_to_image(image2 - denoised)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f2f44d-221f-4129-9863-9b1c313416e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def denoise_image(\n",
    "        image: Union[torch.Tensor, str, Path], \n",
    "        #denoiser: nn.Module, \n",
    "        overlap: Union[int, Tuple[int, int]] = 0,\n",
    "):\n",
    "    if not isinstance(image, torch.Tensor):\n",
    "        image = VF.to_tensor(PIL.Image.open(image))[:3]\n",
    "    \n",
    "    denoised = denoise(\n",
    "        image, # + 0.1 * torch.randn_like(image),\n",
    "        model,\n",
    "        overlap=overlap,\n",
    "    )\n",
    "    #for i in range(3):\n",
    "    #    denoised = denoise(denoised, model, overlap=12+i)\n",
    "    images = [\n",
    "        image, denoised, signed_to_image(image - denoised)\n",
    "    ]\n",
    "    images = make_grid(images)\n",
    "   # images = resize(images, 2)\n",
    "    display(VF.to_pil_image(images))\n",
    "#display(VF.to_pil_image(signed_to_image(image2 - denoised)))\n",
    "denoise_image(\n",
    "    #\"/home/bergi/Pictures/clipig2/cthulhu-fractal-01.png\",\n",
    "    #\"~/Pictures/clipig2/cthulhu-fractal-01-many-steps.png\",\n",
    "    #\"/home/bergi/Pictures/clipig2/fractal-industrial-pipes.png\",\n",
    "    #\"/home/bergi/Pictures/clipig2/fractal-escher-2.png\",\n",
    "    #\"/home/bergi/Pictures/clipig2/fractal-escher.png\",\n",
    "    #\"/home/bergi/Pictures/clipig2/pixelart-fish.png\",\n",
    "    #\"/home/bergi/Pictures/clipig2/monkey-island.png\",\n",
    "    #\"/home/bergi/Pictures/clipig2/rocky-surface.png\",\n",
    "    #\"/home/bergi/Pictures/clipig2/waterfall-06-c.png\",\n",
    "    \"/home/bergi/Pictures/clipig2/maze-of-pipes-01.png\",\n",
    "\n",
    "    overlap=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c8dc29-cba6-4c27-ad1f-94a43cc3c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VF.to_pil_image(signed_to_image(image - denoised))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ace4b-ab96-4e79-9cd7-8b7120e9d743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df934d11-2748-4e7e-8d9e-efe4a74df381",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv = nn.Conv2d(1, 1, 3, padding=1)\n",
    "    x = torch.zeros(1, 32, 32)\n",
    "    for i in range(14):\n",
    "        x = conv(x)    \n",
    "        print(\"  \", x.shape)\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4c4043-01ed-4fd5-94d9-7fddd5035a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590592d-4ff9-42ea-b87b-44361a802aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd517d1-7a83-4abe-ba8b-1545556a3460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9ab44e-c011-49d7-8a39-edbe5010e1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
