{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11851ff1-c6f4-4735-9798-1648f35e3fbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union, Dict\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset, RandomSampler\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "import torchaudio.transforms as AT\n",
    "import torchaudio.functional as AF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display, Audio\n",
    "import torchaudio\n",
    "from torchaudio.io import StreamReader\n",
    "\n",
    "from src.datasets import *\n",
    "from src.algo import GreedyLibrary\n",
    "from src.util.image import *\n",
    "from src.util import *\n",
    "from src.patchdb import PatchDB, PatchDBIndex\n",
    "from src.models.encoder import *\n",
    "from src.models.cnn import *\n",
    "from src.models.util import *\n",
    "from src.models.transform import *\n",
    "from src.util.audio import *\n",
    "from src.util.files import *\n",
    "from src.util.embedding import *\n",
    "from scripts import datasets\n",
    "\n",
    "def resize(img, scale: float, mode: VF.InterpolationMode = VF.InterpolationMode.NEAREST):\n",
    "    return VF.resize(img, [max(1, int(s * scale)) for s in img.shape[-2:]], mode, antialias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3811770-a8ff-4153-85f4-93429eabb976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder = DalleDecoder(n_hid=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e6e05d-da84-4c2d-ac6a-e784ff1a1dc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = decoder(torch.randn(1, 8192, 8, 8))[0]\n",
    "print(output.shape)\n",
    "VF.to_pil_image(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e533f20a-8f78-4f16-8405-235a485e9b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b666904-9562-46e4-93b1-b2992e24d87e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f98ef3-2f74-4ae8-a979-8070c15b457a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906599c5-e85c-4ddd-841e-c41755570db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c1d95f-14a1-4e4e-b7ba-97dfab0c53d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809fd42b-78fc-4647-8f38-3cee97a0f121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca4e77-c1df-4e26-9af1-e528c183c8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f177a-d201-4da8-9d6b-5fd7c4f7f45a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a80a44-169a-4f22-8043-21ddb4488065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41310e0f-9427-4e8f-a215-4bb474c34ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e375d27f-7e27-4ace-af86-6c195aec4027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import attr\n",
    "import math\n",
    "from collections  import OrderedDict\n",
    "from functools    import partial\n",
    "\n",
    "logit_laplace_eps: float = 0.1\n",
    "\n",
    "@attr.s(eq=False)\n",
    "class Conv2d(nn.Module):\n",
    "\tn_in:  int = attr.ib(validator=lambda i, a, x: x >= 1)\n",
    "\tn_out: int = attr.ib(validator=lambda i, a, x: x >= 1)\n",
    "\tkw:    int = attr.ib(validator=lambda i, a, x: x >= 1 and x % 2 == 1)\n",
    "\n",
    "\tuse_float16:   bool         = attr.ib(default=True)\n",
    "\tdevice:        torch.device = attr.ib(default=torch.device('cpu'))\n",
    "\trequires_grad: bool         = attr.ib(default=False)\n",
    "\n",
    "\tdef __attrs_post_init__(self) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tw = torch.empty((self.n_out, self.n_in, self.kw, self.kw), dtype=torch.float32,\n",
    "\t\t\tdevice=self.device, requires_grad=self.requires_grad)\n",
    "\t\tw.normal_(std=1 / math.sqrt(self.n_in * self.kw ** 2))\n",
    "\n",
    "\t\tb = torch.zeros((self.n_out,), dtype=torch.float32, device=self.device,\n",
    "\t\t\trequires_grad=self.requires_grad)\n",
    "\t\tself.w, self.b = nn.Parameter(w), nn.Parameter(b)\n",
    "\n",
    "\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\t\tif self.use_float16 and 'cuda' in self.w.device.type:\n",
    "\t\t\tif x.dtype != torch.float16:\n",
    "\t\t\t\tx = x.half()\n",
    "\n",
    "\t\t\tw, b = self.w.half(), self.b.half()\n",
    "\t\telse:\n",
    "\t\t\tif x.dtype != torch.float32:\n",
    "\t\t\t\tx = x.float()\n",
    "\n",
    "\t\t\tw, b = self.w, self.b\n",
    "\n",
    "\t\treturn F.conv2d(x, w, b, padding=(self.kw - 1) // 2)\n",
    "\n",
    "def map_pixels(x: torch.Tensor) -> torch.Tensor:\n",
    "\tif len(x.shape) != 4:\n",
    "\t\traise ValueError('expected input to be 4d')\n",
    "\tif x.dtype != torch.float:\n",
    "\t\traise ValueError('expected input to have type float')\n",
    "\n",
    "\treturn (1 - 2 * logit_laplace_eps) * x + logit_laplace_eps\n",
    "\n",
    "def unmap_pixels(x: torch.Tensor) -> torch.Tensor:\n",
    "\tif len(x.shape) != 4:\n",
    "\t\traise ValueError('expected input to be 4d')\n",
    "\tif x.dtype != torch.float:\n",
    "\t\traise ValueError('expected input to have type float')\n",
    "\n",
    "\treturn torch.clamp((x - logit_laplace_eps) / (1 - 2 * logit_laplace_eps), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f561fab-c4c8-4554-91d0-9bde6e0998cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9cb14-2632-4cd1-83c3-b0962554c583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@attr.s(eq=False, repr=False)\n",
    "class EncoderBlock(nn.Module):\n",
    "\tn_in:     int = attr.ib(validator=lambda i, a, x: x >= 1)\n",
    "\tn_out:    int = attr.ib(validator=lambda i, a, x: x >= 1 and x % 4 ==0)\n",
    "\tn_layers: int = attr.ib(validator=lambda i, a, x: x >= 1)\n",
    "\n",
    "\tdevice:        torch.device = attr.ib(default=None)\n",
    "\trequires_grad: bool         = attr.ib(default=False)\n",
    "\n",
    "\tdef __attrs_post_init__(self) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.n_hid = self.n_out // 4\n",
    "\t\tself.post_gain = 1 / (self.n_layers ** 2)\n",
    "\n",
    "\t\tmake_conv     = partial(Conv2d, device=self.device, requires_grad=self.requires_grad)\n",
    "\t\tself.id_path  = make_conv(self.n_in, self.n_out, 1) if self.n_in != self.n_out else nn.Identity()\n",
    "\t\tself.res_path = nn.Sequential(OrderedDict([\n",
    "\t\t\t\t('relu_1', nn.ReLU()),\n",
    "\t\t\t\t('conv_1', make_conv(self.n_in,  self.n_hid, 3)),\n",
    "\t\t\t\t('relu_2', nn.ReLU()),\n",
    "\t\t\t\t('conv_2', make_conv(self.n_hid, self.n_hid, 3)),\n",
    "\t\t\t\t('relu_3', nn.ReLU()),\n",
    "\t\t\t\t('conv_3', make_conv(self.n_hid, self.n_hid, 3)),\n",
    "\t\t\t\t('relu_4', nn.ReLU()),\n",
    "\t\t\t\t('conv_4', make_conv(self.n_hid, self.n_out, 1)),]))\n",
    "\n",
    "\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\t\treturn self.id_path(x) + self.post_gain * self.res_path(x)\n",
    "\n",
    "@attr.s(eq=False, repr=False)\n",
    "class Encoder(nn.Module):\n",
    "\tgroup_count:     int = 4\n",
    "\tn_hid:           int = attr.ib(default=256,  validator=lambda i, a, x: x >= 64)\n",
    "\tn_blk_per_group: int = attr.ib(default=2,    validator=lambda i, a, x: x >= 1)\n",
    "\tinput_channels:  int = attr.ib(default=3,    validator=lambda i, a, x: x >= 1)\n",
    "\tvocab_size:      int = attr.ib(default=8192, validator=lambda i, a, x: x >= 512)\n",
    "\n",
    "\tdevice:              torch.device = attr.ib(default=torch.device('cpu'))\n",
    "\trequires_grad:       bool         = attr.ib(default=False)\n",
    "\tuse_mixed_precision: bool         = attr.ib(default=True)\n",
    "\n",
    "\tdef __attrs_post_init__(self) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tblk_range  = range(self.n_blk_per_group)\n",
    "\t\tn_layers   = self.group_count * self.n_blk_per_group\n",
    "\t\tmake_conv  = partial(Conv2d, device=self.device, requires_grad=self.requires_grad)\n",
    "\t\tmake_blk   = partial(EncoderBlock, n_layers=n_layers, device=self.device,\n",
    "\t\t\t\trequires_grad=self.requires_grad)\n",
    "\n",
    "\t\tself.blocks = nn.Sequential(OrderedDict([\n",
    "\t\t\t('input', make_conv(self.input_channels, 1 * self.n_hid, 7)),\n",
    "\t\t\t('group_1', nn.Sequential(OrderedDict([\n",
    "\t\t\t\t*[(f'block_{i + 1}', make_blk(1 * self.n_hid, 1 * self.n_hid)) for i in blk_range],\n",
    "\t\t\t\t('pool', nn.MaxPool2d(kernel_size=2)),\n",
    "\t\t\t]))),\n",
    "\t\t\t('group_2', nn.Sequential(OrderedDict([\n",
    "\t\t\t\t*[(f'block_{i + 1}', make_blk(1 * self.n_hid if i == 0 else 2 * self.n_hid, 2 * self.n_hid)) for i in blk_range],\n",
    "\t\t\t\t('pool', nn.MaxPool2d(kernel_size=2)),\n",
    "\t\t\t]))),\n",
    "\t\t\t('group_3', nn.Sequential(OrderedDict([\n",
    "\t\t\t\t*[(f'block_{i + 1}', make_blk(2 * self.n_hid if i == 0 else 4 * self.n_hid, 4 * self.n_hid)) for i in blk_range],\n",
    "\t\t\t\t('pool', nn.MaxPool2d(kernel_size=2)),\n",
    "\t\t\t]))),\n",
    "\t\t\t('group_4', nn.Sequential(OrderedDict([\n",
    "\t\t\t\t*[(f'block_{i + 1}', make_blk(4 * self.n_hid if i == 0 else 8 * self.n_hid, 8 * self.n_hid)) for i in blk_range],\n",
    "\t\t\t]))),\n",
    "\t\t\t('output', nn.Sequential(OrderedDict([\n",
    "\t\t\t\t('relu', nn.ReLU()),\n",
    "\t\t\t\t('conv', make_conv(8 * self.n_hid, self.vocab_size, 1, use_float16=False)),\n",
    "\t\t\t]))),\n",
    "\t\t]))\n",
    "\n",
    "\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\t\tif len(x.shape) != 4:\n",
    "\t\t\traise ValueError(f'input shape {x.shape} is not 4d')\n",
    "\t\tif x.shape[1] != self.input_channels:\n",
    "\t\t\traise ValueError(f'input has {x.shape[1]} channels but model built for {self.input_channels}')\n",
    "\t\tif x.dtype != torch.float32:\n",
    "\t\t\traise ValueError('input must have dtype torch.float32')\n",
    "\n",
    "\t\treturn self.blocks(x)\n",
    "    \n",
    "Encoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54439bb4-dfa8-401d-99ed-6d6c194b7ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(n_hid=24)\n",
    "print(f\"{num_module_parameters(encoder):,}\")\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4636ea16-df4b-4c01-9512-21bd39a4e21f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder(torch.rand(1, 3, 64, 32)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396a762-5149-4702-927e-c5040de09c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf4d21-18e8-4c8f-91f9-273f3ffd57de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e62ec4-3a1c-4960-9794-0d9b1b47b537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970c901-8f25-45f3-af56-430df928b921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69474d50-1db4-42a3-bf8e-aa40b2376168",
   "metadata": {},
   "source": [
    "# dev space-to-depth pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739fab1-ed5f-40a8-b546-83007a72f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class SpaceToDepthPool(nn.Module):\n",
    "    def __init__(self, n_channels: int, kernel_size: int = 2):\n",
    "        if kernel_size % 2 != 0:\n",
    "            raise ValueError(f\"`kernel_size` must be even, got {kernel_size}\")\n",
    "            \n",
    "        super().__init__()\n",
    "        \n",
    "        layers = OrderedDict()\n",
    "        for i in range(kernel_size // 2):\n",
    "            layers[f\"spd_{i + 1}\"] = SpaceToDepth()\n",
    "            layers[f\"conv_{i + 1}\"] = nn.Conv2d(n_channels * 4, n_channels, kernel_size=1)\n",
    "        \n",
    "        self.layers = nn.Sequential(layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)\n",
    "    \n",
    "    \n",
    "state = torch.randn(1, 4, 16, 16)\n",
    "ks = 4\n",
    "print(nn.MaxPool2d(kernel_size=ks)(state).shape)\n",
    "spd = SpaceToDepthPool(state.shape[-3], kernel_size=ks)\n",
    "print(spd(state).shape)\n",
    "print(spd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48162138-15b6-4027-ad30-895e35cab058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
