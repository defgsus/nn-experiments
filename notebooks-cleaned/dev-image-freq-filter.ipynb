{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e0cb7-a9f6-431e-9df4-02c5b93b0795",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import *\n",
    "from src.util import *\n",
    "from src.algo import *\n",
    "from src.models.decoder import *\n",
    "from src.models.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f06147-f11b-4faf-b5fe-55ca351d843b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resize(img, scale: float, mode: VF.InterpolationMode = VF.InterpolationMode.NEAREST):\n",
    "    return VF.resize(img, [max(1, int(s * scale)) for s in img.shape[-2:]], mode, antialias=False)\n",
    "\n",
    "def plot_samples(\n",
    "        iterable, \n",
    "        total: int = 32, \n",
    "        nrow: int = 8, \n",
    "        return_image: bool = False, \n",
    "        show_compression_ratio: bool = False,\n",
    "        label: Optional[Callable] = None,\n",
    "):\n",
    "    samples = []\n",
    "    labels = []\n",
    "    f = ImageFilter()\n",
    "    try:\n",
    "        for idx, entry in enumerate(tqdm(iterable, total=total)):\n",
    "            image = entry\n",
    "            if isinstance(entry, (list, tuple)):\n",
    "                image = entry[0]\n",
    "            if image.ndim == 4:\n",
    "                image = image.squeeze(0)\n",
    "            samples.append(image)\n",
    "            if show_compression_ratio:\n",
    "                labels.append(round(f.calc_compression_ratio(image), 3))\n",
    "            elif label is not None:\n",
    "                labels.append(label(entry) if callable(label) else idx)\n",
    "                \n",
    "            if len(samples) >= total:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    if labels:\n",
    "        image = VF.to_pil_image(make_grid_labeled(samples, nrow=nrow, labels=labels))\n",
    "    else:\n",
    "        image = VF.to_pil_image(make_grid(samples, nrow=nrow))\n",
    "    if return_image:\n",
    "        return image\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedf40ba-0de7-4eab-a97f-1db873f4a941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SHAPE = (3, 64, 64)\n",
    "dataset = TensorDataset(torch.load(f\"../datasets/kali-uint8-{SHAPE[-2]}x{SHAPE[-1]}.pt\")[:1000])\n",
    "dataset = TransformDataset(dataset, dtype=torch.float, multiply=1./255.)\n",
    "print(len(dataset))\n",
    "VF.to_pil_image(make_grid_labeled(\n",
    "    [i[0] for i, _ in zip(dataset, range(8*8))]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e19990f-acb7-4e09-b410-7cb2a881a2d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_filter(image: torch.Tensor) -> torch.Tensor:\n",
    "    spec = torch.fft.fft2(image)\n",
    "    spec[..., 1:, 1:] = 0\n",
    "    #return spec\n",
    "    repro = torch.fft.ifft2(spec).real\n",
    "    #return repro\n",
    "    return (image - repro).clamp_min(0)\n",
    "\n",
    "images = next(iter(DataLoader(dataset, batch_size=8)))[0]\n",
    "display(VF.to_pil_image(make_grid(list(torch.fft.fft2(images).real.abs()) + list(torch.fft.fft2(images).imag.abs()), nrow=len(images), normalize=True,)))\n",
    "output = image_filter(images)\n",
    "print(output.shape)\n",
    "#print(output)\n",
    "display(VF.to_pil_image(resize(make_grid(list(images) + list((output).clamp(0, 1)), nrow=len(images)), 2)))\n",
    "print(\"l1\", (images - output).abs().mean())\n",
    "display(VF.to_pil_image(resize(make_grid(list((images - output).abs()), normalize=True, scale_each=True, nrow=len(images)), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e5f99e-0ff7-43ab-a91b-03b264b74314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.fft.hfft2(images)\n",
    "print(\"x\", type(x), x.shape)\n",
    "#x = torch.fft.fftshift(x, -3)\n",
    "x[..., 6:, 6:] = 0\n",
    "#x.imag *= .5\n",
    "y = torch.fft.ifft2(x)\n",
    "print(\"y\", type(y), y.shape)\n",
    "display(VF.to_pil_image(resize(make_grid((y).real.clamp(0, 1), nrow=len(images)), 2)))\n",
    "display(VF.to_pil_image(resize(make_grid((images - y).real.clamp(0, 1), nrow=len(images)), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c1666b-979c-4213-80ec-8e457f974c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = VF.gaussian_blur(images, 3, sigma=1.)\n",
    "print(\"y\", type(y), y.shape)\n",
    "display(VF.to_pil_image(resize(make_grid((y).real.clamp(0, 1), nrow=len(images)), 2)))\n",
    "display(VF.to_pil_image(resize(make_grid((images - y).real.clamp(0, 1), nrow=len(images)), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c8d12-58e1-4366-b0e1-f5c31c319d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FFTLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Converts an n-dim input to fourier space.\n",
    "\n",
    "    if `allow_complex==False`, the output shape for images (B, C, H, W) will be:\n",
    "\n",
    "        type   concat_dim  output shape\n",
    "        fft    -1          B, C, H, W * 2\n",
    "        rfft   -1          B, C, H, W + 2\n",
    "        hfft   -1          B, C, H, W * 2 - 2\n",
    "\n",
    "        fft    -2          B, C, H * 2, W\n",
    "        rfft   -2          B, C, H * 2, W // 2 + 1\n",
    "        hfft   -2          B, C, H, W * 2 - 2        # hfft does not produce complex data so `concat_dim` is unused\n",
    "\n",
    "        fft    -2          B, C * 2, H, W\n",
    "        rfft   -2          B, C * 2, H, W // 2 + 1\n",
    "        hfft   -2          B, C, H, W * 2 - 2\n",
    "\n",
    "    if `allow_complex==True`, the output might be complex data and shapes are:\n",
    "\n",
    "        type   output shape          is complex\n",
    "        fft    B, C, H, W            yes\n",
    "        rfft   B, C, H, W // 2 + 1   yes\n",
    "        hfft   B, C, H, W * 2 - 2    no\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            type: str = \"fft\",\n",
    "            allow_complex: bool = False,\n",
    "            concat_dim: int = -1,\n",
    "            norm: str = \"forward\",\n",
    "            inverse: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        supported_types = [\n",
    "            name[:-1] for name in dir(torch.fft)\n",
    "            if name.endswith(\"fftn\") and not name.startswith(\"i\")\n",
    "        ]\n",
    "        if type not in supported_types:\n",
    "            raise ValueError(f\"Expected `type` to be one of {', '.join(supported_types)}, got '{type}'\")\n",
    "\n",
    "        supported_norm = (\"forward\", \"backward\", \"ortho\")\n",
    "        if norm not in supported_norm:\n",
    "            raise ValueError(f\"Expected `norm` to be one of {', '.join(supported_norm)}, got '{norm}'\")\n",
    "\n",
    "        self.type = type\n",
    "        self.norm = norm\n",
    "        self.allow_complex = allow_complex\n",
    "        self.concat_dim = concat_dim\n",
    "        self.inverse = inverse\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.inverse and not self.allow_complex and not torch.is_complex(x) and not self.type == \"hfft\":\n",
    "            x = torch.complex(\n",
    "                    torch.slice_copy(x, self.concat_dim, 0, x.shape[self.concat_dim] // 2),\n",
    "                    torch.slice_copy(x, self.concat_dim, x.shape[self.concat_dim] // 2),\n",
    "            )\n",
    "\n",
    "        func_name = f\"{'i' if self.inverse else ''}{self.type}n\"\n",
    "        output = getattr(torch.fft, func_name)(x, norm=self.norm)\n",
    "\n",
    "        if not self.inverse:\n",
    "            if not self.allow_complex and torch.is_complex(output):\n",
    "                output = torch.concat([output.real, output.imag], dim=self.concat_dim)\n",
    "        else:\n",
    "            output = output.real\n",
    "\n",
    "        return output\n",
    "\n",
    "    \n",
    "input = torch.rand(1, 3, 24, 32)\n",
    "output = FFTLayer(\"fft\", False, -1)(input)\n",
    "print(input.shape, \"->\", output.shape, torch.is_complex(output), output.real.min(), output.real.max(), output.real.sum())\n",
    "#display(VF.to_pil_image(resize(make_grid(output, normalize=False, nrow=len(images)), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fc6f9c-392f-4cee-b821-f5e81d0cfa0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#torch.slice_copy(input, -1, 4).shape\n",
    "torch.fft.ifft(output).real"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
