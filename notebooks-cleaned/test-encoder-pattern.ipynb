{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ce722-a145-414a-8592-2965fb211ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset, RandomSampler\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import *\n",
    "from src.util import *\n",
    "from src.algo import *\n",
    "from src.models.cnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ddbfa3-0c00-411e-9cc0-5d085ce3c36d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts.train_contrastive import EncoderConv, EncoderTrans\n",
    "\n",
    "SHAPE = (3, 64, 64)\n",
    "if 0:\n",
    "    dataset = TensorDataset(torch.load(f\"../datasets/pattern-{SHAPE[-3]}x{SHAPE[-2]}x{SHAPE[-1]}-uint.pt\"))\n",
    "    dataset = TransformDataset(dataset, dtype=torch.float, multiply=1./255.)#, transforms=[VT.Grayscale()])\n",
    "else:\n",
    "    dataset = TensorDataset(torch.load(f\"../datasets/kali-uint8-{SHAPE[-2]}x{SHAPE[-1]}.pt\"))\n",
    "    dataset = TransformDataset(dataset, dtype=torch.float, multiply=1./255.)#, transforms=[VT.Grayscale()])\n",
    "\n",
    "assert SHAPE == dataset[0][0].shape\n",
    "print(f\"{len(dataset):,} x {SHAPE}\")\n",
    "VF.to_pil_image(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceadc787-f146-4046-89ec-2eec10c6ac1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ToRGB(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.repeat(1, 3, 1, 1)\n",
    "    \n",
    "if 0:\n",
    "    CODE_SIZE = 2\n",
    "\n",
    "    model = EncoderConv(SHAPE, code_size=CODE_SIZE, channels=[32, 64], kernel_size=16, pool_kernel_size=16, batch_norm=True)\n",
    "    model.load_state_dict(torch.load(\"../checkpoints/pattern-c7-l2/best.pt\")[\"state_dict\"])\n",
    "    \n",
    "elif 0:\n",
    "    CODE_SIZE = 2\n",
    "\n",
    "    model = EncoderConv(SHAPE, code_size=CODE_SIZE, channels=[32, 32, 64], kernel_size=16, pool_kernel_size=16, batch_norm=True)\n",
    "    model.load_state_dict(torch.load(\"../checkpoints/pattern-c8-l3/snapshot.pt\")[\"state_dict\"])\n",
    "    \n",
    "elif 1:\n",
    "    CODE_SIZE = 512\n",
    "    from scripts.train_from_dataset import EncoderMLP, EncoderTrans\n",
    "    #model = EncoderMLP((3, *SHAPE[-2:]), channels=[CODE_SIZE])\n",
    "    #model.load_state_dict(torch.load(\"../checkpoints/clip2/best.pt\")[\"state_dict\"])\n",
    "    model = EncoderTrans(SHAPE, code_size=CODE_SIZE)\n",
    "    model.load_state_dict(torch.load(\"../checkpoints/clip5-tr/best.pt\")[\"state_dict\"])\n",
    "    sequence = []\n",
    "    if SHAPE[0] == 1:\n",
    "        sequence.append(ToRGB())\n",
    "    sequence.append(model)\n",
    "    model = nn.Sequential(*sequence)\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98719e6-01a3-4a0a-a9f7-b8e49ab6cc89",
   "metadata": {},
   "source": [
    "# dataset -> features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df73e7-3234-48bf-af0c-77100d062f43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_dataset(dataset, max_count=10_000):\n",
    "    feature_list = []\n",
    "    count = 0\n",
    "    try:\n",
    "        with tqdm(total=len(dataset)) as progress:\n",
    "            for image_batch in tqdm(DataLoader(dataset, batch_size=50)):\n",
    "                #print(image_batch[0].shape)\n",
    "                features = model(image_batch[0])\n",
    "                #features = features / torch.norm(features, dim=-1, keepdim=True)\n",
    "                feature_list.append(features)\n",
    "                progress.update(features.shape[0])\n",
    "                count += features.shape[0]\n",
    "                if count >= max_count:\n",
    "                    break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    return torch.cat(feature_list)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = encode_dataset(dataset)\n",
    "\n",
    "features_n = features / features.norm(dim=-1, keepdim=True)\n",
    "print(\"shape:\", features.shape)\n",
    "print(\"min/max:\", features.min(), features.max(), features.mean())\n",
    "VF.to_pil_image(features[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808de33e-e727-42bf-b314-c3e8c77e2ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.line(features_n[:10].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b971ec-9e2e-4bd0-a781-9237af6369cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VF.to_pil_image(make_grid([dataset[i][0] for i in range(10)], nrow=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef56914-1d38-4959-8c9e-b55d2552c847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.line(features.std(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107d823-d4f8-489a-a580-91ee651b46c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "    x=features_n[:1000, 0] * torch.linspace(0.5, 1, 1000), \n",
    "    y=features_n[:1000, 1] * torch.linspace(0.5, 1, 1000), \n",
    "    width=400, height=400, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8892ce75-2c79-4243-89d8-7c96a50e80b7",
   "metadata": {},
   "source": [
    "# sort features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9187d194-8694-4cdb-bcd1-ea0e4d2c21dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "reduction = TSNE(1, verbose=1)\n",
    "positions = torch.Tensor(reduction.fit_transform(features_n)).reshape(-1)\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be754078-f2a0-4f4a-8ab4-c76fc58af218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, indices = torch.sort(positions)\n",
    "images = [\n",
    "    VF.resize(dataset[i][0], (32, 32), VF.InterpolationMode.NEAREST)\n",
    "    for i in itertools.chain(indices[:500], indices[-500:])\n",
    "]\n",
    "VF.to_pil_image(make_grid(images, nrow=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75a3e06-e759-4b5f-8e23-bba9d8983605",
   "metadata": {},
   "source": [
    "# save full sorted image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f48a90a-2258-4982-9aa7-82105156d9d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = [\n",
    "    VF.resize(dataset[i][0], (32, 32), VF.InterpolationMode.NEAREST)\n",
    "    for i in indices\n",
    "]\n",
    "big_image = VF.to_pil_image(make_grid(images, nrow=64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a4b9fd-4948-4ffe-830a-0bb1bccc60da",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_image.save(Path(\"~/Pictures/pattern-tsne1d-of-distilled-clip-transformer-10x256.png\").expanduser())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc37650-8bcf-4bc0-8e73-c63bd3e40a76",
   "metadata": {},
   "source": [
    "# plot similars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c6b9a-daf9-4b4d-a51d-9815354e2d38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_similar_indices(feat, count: int = 10):\n",
    "    #feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "    dot = feat @ features_n.T\n",
    "    _, indices = torch.sort(dot, descending=True)\n",
    "    return indices[:, :count]\n",
    "\n",
    "def plot_similar(indices: Iterable[int], count: int = 10):\n",
    "    indices = list(indices)\n",
    "    sim_indices = get_similar_indices(torch.cat([\n",
    "        features_n[i].unsqueeze(0) for i in indices\n",
    "    ]), count=count)\n",
    "    images = [dataset[i][0] for i in sim_indices.T.reshape(-1)] \n",
    "    display(VF.to_pil_image(make_grid(images, nrow=len(indices))))\n",
    "    \n",
    "#get_similar_indices(features[0:2])\n",
    "plot_similar(list(range(2000, 2020)), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d0f11-e9ed-40ae-a149-c182dbe9688b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54286f81-6d1c-4694-b06b-0789788da73c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
