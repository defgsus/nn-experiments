{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ed52b-36ad-43d0-99d3-236d6bb06640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import * \n",
    "from src.util.image_filter import ImageFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae915d9-f6cd-4cff-b64a-4f22f148fed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#base_images = ImageFolder(Path(\"~/Pictures/__diverse\").expanduser())\n",
    "base_images = ImageFolder(Path(\"~/Pictures/photos/katjacam\").expanduser(), recursive=True)\n",
    "#base_images = ImageFolder(Path(\"~/Pictures/diffusion/\").expanduser(), recursive=True)\n",
    "target_shape = (3, 64, 64)\n",
    "base_images._get_filenames()\n",
    "len(base_images._filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9edee6e-bf84-472a-a1bc-d8641cf03e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_filter = ImageFilter(\n",
    "    #min_compression_ratio=0.9,\n",
    "    min_blurred_compression_ratio=0.3,\n",
    "    \n",
    ")\n",
    "\n",
    "def iter_images():\n",
    "    cropper = VT.RandomCrop(target_shape[-2:])\n",
    "    def _iter_crops(image, min_size: int):\n",
    "        count = 2 + max(1, min(40, (min_size - 400) // 200))\n",
    "        min_scale = max(.05, 1. - min_size / 400)\n",
    "        #print(min_size, min_scale)\n",
    "        num_yielded = 0\n",
    "        num_tried = 0\n",
    "        while num_yielded < count and num_tried < count * 5:\n",
    "            img = image\n",
    "            #image = VT.RandomAffine(degrees=30, scale=[2, 2])(image)\n",
    "            scale = min_scale + math.pow(random.random() * (1. - min_scale), 10.)\n",
    "            #if scale < random.random():\n",
    "            #    img = VT.RandomPerspective(distortion_scale=.7)(img)\n",
    "            #    crop_x = max(target_shape[-1], img.shape[-1] // 5)\n",
    "            #    crop_y = max(target_shape[-2], img.shape[-2] // 5)\n",
    "            #    img = VF.crop(img, crop_y // 2, crop_x // 2, img.shape[-2] - crop_y, img.shape[-1] - crop_x)\n",
    "            img = VF.resize(img, [\n",
    "                max(target_shape[-2], int(image.shape[-2] * scale)), \n",
    "                max(target_shape[-1], int(image.shape[-1] * scale)),\n",
    "            ])\n",
    "            if random.random() < .5:\n",
    "                center = center=[random.randrange(target_shape[-2]), random.randrange(target_shape[-2])]\n",
    "                img = VT.RandomRotation(30, center=center)(img)\n",
    "            img = cropper(img)\n",
    "            \n",
    "            num_tried += 1\n",
    "            if image_filter(img):\n",
    "                yield img\n",
    "                num_yielded += 1\n",
    "    \n",
    "    last_image_idx = 0\n",
    "    for idx, base_image in enumerate(base_images):\n",
    "        if idx - last_image_idx > 100:\n",
    "            last_image_idx = idx\n",
    "            print(f\"image: #{idx}\")\n",
    "        \n",
    "        image = set_image_channels(base_image, target_shape[0])\n",
    "        #if not image_filter(image):\n",
    "        #    continue\n",
    "        #yield image\n",
    "        yield image_resize_crop(image, target_shape[-2:])\n",
    "        \n",
    "        min_size = min(*image.shape[-2:])\n",
    "        if min_size >= 200:\n",
    "            yield from _iter_crops(image, min_size)\n",
    "\n",
    "def plot_images(iterable, total=16, nrow=16):\n",
    "    samples = []\n",
    "    try:\n",
    "        for i in tqdm(iter_images(), total=total):\n",
    "            i = i.clamp(0, 1)\n",
    "            samples.append(i)\n",
    "            if len(samples) >= total:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    display(VF.to_pil_image(make_grid(samples, nrow=nrow)))\n",
    "    \n",
    "plot_images(iter_images(), 16*16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d225cf1-7d2a-4428-abe4-f274920989b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples[15].view(3, -1).std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c76813-e131-4e04-bd72-96e55df6be0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILENAME = f\"../datasets/photos-{target_shape[-2]}x{target_shape[-1]}-bcr03.pt\"\n",
    "\n",
    "def store_dataset(\n",
    "        images: Iterable,\n",
    "        dtype=torch.float32,\n",
    "        #image_folder=\"~/Pictures/__diverse/\",\n",
    "        output_filename=FILENAME,\n",
    "        max_megabyte=1_000,\n",
    "):\n",
    "    tensor_batch = []\n",
    "    tensor_size = 0\n",
    "    last_print_size = 0\n",
    "    try:\n",
    "        for image in tqdm(images):\n",
    "            if len(image.shape) < 4:\n",
    "                image = image.unsqueeze(0)\n",
    "            tensor_batch.append(image.clamp(0, 1))\n",
    "            tensor_size += math.prod(image.shape) * 4\n",
    "\n",
    "            if tensor_size - last_print_size > 1024 * 1024 * 50:\n",
    "                last_print_size = tensor_size\n",
    "\n",
    "                print(f\"size: {tensor_size:,}\")\n",
    "\n",
    "            if tensor_size >= max_megabyte * 1024 * 1024:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    tensor_batch = torch.cat(tensor_batch)\n",
    "    torch.save(tensor_batch, output_filename)\n",
    "\n",
    "store_dataset(iter_images())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75290a4b-136d-4ad4-9827-cc860d21bbb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = TensorDataset(torch.load(FILENAME))\n",
    "dl = DataLoader(ds, shuffle=True, batch_size=16**2)\n",
    "for batch in dl:\n",
    "    img = VF.to_pil_image(make_grid(batch[0], nrow=16))\n",
    "    break\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c1f44-9eb9-4911-bf43-682fe4db7a01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = torch.rand(16, 8)\n",
    "torch.concat([t, t]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
