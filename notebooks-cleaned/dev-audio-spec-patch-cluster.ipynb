{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a98d4-6c7a-4899-9e81-bd8a489966c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union, Dict\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset, RandomSampler\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "import torchaudio.transforms as AT\n",
    "import torchaudio.functional as AF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display, Audio\n",
    "import torchaudio\n",
    "from torchaudio.io import StreamReader\n",
    "\n",
    "from src.datasets import *\n",
    "from src.algo import GreedyLibrary\n",
    "from src.util.image import *\n",
    "from src.util import to_torch_device, iter_batches\n",
    "from src.patchdb import PatchDB, PatchDBIndex\n",
    "from src.models.encoder import *\n",
    "from src.util.audio import *\n",
    "from src.util.files import *\n",
    "from src.util.embedding import *\n",
    "from scripts import datasets\n",
    "\n",
    "def resize(img, scale: float, mode: VF.InterpolationMode = VF.InterpolationMode.NEAREST):\n",
    "    return VF.resize(img, [max(1, int(s * scale)) for s in img.shape[-2:]], mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b700d-c3c3-48ad-af12-e632d39598d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8f856ab-1fe2-4701-8a27-779fa7aad7f2",
   "metadata": {},
   "source": [
    "# create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c428cf-be3f-4e24-aea9-304d92b48371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SPEC_SHAPE = (128, 128)\n",
    "SHAPE = (8, 8)\n",
    "PATCHES_PER_SLICE = math.prod(s1 // s2 for s1, s2 in zip(SPEC_SHAPE, SHAPE))\n",
    "TOTAL_SLICES = 20103\n",
    "TOTAL_PATCHES = TOTAL_SLICES * PATCHES_PER_SLICE\n",
    "print(PATCHES_PER_SLICE, \"patches per slice\")\n",
    "\n",
    "ds = datasets.audio_slice_dataset(\n",
    "    interleave_files=200,\n",
    "    mono=True,\n",
    "    spectral_size=SPEC_SHAPE[-1],\n",
    "    spectral_patch_shape=SHAPE,\n",
    "    spectral_normalize=1,\n",
    ")\n",
    "\n",
    "# ds = ImageFilterIterableDataset(ds, ImageFilter(min_std=.1))\n",
    "\n",
    "grid = [patch for i, patch in zip(range(32*32), ds)]\n",
    "img = make_grid(grid, nrow=32, normalize=True)\n",
    "VF.to_pil_image(VF.resize(img, [s * 3 for s in img.shape[-2:]], VF.InterpolationMode.NEAREST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b335c273-492a-4f21-9be6-b69bb825a8ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max([g.max() for g in grid])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb330b0-197d-4ff6-878e-ce46ff856024",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade9a1d6-e8de-4622-ac0c-1f8dd3af9c85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = IncrementalPCA(math.prod(SHAPE), batch_size=1024)\n",
    "\n",
    "try:\n",
    "    with tqdm(total=TOTAL_PATCHES) as progress:\n",
    "        for batch in DataLoader(ds, batch_size=1024):\n",
    "            pca.partial_fit(batch.flatten(1))\n",
    "            progress.update(batch.shape[0])\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73953d4b-6b57-4e4c-afbe-a4bb063219bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VF.to_pil_image(resize(make_grid(torch.Tensor(pca.components_).view(64, 1, 8, 8), normalize=True), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38a255c-207a-4925-aedc-88f13ecde2a1",
   "metadata": {},
   "source": [
    "# greedy lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e380719-248a-4831-ae3b-7b5899c434d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lib = GreedyLibrary(1, shape=(1, *SHAPE))\n",
    "\n",
    "try:\n",
    "    with tqdm(total=TOTAL_PATCHES) as progress:\n",
    "        for batch in DataLoader(ds, batch_size=1024):\n",
    "            lib.fit(\n",
    "                batch, \n",
    "                metric=\"corr\",\n",
    "                max_entries=math.prod(SHAPE),\n",
    "                grow_if_distance_above=.1,\n",
    "            )\n",
    "            progress.update(batch.shape[0])\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c5aa1f-a00d-4866-b164-1ce18ff8149f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lib.plot_entries(sort_by=\"hits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdcec1c-cd8f-488e-887e-398b59e2872f",
   "metadata": {},
   "source": [
    "# multiple autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3eb896-2942-45fa-894e-597681f38983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts.train_vae_spectral import SimpleVAE\n",
    "\n",
    "vaes = []\n",
    "for filename, shape in (\n",
    "        (\"../checkpoints/spec2/best.pt\", (8, 8)),\n",
    "        (\"../checkpoints/spec3-8x128/best.pt\", (8, 128)),\n",
    "        (\"../checkpoints/spec4-128x8/best.pt\", (128, 8)),\n",
    "):\n",
    "    vae = SimpleVAE(shape, latent_dims=math.prod(shape) // 8)\n",
    "    data = torch.load(filename)\n",
    "    print(f\"{filename} inputs: {data['num_input_steps']:,}\")\n",
    "    vae.load_state_dict(data[\"state_dict\"])\n",
    "    vaes.append((vae, shape))\n",
    "    print(vae)\n",
    "    for W in (vae.encoder.linear_mu.weight, vae.decoder[0].weight.permute(1, 0)):\n",
    "        print(float(W.min()), float(W.max()))\n",
    "        display(VF.to_pil_image(resize(signed_to_image(make_grid(W.view(-1, 1, *shape)[:16])), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a811222-f524-4ea1-9923-09403b301c29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355da83d-d2f7-42e8-8d41-4e9c37cfbbbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a352dd9d-1ddc-43d9-9b75-6e779aa74c0b",
   "metadata": {},
   "source": [
    "# create KMeans cluster for each autoencoder / spec-patch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66daffae-99a6-41b3-8a8f-c67349548f7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 256\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "clusterers = []\n",
    "for vae, shape in vaes:\n",
    "    ds_local = datasets.audio_slice_dataset(\n",
    "        interleave_files=200,\n",
    "        mono=True,\n",
    "        spectral_size=SPEC_SHAPE[-1],\n",
    "        spectral_normalize=1_000,\n",
    "        spectral_patch_shape=shape,\n",
    "    )\n",
    "    clusterer = MiniBatchKMeans(\n",
    "        n_clusters=NUM_CLUSTERS,\n",
    "        batch_size=1024,\n",
    "        random_state=23,\n",
    "        n_init=1,\n",
    "        reassignment_ratio=.1,\n",
    "    )\n",
    "    clusterers.append(clusterer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            patches_per_slice = math.prod(s1 // s2 for s1, s2 in zip(SPEC_SHAPE, shape))\n",
    "            W = vae.encoder.linear_mu.weight\n",
    "            with tqdm(\n",
    "                    total=TOTAL_SLICES * patches_per_slice, \n",
    "                    desc=f\"cluster spec-shape: {tuple(shape)}, encoder: {W.shape[-1]}->{W.shape[-2]}\"\n",
    "            ) as progress:\n",
    "                hist_sum = None\n",
    "                for i, batch in enumerate(DataLoader(ds_local, batch_size=1024)):\n",
    "                    embeddings = vae.encoder(batch)\n",
    "                    clusterer.partial_fit(embeddings.flatten(1))\n",
    "                    progress.update(batch.shape[0])\n",
    "                    \n",
    "                    if False and i > 10:\n",
    "                        labels = clusterer.predict(embeddings.flatten(1))\n",
    "                        hist, _ = np.histogram(labels, clusterer.n_clusters)\n",
    "                        if hist_sum is None:\n",
    "                            hist_sum = hist\n",
    "                        else:\n",
    "                            hist_sum = hist_sum + hist\n",
    "                        if i % 200 == 0:\n",
    "                            print(clusterer.inertia_, hist_sum.max(), hist_sum)\n",
    "                            hist_sum = None\n",
    "                            \n",
    "        except KeyboardInterrupt:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95f096a-f3c4-4e4c-8f09-dd435340d2f4",
   "metadata": {},
   "source": [
    "## test single clusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53614c-22b2-41ec-90ca-bb2c4880a76f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 1\n",
    "with torch.no_grad():\n",
    "    ds_local = datasets.audio_slice_dataset(\n",
    "        #interleave_files=200,\n",
    "        mono=True,\n",
    "        spectral_size=SPEC_SHAPE[-1],\n",
    "        spectral_normalize=1_000,\n",
    "        spectral_patch_shape=vaes[idx][1],\n",
    "        #seek_offset=30,\n",
    "    )\n",
    "    hist_sum = None\n",
    "    max_v = 0\n",
    "    for i, spec in zip(tqdm(range(1000)), ds_local):\n",
    "        max_v = max(max_v, float(spec.max()))\n",
    "    print(\"MAX\", max_v)\n",
    "    hists = []\n",
    "    for i, patches in zip(tqdm(range(20)), DataLoader(ds_local, batch_size=1000)):\n",
    "        #patches = (patches / max_v).clamp(0, 1)\n",
    "        #patches = torch.concat(list(iter_image_patches(spec, vaes[idx][1])))\n",
    "        embeddings = vaes[idx][0].encoder.linear_mu(patches.flatten(1))\n",
    "        labels = clusterers[idx].predict(embeddings)\n",
    "        hist, _ = np.histogram(labels, clusterers[idx].n_clusters)\n",
    "        hists.append(torch.Tensor(hist).unsqueeze(0))\n",
    "        if hist_sum is None:\n",
    "            hist_sum = hist\n",
    "        else:\n",
    "            hist_sum = hist_sum + hist\n",
    "\n",
    "hists = torch.concat(hists)\n",
    "hists = hists / hists.max() \n",
    "display(VF.to_pil_image(hists))\n",
    "px.line(hists.sum(0))#.type(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c68fbc-b172-41c9-a8e5-63de1aac619b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.histogram(np.array([0, 1, 1, 1, 2]), 4, range=(0, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb77297b-5163-49aa-a430-f191dc41fd19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 2\n",
    "with torch.no_grad():\n",
    "    audio, sr = torchaudio.load(\"/home/bergi/Music/theDropper/05 CD Track 05.mp3\")\n",
    "    audio = audio.mean(0)\n",
    "    slices = list(iter_audio_slices((audio, ), 44100))\n",
    "    print(len(slices), \"slices\")\n",
    "    \n",
    "    speccer = AT.MelSpectrogram(\n",
    "        sample_rate=au.sample_rate,\n",
    "        n_fft=1024 * 2,\n",
    "        win_length=au.sample_rate // 30,\n",
    "        hop_length=au.sample_rate // au.spectral_shape[-1],\n",
    "        n_mels=au.spectral_shape[-2],\n",
    "        power=1.,\n",
    "    )\n",
    "    specs = [speccer(slice) for slice in slices if slice.shape[-1] == au.slice_size]\n",
    "    \n",
    "    hist_sum = None\n",
    "    max_v = 0\n",
    "    for spec in specs:\n",
    "        max_v = max(max_v, float(spec.max()))\n",
    "    print(\"MAX\", max_v)\n",
    "    hists = []\n",
    "    embeddingss = []\n",
    "    for spec in specs:\n",
    "        spec = (spec / max_v).clamp(0, 1)\n",
    "        patches = torch.concat(list(iter_image_patches(spec.unsqueeze(0), vaes[idx][1])))\n",
    "        #display(VF.to_pil_image(make_grid(patches.unsqueeze(1), nrow=128)))\n",
    "        embeddings = vaes[idx][0].encoder.linear_mu(patches.flatten(1))\n",
    "        # print(patches.shape, embeddings.shape)\n",
    "        embeddingss.append(embeddings)\n",
    "        labels = clusterers[idx].predict(embeddings)\n",
    "        # print(embeddings.shape, labels.shape, labels)\n",
    "        hist, _ = np.histogram(labels, clusterers[idx].n_clusters, range=(0, clusterers[idx].n_clusters - 1))\n",
    "        # print(hist)\n",
    "        hists.append(torch.Tensor(hist).unsqueeze(0))\n",
    "        if hist_sum is None:\n",
    "            hist_sum = hist\n",
    "        else:\n",
    "            hist_sum = hist_sum + hist\n",
    "\n",
    "embeddingss = torch.concat(embeddingss)\n",
    "embeddingss = embeddingss / embeddingss.max()\n",
    "# display(VF.to_pil_image(embeddingss))\n",
    "hists = torch.concat(hists)\n",
    "hists = hists / hists.max() \n",
    "display(VF.to_pil_image(hists))\n",
    "px.line(hists.sum(0))#.type(torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de826706-4e50-4708-a38c-e1a6c9e85a52",
   "metadata": {
    "tags": []
   },
   "source": [
    "# create AudioUnderstander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b839eb-e374-471a-924f-3cd7ff2ce1e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AudioUnderstander:\n",
    "    \"\"\"\n",
    "    Based on: \n",
    "\n",
    "    Attention is All You Need?\n",
    "    Good Embeddings with Statistics are Enough: Large Scale Audio Understanding without Transformers/ Convolutions/ BERTs/ Mixers/ Attention/ RNNs or ....\n",
    "    Prateek Verma\n",
    "    https://browse.arxiv.org/pdf/2110.03183.pdf\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            sample_rate: int = 44100,\n",
    "            slice_size: int = 44100,\n",
    "            spectral_shape: Tuple[int, int] = (128, 128), \n",
    "            spectral_patch_shapes: Iterable[Tuple[int, int]] = ((8, 8), ),\n",
    "            encoder_ratios: Iterable[int] = (8, ),\n",
    "            num_clusters: Iterable[int] = (256, ),\n",
    "    ):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.slice_size = slice_size\n",
    "        self.spectral_shape = tuple(spectral_shape)\n",
    "        self.spectral_patch_shapes = [tuple(s) for s in spectral_patch_shapes]\n",
    "        self.encoder_ratios = list(encoder_ratios)\n",
    "        self.num_clusters = list(num_clusters)\n",
    "        self._spectrogrammer = AT.MelSpectrogram(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_fft=1024 * 2,\n",
    "            win_length=sample_rate // 30,\n",
    "            hop_length=sample_rate // spectral_shape[-1],\n",
    "            n_mels=spectral_shape[-2],\n",
    "            power=1.,\n",
    "        )\n",
    "        self.encoders = [\n",
    "            nn.Linear(math.prod(spectral_patch_shape), math.prod(spectral_patch_shape) // encoder_ratio)\n",
    "            for spectral_patch_shape, encoder_ratio in zip(self.spectral_patch_shapes, self.encoder_ratios)\n",
    "        ]\n",
    "        self.clusterers = [\n",
    "            MiniBatchKMeans(\n",
    "                n_clusters=num_clusters,\n",
    "                batch_size=1024,\n",
    "                random_state=23,\n",
    "                n_init=1,\n",
    "            )\n",
    "            for num_clusters in self.num_clusters\n",
    "        ]\n",
    "        \n",
    "        for check_attribute in (\"encoder_ratios\", \"clusterers\"):\n",
    "            if len(self.spectral_patch_shapes) != len(getattr(self, check_attribute)):\n",
    "                raise ValueError(\n",
    "                    f\"`{check_attribute}` must be same length as `spectral_patch_shapes`\"\n",
    "                    f\", expected {len(self.spectral_patch_shapes)}, got {len(getattr(self, check_attribute))}\"\n",
    "                )\n",
    "            \n",
    "    @torch.inference_mode()\n",
    "    def encode_audio(self, audio: torch.Tensor, normalize_spec: bool = True) -> torch.Tensor:\n",
    "        if audio.ndim == 1:\n",
    "            pass\n",
    "        elif audio.ndim == 2:\n",
    "            if audio.shape[0] > 1:\n",
    "                audio = audio.mean(0)\n",
    "            else:\n",
    "                audio = audio.squeeze(0)\n",
    "        else:\n",
    "            raise ValueError(f\"Need audio.ndim == 1 or 2, got {audio.ndim}\")\n",
    "        \n",
    "        if audio.shape[-1] < self.slice_size:\n",
    "            audio = torch.concat([audio, torch.zeros(self.slice_size - audio.shape[-1]).to(audio.dtype)])\n",
    "        \n",
    "        histograms = []\n",
    "        while audio.shape[-1] >= self.slice_size:            \n",
    "            spec = self._get_spec(audio[:self.slice_size], normalize=normalize_spec)\n",
    "            \n",
    "            histograms.append(self._get_histogram(spec))\n",
    "            \n",
    "            audio = audio[self.slice_size:]\n",
    "        \n",
    "        return torch.concat(histograms)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def encode_spectrum(self, spectrum: torch.Tensor, normalize: bool = True) -> torch.Tensor:\n",
    "        if spectrum.ndim == 2:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Need spectrum.ndim == 2, got {spectrum.ndim}\")\n",
    "        \n",
    "        if spectrum.shape[-2] != self.spectral_shape[0]:\n",
    "            raise ValueError(f\"spectrum.shape must fit `spectral_shape`, expected {self.spectral_shape}, got {spectrum.shape}\")\n",
    "\n",
    "        if spectrum.shape[-1] < self.slice_size:\n",
    "            audio = torch.concat([spectrum, torch.zeros(spectrum.shape[-2], self.slice_size - spectrum.shape[-1]).to(spectrum.dtype)])\n",
    "        \n",
    "        if normalize:\n",
    "            spec_max = spec.max()\n",
    "            if spec_max:\n",
    "                spectrum = spectrum / spec_max\n",
    "        \n",
    "        histograms = []\n",
    "        while spectrum.shape[-1] >= self.spectral_shape[-1]:\n",
    "            spec = spectrum[:, :self.spectral_shape[-1]]\n",
    "            histograms.append(self._get_histogram(spec))\n",
    "            \n",
    "            spectrum = spectrum[:, self.spectral_shape[-1]:]\n",
    "        \n",
    "        return torch.concat(histograms)\n",
    "    \n",
    "    def _get_histogram(self, spec: torch.Tensor) -> torch.Tensor:\n",
    "        one_full_hist = []\n",
    "        for spectral_patch_shape, encoder, clusterer in zip(self.spectral_patch_shapes, self.encoders, self.clusterers):\n",
    "            patches = torch.concat(list(iter_image_patches(spec.unsqueeze(0), spectral_patch_shape)))\n",
    "            # print(patches.max())\n",
    "            embeddings = encoder(patches.flatten(1))\n",
    "\n",
    "            labels = clusterer.predict(embeddings.flatten(1))\n",
    "\n",
    "            hist, _ = np.histogram(labels, bins=clusterer.n_clusters, range=(0, clusterer.n_clusters - 1))\n",
    "            \n",
    "            if spectral_patch_shape[-1] != spectral_patch_shape[-2]:\n",
    "                pass\n",
    "                #print(spec.max(), hist)\n",
    "                #hist = hist[1:-1]\n",
    "            \n",
    "            one_full_hist.append(torch.Tensor(hist) / patches.shape[0])\n",
    "\n",
    "        return torch.concat(one_full_hist).unsqueeze(0)\n",
    "        \n",
    "    def _get_spec(self, audio: torch.Tensor, normalize: bool):\n",
    "        spec = self._spectrogrammer(audio)[:, :self.spectral_shape[-1]]\n",
    "        if normalize:\n",
    "            spec_max = spec.max()\n",
    "            if spec_max:\n",
    "                spec = spec / spec_max\n",
    "        return spec.clamp(0, 1)\n",
    "    \n",
    "    def save(self, file) -> None:\n",
    "        data = {\n",
    "            \"sample_rate\": self.sample_rate,\n",
    "            \"slice_size\": self.slice_size,\n",
    "            \"spectral_shape\": self.spectral_shape, \n",
    "            \"spectral_patch_shapes\": self.spectral_patch_shapes,\n",
    "            \"encoder_ratios\": self.encoder_ratios,\n",
    "            \"num_clusters\": self.num_clusters,\n",
    "        }\n",
    "        for i in range(len(self.clusterers)):\n",
    "            data.update({\n",
    "                f\"encoder.{i}.weight\": self.encoders[i].weight[:],\n",
    "                f\"encoder.{i}.bias\": self.encoders[i].bias[:],\n",
    "                f\"clusterer.{i}\": self.clusterers[i]\n",
    "            })\n",
    "        torch.save(data, file)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, fp):\n",
    "        data = torch.load(fp)\n",
    "        c = cls(\n",
    "            sample_rate=data[\"sample_rate\"],\n",
    "            slice_size=data[\"slice_size\"],\n",
    "            spectral_shape=data[\"spectral_shape\"],\n",
    "            spectral_patch_shapes=data[\"spectral_patch_shapes\"],\n",
    "            encoder_ratios=data[\"encoder_ratios\"],\n",
    "            num_clusters=data[\"num_clusters\"],\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(c.clusterers)):\n",
    "                c.encoders[i].weight[:] = data[f\"encoder.{i}.weight\"]\n",
    "                c.encoders[i].bias[:] = data[f\"encoder.{i}.bias\"]\n",
    "                c.clusterers[i] = data[f\"clusterer.{i}\"]\n",
    "        return c\n",
    "    \n",
    "au = AudioUnderstander(\n",
    "    spectral_patch_shapes=[shape for vae, shape in vaes],\n",
    "    encoder_ratios=[8] * len(vaes),\n",
    "    num_clusters=[NUM_CLUSTERS] * len(vaes),    \n",
    ")\n",
    "with torch.no_grad():\n",
    "    for i, ((vae, shape), clusterer) in enumerate(zip(vaes, clusterers)):\n",
    "        au.encoders[i].weight[:] = vae.encoder.linear_mu.weight\n",
    "        au.encoders[i].bias[:] = vae.encoder.linear_mu.bias\n",
    "        au.clusterers[i] = deepcopy(clusterer)\n",
    "\n",
    "if 1:\n",
    "    import io\n",
    "    fp = io.BytesIO()\n",
    "    au.save(fp)\n",
    "    print(f\"filesize: {fp.tell():,} bytes\")\n",
    "    fp.seek(0)\n",
    "    au = AudioUnderstander.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888dad0-9e08-48cb-943e-241b78759590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SAVE\n",
    "au.save(\"../models/au/au-1sec-3x256.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268888e5-5ed5-4197-b8df-dd452eaecf62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_features(filename, histograms: bool = True):\n",
    "    audio, sr = torchaudio.load(filename)\n",
    "    print(f\"{audio.shape[-2]} x {audio.shape[-1] / sr:.2f} secs @ {sr:,}Hz\")\n",
    "    if sr != au.sample_rate:\n",
    "        audio = AF.resample(audio, sr, au.sample_rate)\n",
    "    # num_slices = audio.shape[-1] / au.slice_size \n",
    "    hists = au.encode_audio(audio)\n",
    "    if histograms:\n",
    "        img = hists.T.unsqueeze(0) / hists.max()\n",
    "        img = (img * 10).clamp(0, 1)\n",
    "        display(VF.to_pil_image(resize(img, 3)))\n",
    "    display(px.line(hists.mean(0)))\n",
    "\n",
    "plot_features(\"/home/bergi/Music/theDropper/05 CD Track 05.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928add9-dffd-45f8-a130-bb3375a4fa58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_features(\"/home/bergi/Music/Scirocco/07 Bebop.mp3\", histograms=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe4556-8b38-4d14-ad57-8e0150d169ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_features(\"/home/bergi/Music/Hitchhiker's Guide - Radio Play/Hitchhiker'sGuideEpisode-03.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7854bf-e4e8-4869-87d1-0a790e184382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_features(\"/home/bergi/Music/Ray Kurzweil The Age of Spiritual Machines/(audiobook) Ray Kurzweil - The Age of Spiritual Machines - 1 of 4.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec6711-7f70-4062-a924-e60f25a71cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
