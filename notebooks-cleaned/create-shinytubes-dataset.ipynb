{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20d7805-5073-4601-8a5c-4cc209a4f5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_notebook import *\n",
    "from src.train.experiment import load_experiment_trainer\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891c5d00-d8cc-4686-b6df-2bc2e5639457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e938945c-8b94-4b44-afaa-0510d7f7b177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(ds, count=16*16):\n",
    "    batch = next(iter(DataLoader(ds, batch_size=count)))\n",
    "    if isinstance(batch, (tuple, list)):\n",
    "        images = batch[0]\n",
    "        for b in batch[1:]:\n",
    "            if isinstance(b, torch.Tensor) and b.shape[-3:] == images.shape[-3:]:\n",
    "                images = torch.cat([images, b], dim=0)\n",
    "    else:\n",
    "        images = batch\n",
    "        \n",
    "    display(VF.to_pil_image(make_grid(images, nrow=int(math.sqrt(count)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaf5b74-8153-45ba-9b7a-4e9e7cce1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.base_dataset import BaseDataset\n",
    "from torchvision.datasets.folder import is_image_file\n",
    "\n",
    "class ImageSourceTargetDataset(BaseDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            path: Union[str, Path],\n",
    "            source_subpath: str = \"source\",\n",
    "            target_subpath: str = \"target\",\n",
    "    ):\n",
    "        path = Path(path)\n",
    "        self._source_path = path / source_subpath\n",
    "        self._target_path = path / target_subpath\n",
    "        self._source_images: Dict[str, Optional[torch.Tensor]] = {}\n",
    "        self._target_images: Dict[str, Optional[torch.Tensor]] = {}\n",
    "\n",
    "        for filename in sorted(self._source_path.glob(\"*\")):\n",
    "            if is_image_file(str(filename)):\n",
    "                self._source_images[filename.name] = VF.to_tensor(PIL.Image.open(filename))\n",
    "\n",
    "        for filename in sorted(self._target_path.glob(\"*\")):\n",
    "            if is_image_file(str(filename)):\n",
    "                self._target_images[filename.name] = VF.to_tensor(PIL.Image.open(filename))\n",
    "\n",
    "        if sorted(self._source_images) != sorted(self._target_images):\n",
    "            raise RuntimeError(f\"Source and target filenames are not identical\")\n",
    "\n",
    "        self._index = {\n",
    "            i: key\n",
    "            for i, key in enumerate(self._source_images)\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._source_images)\n",
    "        \n",
    "    def __getitem__(self, idx: int):\n",
    "        key = self._index[idx]\n",
    "        return self._source_images[key], self._target_images[key]\n",
    "\n",
    "\n",
    "class ImageSourceTargetCropDataset(BaseDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            path: Union[str, Path],\n",
    "            shape: Tuple[int, int],\n",
    "            num_crops: int,  # per image\n",
    "            source_subpath: str = \"source\",\n",
    "            target_subpath: str = \"target\",\n",
    "            random: bool = False,\n",
    "    ):\n",
    "        self._dataset = ImageSourceTargetDataset(path=path, source_subpath=source_subpath, target_subpath=target_subpath)\n",
    "        self._shape = shape\n",
    "        self._num_crops = num_crops\n",
    "        self._random = random\n",
    "        if not self._random:\n",
    "            self._crop_positions = []\n",
    "            rng = globals()[\"random\"].Random(23)\n",
    "            for idx in range(len(self._dataset) * self._num_crops):\n",
    "                image_idx = idx % len(self._dataset)\n",
    "                source_image, target_image = self._dataset[image_idx]\n",
    "                self._crop_positions.append((image_idx, *self._get_crop_pos(source_image, rng)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._dataset) * self._num_crops\n",
    "        \n",
    "    def __getitem__(self, idx: int):\n",
    "        if self._random:\n",
    "            image_idx = random.randrange(len(self._dataset))\n",
    "            \n",
    "            source_image, target_image = self._dataset[image_idx]\n",
    "            assert source_image.shape == target_image.shape\n",
    "\n",
    "            x, y = self._get_crop_pos(source_image, random)\n",
    "        else:\n",
    "            image_idx, x, y = self._crop_positions[idx]\n",
    "            source_image, target_image = self._dataset[image_idx]\n",
    "            assert source_image.shape == target_image.shape\n",
    "\n",
    "        return (\n",
    "            source_image[..., y: y + self._shape[0], x: x + self._shape[1]],\n",
    "            target_image[..., y: y + self._shape[0], x: x + self._shape[1]],\n",
    "        )\n",
    "        \n",
    "    def _get_crop_pos(self, image: torch.Tensor, rng: random.Random) -> Tuple[int, int]:\n",
    "        H, W = image.shape[-2:]\n",
    "        if self._shape[0] > H or self._shape[1] > W:\n",
    "            raise RuntimeError(f\"Crop shape {self._shape} is too large for image {image.shape}\")\n",
    "        x = rng.randrange(W - self._shape[1])\n",
    "        y = rng.randrange(H - self._shape[0])\n",
    "        return x, y\n",
    "\n",
    "ds = ImageSourceTargetCropDataset(\"../datasets/shiny-tubes/train\", (32, 32), 5, random=False)\n",
    "\n",
    "plot(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858d8b3f-e11d-458e-9396-d3623341a619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ef201-9629-4c69-8c21-421f3445a1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf9deb4-6d46-49c9-b840-dd7eca369e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0acca63f-10b1-4be5-85a2-472b57ebab55",
   "metadata": {},
   "source": [
    "## play with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4967e9-d93a-4791-a095-8854b8d3f972",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = load_experiment_trainer(\"../experiments/img2img/shinytubes-spikes-gate.yml\", device=\"cpu\")\n",
    "assert trainer.load_checkpoint(\"snapshot\")\n",
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda96505-52b6-4075-9abd-de33ae59b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035f0606-63fd-48f0-aebf-e928cb35ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_v = PIL.Image.open(\"../datasets/shiny-tubes2/validation/source/tubes-01.png\")\n",
    "image_v = VF.to_tensor(image_v)[:, :100, :100]\n",
    "VF.to_pil_image(image_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb134e-f784-4086-a60f-79c28407092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "font = ImageFont.truetype(\n",
    "    #\"/home/bergi/.local/share/fonts/LEMONMILK-LIGHTITALIC.OTF\", 20\n",
    "    \"/home/bergi/.local/share/fonts/LEMONMILK-MEDIUMITALIC.OTF\", 20\n",
    "    #\"/home/bergi/.local/share/fonts/unscii-16-full.ttf\", 25\n",
    "    #\"/usr/share/fonts/truetype/open-sans/OpenSans-ExtraBold.ttf\", 25\n",
    "    #\"/usr/share/fonts/truetype/dejavu/DejaVuSerif.ttf\", 25\n",
    ")\n",
    "image = PIL.Image.new(\"RGB\", (200, 40))\n",
    "draw = ImageDraw.ImageDraw(image)\n",
    "draw.text((30, 7), \"hello world\", font=font, fill=(255, 255, 255))\n",
    "image = VF.to_tensor(image)\n",
    "VF.to_pil_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6118810b-ee3a-4757-adc9-0a442d546e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    noisy_image = (image - image * torch.randn_like(image[:1]) * .4).clamp(0, 1)\n",
    "    model.eval()\n",
    "    output1 = model(image.unsqueeze(0)).squeeze(0).clamp(0, 1)\n",
    "    output2 = model(noisy_image.unsqueeze(0)).squeeze(0).clamp(0, 1)\n",
    "    grid = make_grid([image, noisy_image, output1, output2], nrow=2).clamp(0, 1)\n",
    "    display(VF.to_pil_image(resize(grid, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c70e2e7-586a-4b12-b38a-56c19d0d03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image2 = VF.to_tensor(PIL.Image.open(\"/home/bergi/Pictures/eisenach/wartburg.jpg\"))\n",
    "image2 = resize(image2, .25, VF.InterpolationMode.BICUBIC)\n",
    "image2 = (1. - image2).clamp(0, 1)\n",
    "VF.to_pil_image(image2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12286b4-1490-4277-bfc5-c4dbad95db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(image2.unsqueeze(0)).squeeze(0)\n",
    "    display(VF.to_pil_image(resize(make_grid([image2, output.clamp(0, 1)], nrow=1), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83874d2-73ea-4cc2-b3b6-495e3fe9b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for p in (\n",
    "    f\"../datasets/shiny-tubes3/validation/source/tubes-01.png\",\n",
    "    f\"../datasets/shiny-tubes3/validation/target/tubes-01.png\"\n",
    "):\n",
    "    i1 = PIL.Image.open(p)\n",
    "    i1 = VF.to_tensor(i1)[:, :256, :256]\n",
    "    images.append(i1)\n",
    "VF.to_pil_image(make_grid(images, padding=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdfea96-25e6-4610-a7c4-84ea76732d61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
