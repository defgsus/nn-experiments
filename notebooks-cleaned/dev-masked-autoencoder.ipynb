{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678659f-6416-452b-b4a1-11e5bbd2b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aae462f-1def-415e-8cce-0a6fdce245b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfab008-ae42-4079-ae11-cfb21437cdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            dimensions: int, \n",
    "            max_len: int = 128,\n",
    "            k: Optional[int] = None,\n",
    "            dtype: torch.dtype = torch.float32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._dimensions = dimensions\n",
    "        self._max_len = max_len\n",
    "        if k is None:\n",
    "            k = round(max_len / (20 + max_len / 20), 4)\n",
    "        self._k = k\n",
    "        \n",
    "        pe = [[0] * dimensions for _ in range(max_len)]\n",
    "        \n",
    "        for pos in range(max_len):   \n",
    "            # for each dimension of the each position\n",
    "            for i in range(0, dimensions, 2):   \n",
    "                pe[pos][i] = math.sin(pos / (k ** ((2 * i) / dimensions)))\n",
    "                pe[pos][i + 1] = math.cos(pos / (k ** ((2 * (i + 1)) / dimensions)))\n",
    "\n",
    "        self.pe = nn.Parameter(torch.tensor(pe, dtype=dtype), requires_grad=False)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"dimensions={self._dimensions}, max_len={self._max_len}, k={self._k}\"\n",
    "        \n",
    "    def forward(self, x: Union[int, Tuple[int], List[int], torch.LongTensor]):\n",
    "        if isinstance(x, int):\n",
    "            return self.pe[x]\n",
    "        elif not isinstance(x, torch.Tensor):       \n",
    "            x = torch.tensor(x, dtype=torch.int64)\n",
    "\n",
    "        return self.pe[x]\n",
    "\n",
    "pe = PositionalEmbedding(32)\n",
    "#pe([0, 1, 2, 0])\n",
    "display(VF.to_pil_image(resize(.5+.5*pe.pe.T[:, -400:].unsqueeze(0), 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d7c0fc-d97f-4ecf-a969-5d6b4f80f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalPatchEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            patch_shape: Tuple[int, int, int],\n",
    "            max_size: Tuple[int, int], \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._patch_shape = patch_shape\n",
    "        self._max_size = max_size\n",
    "        embed_dim = math.prod(patch_shape[-2:])\n",
    "        self.pos_embed = PositionalEmbedding(embed_dim, max_len=max(max_size))\n",
    "        \n",
    "    def forward(self, patches: torch.Tensor, positions: torch.LongTensor):\n",
    "        assert patches.ndim == 4, f\"Expected patches.ndim == 4, got {patches.shape}\"\n",
    "        B, C, H, W = patches.shape\n",
    "        assert positions.shape == torch.Size((B, 2)), f\"Expected positions.shape == ({B}, 2), got {positions.shape}\"\n",
    "        \n",
    "        pos_emb_x = self.pos_embed(positions[:, -1]).view(B, 1, *self._patch_shape[-2:])\n",
    "        pos_emb_y = self.pos_embed(positions[:, -2]).view(B, 1, *self._patch_shape[-2:])\n",
    "\n",
    "        patches_with_embeddings = torch.concat([patches, pos_emb_y, pos_emb_x], dim=-3) \n",
    "        \n",
    "        return patches_with_embeddings\n",
    "        \n",
    "menc = PositionalPatchEncoder((1, 8, 8), (4, 4))\n",
    "display(menc)\n",
    "display(VF.to_pil_image(resize(make_grid(menc(\n",
    "    patches=torch.randn(2, 1, 8, 8),\n",
    "    positions=torch.LongTensor([[0, 1], [2, 1]])\n",
    "    \n",
    ")), 5).clamp(0, 1)))\n",
    "#pe([0, 1, 2, 0])\n",
    "#display(VF.to_pil_image(resize(.5+.5*pe.pe.T[:, -400:].unsqueeze(0), 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab455056-ca4e-4043-ae6c-642a5985a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.cl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
