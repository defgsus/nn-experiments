{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b160454-4ea2-43d4-a3ad-cc245c9b5abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_notebook import *\n",
    "import tokenizers\n",
    "import transformers \n",
    "from src.datasets.fefe import FefePostIterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf3137-b8a3-4798-92ed-788b282163cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = FefePostIterableDataset().freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e051b1c-0765-46c7-bb82-5095e7936c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_map = {}\n",
    "for text in tqdm(ds):\n",
    "    for ch in text:\n",
    "        char_map[ch] = char_map.get(ch, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddfe9ef-1e6c-4efc-af3a-485dbf82f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(char_map))\n",
    "for key in sorted(char_map, key=lambda k: char_map[k], reverse=True):\n",
    "    print(key, char_map[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9ecd9-a35e-4372-8006-8a01a65109dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_chars = \"\".join(key for key in char_map if char_map[key] < 40)\n",
    "small_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000bdf28-731e-4fce-adbe-723a83473cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_map_3 = {}\n",
    "for text in tqdm(ds):\n",
    "    for i in range(len(text)):\n",
    "        for j in range(3, 6):\n",
    "            chars = text[i:i+j]\n",
    "            if len(chars) == j and \" \" not in chars and chars.isalpha():\n",
    "                token_map_3[chars] = token_map_3.get(chars, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e538c-d605-4b3f-90de-99b4f6d0eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(token_map_3))\n",
    "for key in sorted(token_map_3, key=lambda k: token_map_3[k], reverse=True)[:1000]:\n",
    "    print(key, token_map_3[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb72b03-e1b6-4002-922e-246d26409f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sorted(token_map_3):\n",
    "    if \"ich\" in key:\n",
    "        print(key, token_map_3[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031fe0c1-3e46-4508-acc0-f74679bcd7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "tokenizers.pre_tokenizers.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce64bc-fff2-4f65-8dfb-56c57d9c5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = tokenizers.normalizers.Replace(\" \", \"X\")\n",
    "n.normalize_str(\"Hallo Welt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c4c8fd-4d50-4d09-9c9b-6ae681a901d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ord(\"ðŸ…±\"), ord(\"ðŸ‡¸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369bbc98-6c7a-4333-bee6-340fb8d7cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=tokenizers.Tokenizer(tokenizers.models.BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.normalizer = tokenizers.normalizers.Sequence([\n",
    "    #tokenizers.normalizers.NFD(), \n",
    "    #tokenizers.normalizers.NFKD(), \n",
    "    #tokenizers.normalizers.StripAccents(),\n",
    "    tokenizers.normalizers.Replace(tokenizers.Regex(f\"[{small_chars}]\"), \"\"),\n",
    "    tokenizers.normalizers.Replace(\"\\n\", \"â¬…\"),\n",
    "    tokenizers.normalizers.Replace(tokenizers.Regex(r\"\\s+\"), \"â¬‡\"),\n",
    "])\n",
    "print(tokenizer.normalizer.normalize_str(\"Bla Blub ðŸ…± ðŸ‡¸\\nnewline\"))\n",
    "#tokenizer.add_tokens([r\"\\s\"])\n",
    "if 1:\n",
    "    tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Sequence([\n",
    "        #tokenizers.pre_tokenizers.Whitespace()\n",
    "        #tokenizers.pre_tokenizers.\n",
    "        tokenizers.pre_tokenizers.Split(tokenizers.Regex(r\"[â¬‡â¬…]\"), \"contiguous\"),\n",
    "        #tokenizers.pre_tokenizers.Split(\" \", \"contiguous\"),\n",
    "        #tokenizers.pre_tokenizers.Split(tokenizers.Regex(\"\\s+\"), \"contiguous\"),\n",
    "        tokenizers.pre_tokenizers.Punctuation(),\n",
    "    ])\n",
    "    #tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.CharDelimiterSplit(\" \")\n",
    "    import copy\n",
    "    copy.deepcopy(tokenizer)\n",
    "    print(tokenizer.pre_tokenizer.pre_tokenize_str(\"How's \\\"life\\\"?\\nNext line\"))\n",
    "    for post in ds.limit(10):\n",
    "        print([i[0] for i in tokenizer.pre_tokenizer.pre_tokenize_str(tokenizer.normalizer.normalize_str(post))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437ccad4-4be3-472c-b38e-4bed9d0256fd",
   "metadata": {},
   "source": [
    "# train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caec7000-8fb9-42dc-b8fd-9fc17491dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_alphabet = [chr(c) for c in range(33, 127)] + [\"â¬‡\", \"Ã¤\", \"Ã¶\", \"Ã¼\", \"Ã„\", \"Ã–\", \"Ãœ\", \"ÃŸ\"]\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = tokenizers.trainers.BpeTrainer(\n",
    "    show_progress=True,\n",
    "    vocab_size=4096, \n",
    "    special_tokens=special_tokens,\n",
    "    max_token_length=10,\n",
    "    min_frequency=10,\n",
    "    initial_alphabet=initial_alphabet,\n",
    "    #limit_alphabet=len(initial_alphabet) + 50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6792f48b-60b9-45d4-b63e-91c06d60b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(ds, trainer=trainer)\n",
    "\" \".join(k for k in tokenizer.get_vocab() if len(k) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9319e532-857b-4299-abc3-e54f3cbbb60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in ds.skip(0).limit(10):\n",
    "    tokens = tokenizer.encode(post)\n",
    "    print(tokens.tokens)\n",
    "    #print(tokenizer.id_to_token(tokens.ids))\n",
    "    print(\"\".join(tokenizer.id_to_token(id) for id in tokens.ids).replace(\"â¬‡\", \" \"))\n",
    "    #print(tokenizer.decode(tokens.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c38ad3-fdd4-4a8c-9fdf-5f2a7eb17362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.model.__setstate__ = None\n",
    "copy.deepcopy(tokenizer.model)\n",
    "#os.makedirs(\"/tmp/tok-model-DELME/\", exist_ok=True)\n",
    "#files = tokenizer.model.save(\"/tmp/tok-model-DELME/\")\n",
    "#new_model = tokenizer.model.__class__.from_file(*files)\n",
    "#copy.deepcopy(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b88875-0cb1-4162-90df-18dd15561a1e",
   "metadata": {},
   "source": [
    "# save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b03aa9c-2877-43c8-a21b-467fbb0e3f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    clean_up_tokenization_spaces=True,\n",
    "    bos_token=\"[BOS]\",\n",
    "    eos_token=\"[EOS]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    "    padding_side=\"left\",\n",
    ").save_pretrained(str(config.SMALL_DATASETS_PATH / \"fefe\" / \"tokenizer-bpe-4096-spaces\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa9104-7c20-4ca1-87ba-0389a0ab4400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizers.tokenizers.\n",
    "tokenizer.encode(\"Hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33254ec-b53b-433a-b38d-63f42932b263",
   "metadata": {},
   "source": [
    "# load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da5540a-00b6-4283-8fad-b2dfda1476e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_tok = transformers.AutoTokenizer.from_pretrained(str(config.SMALL_DATASETS_PATH / \"fefe\" / \"tokenizer-bpe-4096\"))\n",
    "fast_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f83d6-7011-43c3-9f85-a4c94fb8f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Let's test tokenization and un-tokenization, und mal *kucken* was \"hier\" passiert?!\"\"\"\n",
    "token_ids = fast_tok(sentence).input_ids\n",
    "print(len(token_ids), token_ids)\n",
    "print(fast_tok.decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f2ce5d-ee0e-4c7d-85ef-8c2fc5b1a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.minimind import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac8529b-10bf-44fc-ac38-edd30b1f802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniMindLM(\n",
    "    LMConfig(\n",
    "        dim=512,\n",
    "        n_layers=8,\n",
    "        n_heads=8,\n",
    "        n_kv_heads=2,\n",
    "        vocab_size=4096,\n",
    "        hidden_dim=None,\n",
    "        multiple_of=64,\n",
    "    )\n",
    ")\n",
    "print(f\"params: {num_module_parameters(model):,}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68b3f8-ae4d-421f-b589-e19e1c3835e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LMConfig?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1968fcf7-e5db-4834-a301-bad02fe1bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train.experiment import load_experiment_trainer\n",
    "trainer = load_experiment_trainer(\"../experiments/minimind/fefe.yml\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae68cd-5623-49c2-9550-9b96631f800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = fast_tok(\"Und was ich noch sagen wollte\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcdc10b-da31-446e-9df2-9ed135698972",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.cpu()\n",
    "out_tokens = trainer.model.generate(\n",
    "    tokens.input_ids, \n",
    "    eos_token_id=fast_tok.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d9434-bb7e-4281-96aa-2c03b4e7bdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fast_tok.decode?\n",
    "fast_tok.decode(out_tokens.flatten(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d28d07-8323-4247-9938-2d952aa2f1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbce510-544d-47e2-a201-abbac7bdb85e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a17b11c9-ce68-4043-9c5d-b6c930d28cc1",
   "metadata": {},
   "source": [
    "# better batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9afd6ce-e383-4b6a-a68f-4cda062522ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_snippets(batch_size: int = 16):\n",
    "    seq_length = 64\n",
    "    count = 0\n",
    "    for text in ds:\n",
    "        encoding = fast_tok(\n",
    "            text,\n",
    "            #max_length=64,\n",
    "            #padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding.input_ids.squeeze()\n",
    "        while True:\n",
    "            count += 1\n",
    "            if count % batch_size == 0:\n",
    "                seq_length = random.randint(64, 128)\n",
    "                \n",
    "            if input_ids.shape[0] == seq_length:\n",
    "                yield input_ids\n",
    "                break\n",
    "            elif input_ids.shape[0] < seq_length:\n",
    "                yield torch.cat([\n",
    "                    torch.ones((seq_length - input_ids.shape[0], ), dtype=input_ids.dtype) * fast_tok.pad_token_id,\n",
    "                    input_ids\n",
    "                ])\n",
    "                break\n",
    "            else:\n",
    "                yield input_ids[:seq_length]\n",
    "                input_ids = input_ids[seq_length // 2:]\n",
    "\n",
    "counts = {}\n",
    "for i, text in tqdm(zip(range(10000), iter_snippets())):\n",
    "    key = len(text)\n",
    "    counts[key] = counts.get(key, 0) + 1\n",
    "df = pd.DataFrame(counts.values(), index=counts.keys()).sort_index()\n",
    "px.bar(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc32dd5f-96f4-451e-bca7-f094c422afb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e8d970-6eab-49ed-8195-93db88eca831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
