{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b67930-6b3e-4ff2-9f8f-e6e4e7cfe55e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import * \n",
    "from src.util import ImageFilter\n",
    "from src.algo import Space2d, IFS\n",
    "from src.datasets.generative import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf96c7-a8d7-48eb-a601-746c5b294e06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_samples(\n",
    "        iterable, \n",
    "        total: int = 32, \n",
    "        nrow: int = 8, \n",
    "        return_image: bool = False, \n",
    "        show_compression_ratio: bool = False,\n",
    "        label: Optional[Callable] = None,\n",
    "):\n",
    "    samples = []\n",
    "    labels = []\n",
    "    f = ImageFilter()\n",
    "    try:\n",
    "        for entry in tqdm(iterable, total=total):\n",
    "            image = entry\n",
    "            if isinstance(entry, (list, tuple)):\n",
    "                image = entry[0]\n",
    "            if image.ndim == 4:\n",
    "                image = image.squeeze(0)\n",
    "            samples.append(image)\n",
    "            if show_compression_ratio:\n",
    "                labels.append(round(f.calc_compression_ratio(image), 3))\n",
    "            elif label is not None:\n",
    "                labels.append(label(entry))\n",
    "                \n",
    "            if len(samples) >= total:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    if labels:\n",
    "        image = VF.to_pil_image(make_grid_labeled(samples, nrow=nrow, labels=labels))\n",
    "    else:\n",
    "        image = VF.to_pil_image(make_grid(samples, nrow=nrow))\n",
    "    if return_image:\n",
    "        return image\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92da8d7e-30de-4e7c-bf96-1c5d138e571d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts.train_autoencoder_vae import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34d328-fa28-4ea2-ab3d-01c102cf7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAPE = (1, 64, 64)\n",
    "CODE_SIZE = 128\n",
    "model = VariationalAutoencoderConv(SHAPE, CODE_SIZE, channels=[32, 64, 128])\n",
    "model.load_state_dict(torch.load(\"../checkpoints/vae13-kali-convl3-128/best.pt\")[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a640af2-a7ea-4187-919e-34b8f85c939f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SHAPE = (3, 64, 64)\n",
    "CODE_SIZE = 128\n",
    "model = VariationalAutoencoderConv(SHAPE, CODE_SIZE, channels=[32, 64, 128])\n",
    "model.load_state_dict(torch.load(\"../checkpoints/vae13-kali-convl3-128/best.pt\")[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b265a06-aedc-461a-896c-2a5c047c24d5",
   "metadata": {},
   "source": [
    "## plot random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35089fc9-bc34-4447-9890-b7f2ee1e1259",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VF.to_pil_image(make_grid(model.decoder(torch.randn(32, CODE_SIZE)*.005).clamp(0, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d5cc9f-0c91-4c7d-a1fe-13171d02c21f",
   "metadata": {},
   "source": [
    "## transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c3e6f0-ae0e-402d-8ab5-db1b9de0431f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1, f2 = torch.randn(2, CODE_SIZE) * 0.01\n",
    "\n",
    "f = torch.zeros(8, 128)\n",
    "for i in range(f.shape[0]):\n",
    "    t = i / (f.shape[0] - 1)\n",
    "    f[i] = f1 * (t-1) + t * f2\n",
    "VF.to_pil_image(make_grid(model.decoder(f).clamp(0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bb7274-be2a-467d-a883-4ecca34836df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a77830f-d1b3-4bbd-84c7-77dda3db7376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
