{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0912fc2f-3f61-4e44-adee-5539bbd330be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset, RandomSampler\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import *\n",
    "from src.util import *\n",
    "from src.algo import *\n",
    "from src.models.cnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a568b6-94ea-4eb8-a3e8-c60c5d07dc70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SHAPE = (3, 128, 128)\n",
    "#dataset = TensorDataset(torch.load(f\"../datasets/kali-uint8-{SHAPE[-2]}x{SHAPE[-1]}.pt\"))\n",
    "\n",
    "if 0:\n",
    "    SHAPE = (3, 64, 64)\n",
    "    dataset = TensorDataset(torch.load(f\"../datasets/pattern-{1}x{SHAPE[-2]}x{SHAPE[-1]}-uint.pt\"))\n",
    "    dataset = TransformDataset(dataset, dtype=torch.float, multiply=1./255., transforms=[lambda i: i.repeat(3, 1, 1)])\n",
    "\n",
    "if 1:\n",
    "    SHAPE = (3, 64, 64)\n",
    "    dataset = TensorDataset(torch.load(f\"../datasets/kali-uint8-{SHAPE[-2]}x{SHAPE[-1]}.pt\"))\n",
    "    dataset = TransformDataset(dataset, dtype=torch.float, multiply=1./255.)#, transforms=[lambda i: i.repeat(3, 1, 1)])\n",
    "\n",
    "assert SHAPE == dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f9296-3ad9-495c-bd40-63b79a11e3dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    CODE_SIZE = 512\n",
    "    from scripts.train_from_dataset import EncoderMLP\n",
    "    model = EncoderMLP(SHAPE, channels=[CODE_SIZE])\n",
    "    model.load_state_dict(torch.load(\"../checkpoints/clip2/best.pt\")[\"state_dict\"])\n",
    "\n",
    "if 1:\n",
    "    CODE_SIZE = 512\n",
    "    from scripts.train_from_dataset import EncoderMLP\n",
    "    model = EncoderMLP(SHAPE, channels=[CODE_SIZE * 4, CODE_SIZE], hidden_act=nn.GELU())\n",
    "    model.load_state_dict(torch.load(\"../checkpoints/clip3/best.pt\")[\"state_dict\"])\n",
    "    \n",
    "if 0:\n",
    "    CODE_SIZE = 512\n",
    "    from scripts.train_from_dataset import EncoderTrans\n",
    "    model = EncoderTrans(SHAPE, code_size=CODE_SIZE)\n",
    "    model.load_state_dict(torch.load(\"../checkpoints/clip4-tr/best.pt\")[\"state_dict\"])\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a767f30-3ca9-400a-9490-c7ca15ee4df6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Pixels(nn.Module):\n",
    "    def __init__(self, shape: Tuple[int, int, int]):\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "        self.pixels = nn.Parameter(\n",
    "            torch.rand(self.shape) * .1 + .3\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f92283a-39a1-43ae-9af9-68a135be0bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_vis(\n",
    "    pixels: nn.Module,\n",
    "    model: nn.Module,\n",
    "    target_feature: torch.Tensor,\n",
    "    batch_size: int = 50,\n",
    "    num_steps: int = 4000,\n",
    "    learnrate: float = 0.03,\n",
    "    random_offset: int = 4,\n",
    "    random_rotation: float = 3,\n",
    "    show: bool = False,\n",
    "):\n",
    "    optimizer = torch.optim.AdamW(pixels.parameters(), lr=learnrate)\n",
    "    display_images = []\n",
    "    for batch_idx in range(0, num_steps, batch_size):        \n",
    "        pixel_batch = []\n",
    "        for image_idx in range(batch_size):\n",
    "            if batch_idx + image_idx >= num_steps:\n",
    "                break\n",
    "            image = pixels.pixels\n",
    "            shape = image.shape\n",
    "            \n",
    "            #image = VF.resize(image, [SHAPE[-2] + 8, SHAPE[-1] + 8], VF.InterpolationMode.BICUBIC)\n",
    "            if random_offset:\n",
    "                image = VF.pad(image, random_offset, padding_mode=\"edge\")\n",
    "                image = VT.RandomCrop(shape[-2:])(image)\n",
    "                \n",
    "            if 1:\n",
    "                center_x = int(shape[-1] * (torch.rand(1).item()))\n",
    "                center_y = int(shape[-2] * (torch.rand(1).item()))\n",
    "                image = VT.RandomRotation(\n",
    "                    random_rotation, center=[center_x, center_y],\n",
    "                    interpolation=VF.InterpolationMode.BILINEAR,\n",
    "                )(image)\n",
    "            \n",
    "            pixel_batch.append(image.unsqueeze(0))\n",
    "        if not pixel_batch:\n",
    "            break\n",
    "        pixel_batch = torch.cat(pixel_batch)\n",
    "        \n",
    "        features = model(pixel_batch)\n",
    "        target_features = target_feature.unsqueeze(0).repeat(features.shape[0], 1)\n",
    "        loss = F.l1_loss(features, target_features)\n",
    "        \n",
    "        pixels.zero_grad()\n",
    "        model.zero_grad()\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if show:\n",
    "                display_images.append(pixels.pixels.clamp(0, 1))\n",
    "                if len(display_images) >= 8:\n",
    "                    print(float(loss))\n",
    "                    display(VF.to_pil_image(make_grid(display_images)))\n",
    "                    display_images.clear()\n",
    "\n",
    "    if len(display_images):\n",
    "        display(VF.to_pil_image(make_grid(display_images)))\n",
    "\n",
    "the_image = dataset[8][0]\n",
    "display(VF.to_pil_image(the_image))\n",
    "pixels = Pixels(SHAPE)\n",
    "feature_vis(\n",
    "    pixels, model, show=True, \n",
    "    #random_offset=20,\n",
    "    target_feature=model(the_image.unsqueeze(0)).squeeze(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee13f61-f9df-4e3b-a46c-b52e5084ae69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_vis_many(\n",
    "    model: nn.Module, \n",
    "    indices: List[int], \n",
    "    num_vis: int = 1,\n",
    "    **kwargs,\n",
    "):\n",
    "    images = []\n",
    "    for index in tqdm(indices):\n",
    "        the_image = dataset[index][0]\n",
    "        images.append(the_image)\n",
    "        for i in range(num_vis):\n",
    "            pixels = Pixels(SHAPE)\n",
    "            feature_vis(\n",
    "                pixels, model,\n",
    "                target_feature=model(the_image.unsqueeze(0)).squeeze(0),\n",
    "                **kwargs,\n",
    "            )\n",
    "            images.append(pixels.pixels.clamp(0, 1))\n",
    "    \n",
    "    grid_images = []\n",
    "    for j in range(num_vis + 1):\n",
    "        for i in range(len(indices)):\n",
    "            grid_images.append(images[i * (num_vis + 1) + j])\n",
    "            \n",
    "    display(VF.to_pil_image(make_grid(grid_images, nrow=len(indices))))\n",
    "\n",
    "\n",
    "feature_vis_many(model, list(range(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f6734-8daf-48d4-8882-da89dd7028f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_vis_many(model, list(range(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d23e53-135d-49e9-86ae-2bbb7c926de7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
