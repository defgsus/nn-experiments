{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b67930-6b3e-4ff2-9f8f-e6e4e7cfe55e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util import *\n",
    "from src.util.image import * \n",
    "from src.algo import *\n",
    "from src.datasets.generative import *\n",
    "from src.models.cnn import *\n",
    "from src.util.embedding import *\n",
    "from src.models.encoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf96c7-a8d7-48eb-a601-746c5b294e06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resize(img, scale: float, mode: VF.InterpolationMode = VF.InterpolationMode.NEAREST):\n",
    "    return VF.resize(img, [max(1, int(s * scale)) for s in img.shape[-2:]], mode, antialias=False)\n",
    "\n",
    "def plot_samples(\n",
    "        iterable, \n",
    "        total: int = 32, \n",
    "        nrow: int = 8, \n",
    "        return_image: bool = False, \n",
    "        show_compression_ratio: bool = False,\n",
    "        label: Optional[Callable] = None,\n",
    "):\n",
    "    samples = []\n",
    "    labels = []\n",
    "    f = ImageFilter()\n",
    "    try:\n",
    "        for idx, entry in enumerate(tqdm(iterable, total=total)):\n",
    "            image = entry\n",
    "            if isinstance(entry, (list, tuple)):\n",
    "                image = entry[0]\n",
    "            if image.ndim == 4:\n",
    "                image = image.squeeze(0)\n",
    "            samples.append(image)\n",
    "            if show_compression_ratio:\n",
    "                labels.append(round(f.calc_compression_ratio(image), 3))\n",
    "            elif label is not None:\n",
    "                labels.append(label(entry) if callable(label) else idx)\n",
    "                \n",
    "            if len(samples) >= total:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    if labels:\n",
    "        image = VF.to_pil_image(make_grid_labeled(samples, nrow=nrow, labels=labels))\n",
    "    else:\n",
    "        image = VF.to_pil_image(make_grid(samples, nrow=nrow))\n",
    "    if return_image:\n",
    "        return image\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c9e53-bd7e-4c2b-9037-a6901db7604f",
   "metadata": {},
   "source": [
    "# load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34d328-fa28-4ea2-ab3d-01c102cf7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    SHAPE = (1, 32, 32)\n",
    "    CODE_SIZE = 512\n",
    "    from scripts.train_image_diffusion import DiffusionModel, DiffusionModel1\n",
    "    \n",
    "    model = DiffusionModel1(shape=SHAPE, code_size=CODE_SIZE)\n",
    "    model = DiffusionModel(SHAPE, CODE_SIZE, model)\n",
    "    model.load_state_dict(torch.load(\"../checkpoints/diff1/best.pt\")[\"state_dict\"])\n",
    "    \n",
    "print(f\"params: {num_module_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270cdd6-ca8d-4b28-b6f3-5c667a1f03ed",
   "metadata": {},
   "source": [
    "# load samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f873a-85cd-490d-b34e-17b2d71c3ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = TensorDataset(\n",
    "    torch.load(f\"../datasets/kali-uint8-{64}x{64}.pt\"),\n",
    "    torch.load(f\"../datasets/kali-uint8-{64}x{64}-CLIP.pt\"),\n",
    ")\n",
    "ds = TransformDataset(\n",
    "    ds,\n",
    "    dtype=torch.float, multiply=1. / 255.,\n",
    "    transforms=[\n",
    "        #VT.CenterCrop(64),\n",
    "        VT.RandomCrop(SHAPE[-2:]),\n",
    "        VT.Grayscale(),\n",
    "    ],\n",
    "    num_repeat=1,\n",
    ")\n",
    "plot_samples(ds, label=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446956e6-4dda-490f-aa6d-62a919fbd17e",
   "metadata": {},
   "source": [
    "# create images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249259ff-c206-4096-9b9e-9cf56f999552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def create_image(code: torch.Tensor, steps: int = 64):\n",
    "    code = code.unsqueeze(0)\n",
    "    image = torch.randn(1, *SHAPE)\n",
    "    \n",
    "    images = []\n",
    "    for step in range(steps - 1, -1, -1):\n",
    "        noise = model.predict_noise(image, code, min(5, step))\n",
    "        image = image - noise\n",
    "        images.append(image.squeeze(0))\n",
    "    \n",
    "    display(VF.to_pil_image(\n",
    "        resize(make_grid(images, nrow=16).clamp(0, 1), 3)\n",
    "    ))\n",
    "    \n",
    "create_image(ds[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d2c9b-9e10-4995-b332-70d094fb3e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1c8271-fe62-4c74-9768-1841c3b037be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21f220-88af-4521-a128-9c92a8dd7a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2320d0ea-64be-46ec-bb87-328e8c86cf6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "class DiffusionModel2(nn.Module):\n",
    "    def __init__(self, shape: Tuple[int, int, int], code_size: int, step_encoding_size: int = 10):\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "        self.code_size = code_size\n",
    "        self.step_encoding_size = step_encoding_size\n",
    "        self.transformer = torchvision.models.VisionTransformer(\n",
    "            image_size=SHAPE[-1],\n",
    "            patch_size=SHAPE[-1],\n",
    "            num_layers=3,\n",
    "            num_heads=4,\n",
    "            hidden_dim=128,\n",
    "            mlp_dim=128,\n",
    "            dropout=0.1,\n",
    "            num_classes=512,\n",
    "        )\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(512 + code_size + step_encoding_size, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1024, math.prod(shape)),\n",
    "        )\n",
    "\n",
    "    def forward(self, image_batch: torch.Tensor, code_batch: torch.Tensor, step: int) -> torch.Tensor:\n",
    "        encoded_batch = self.transformer(image_batch.expand(-1, 3, -1, -1))\n",
    "        print(encoded_batch.shape)\n",
    "        step_encoding = torch.Tensor([[step]]).to(code_batch.device).expand(code_batch.shape[0], self.step_encoding_size)\n",
    "        \n",
    "        x = torch.concat([\n",
    "            encoded_batch,\n",
    "            code_batch,\n",
    "            torch.sin(step_encoding * 10_000.),\n",
    "        ], dim=1)\n",
    "        print(x.shape)\n",
    "        y = self.layers(x)\n",
    "\n",
    "        return y.view(-1, *self.shape)\n",
    "\n",
    "diff2 = DiffusionModel2(SHAPE, CODE_SIZE)\n",
    "print(diff2(torch.randn(1, *SHAPE), torch.randn(1, CODE_SIZE), 1).shape)\n",
    "diff2\n",
    "#VF.to_pil_image(diff2(torch.randn(1, *SHAPE), torch.randn(1, CODE_SIZE), 1).squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25ebf4-7186-481a-bfc2-c5d0c1428976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torchvision.models.VisionTransformer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e68fbe-f26e-4d8f-9ec1-0b4f67add49e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c490b-3ec2-4c73-860e-44ea47525fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18f09a-709d-4325-9774-8d28018337d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca1c76-9bbe-4d3d-acf5-b45b0b4eca94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp =  nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu  = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # 第一次卷积\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        # 时间嵌入\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        # 扩展到最后2个维度\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        # 添加时间通道\n",
    "        h = h + time_emb\n",
    "        # 第二次卷积\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        # 上采样或者下采样\n",
    "        return self.transform(h)\n",
    "\n",
    "\n",
    "class SimpleUnet(nn.Module):\n",
    "    \"\"\"\n",
    "    Unet架构的一个简化版本\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            image_channels: int = 3,\n",
    "            down_channels: Tuple[int] = (64, 128, 256, 512, 1024),\n",
    "            up_channels: Tuple[int] = (1024, 512, 256, 128, 64),\n",
    "            time_emb_dim: int = 32,\n",
    "            code_dim: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        big_dim = time_emb_dim\n",
    "        if code_dim is not None:\n",
    "            big_dim += code_dim\n",
    "        # 时间嵌入\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalNumberEmbedding(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        if code_dim is not None:\n",
    "            self.code_mlp = nn.Sequential(\n",
    "                nn.Linear(code_dim, code_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "        # 初始预估\n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        # 下采样\n",
    "        self.downs = nn.ModuleList([\n",
    "            Block(down_channels[i], down_channels[i+1], big_dim)\n",
    "            for i in range(len(down_channels)-1)\n",
    "        ])\n",
    "        # 上采样\n",
    "        self.ups = nn.ModuleList([\n",
    "            Block(up_channels[i], up_channels[i+1], big_dim, up=True)\n",
    "            for i in range(len(up_channels)-1)\n",
    "        ])\n",
    "\n",
    "        self.output = nn.Conv2d(up_channels[-1], image_channels, 1)\n",
    "\n",
    "    def forward(self, x, timestep, code: Optional[torch.Tensor] = None):\n",
    "        # 时间嵌入\n",
    "        t = self.time_mlp(timestep)\n",
    "\n",
    "        if code is not None:\n",
    "            if not hasattr(self, \"code_mlp\"):\n",
    "                raise ValueError(f\"code specified in forward but no code_dim in constructor\")\n",
    "            t = torch.concat([t, self.code_mlp(code)], dim=-1)\n",
    "\n",
    "        # 初始卷积\n",
    "        x = self.conv0(x)\n",
    "        # Unet\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            # 添加残差结构作为额外的通道\n",
    "            x = torch.cat((x, residual_x), dim=1)\n",
    "            x = up(x, t)\n",
    "        return self.output(x)\n",
    "\n",
    "    \n",
    "    \n",
    "unet = SimpleUnet(image_channels=1, code_dim=333)\n",
    "print(f\"params: {num_module_parameters(unet):,}\")\n",
    "o = unet(torch.ones(1, 1, 32, 32), torch.Tensor([0]), torch.ones(1, 333))\n",
    "print(o.shape)\n",
    "unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588bcc6e-56ef-4a78-9860-8908c15f5dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2dc43e-4255-4724-889d-d39fbea4739a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "t = torch.linspace(0, 20, 21)#.unsqueeze(1)\n",
    "SinusoidalNumberEmbedding(8)(t).round(decimals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f775563-66c5-4129-8213-538ecad61284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t[:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
