{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ed52b-36ad-43d0-99d3-236d6bb06640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import *\n",
    "from src.util import ImageFilter\n",
    "from src.algo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552290e-59ed-46b3-a14c-bb0040777c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def make_grid_labeled(\n",
    "    tensor: Union[torch.Tensor, List[torch.Tensor]],\n",
    "    labels: Union[bool, Iterable[str]] = True,\n",
    "    nrow: int = 8,\n",
    "    padding: int = 2,\n",
    "    normalize: bool = False,\n",
    "    value_range: Optional[Tuple[int, int]] = None,\n",
    "    scale_each: bool = False,\n",
    "    pad_value: float = 0.0,\n",
    "    return_pil: bool = False,\n",
    "    **kwargs,\n",
    ") -> torch.Tensor:\n",
    "    grid = make_grid(\n",
    "        tensor=tensor, nrow=nrow, padding=padding, value_range=value_range, \n",
    "        scale_each=scale_each, pad_value=pad_value, \n",
    "        **kwargs,\n",
    "    )\n",
    "    \n",
    "    if labels:\n",
    "        if isinstance(tensor, (list, tuple)):\n",
    "            num_images = len(tensor)\n",
    "            shape = tensor[0].shape\n",
    "        else:\n",
    "            assert tensor.ndim == 4, f\"make_grid_labeled() only supports [N, C, H, W] shape, got '{tensor.shape}'\"\n",
    "            num_images = tensor.shape[0]\n",
    "            shape = tensor.shape[1:]\n",
    "\n",
    "        if labels is True:\n",
    "            labels = [str(i) for i in range(num_images)]\n",
    "        else:\n",
    "            labels = [str(i) for i in labels]\n",
    "        \n",
    "        grid_pil = VF.to_pil_image(grid)\n",
    "        draw = PIL.ImageDraw.ImageDraw(grid_pil)\n",
    "        \n",
    "        for idx, label in enumerate(labels):\n",
    "            x = padding + ((idx % nrow) * (shape[-1] + padding))\n",
    "            y = padding + ((idx // nrow) * (shape[-2] + padding))\n",
    "            draw.text((x-1, y), label, fill=(0, 0, 0))\n",
    "            draw.text((x+1, y), label, fill=(0, 0, 0))\n",
    "            draw.text((x, y-1), label, fill=(0, 0, 0))\n",
    "            draw.text((x, y+1), label, fill=(0, 0, 0))\n",
    "            draw.text((x, y), label, fill=(256, 256, 256))\n",
    "        \n",
    "        if return_pil:\n",
    "            return grid_pil\n",
    "        \n",
    "        grid = VF.to_tensor(grid_pil)\n",
    "        \n",
    "    return grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b292f118-e065-4c79-afa7-bfc3b8df4739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_samples(\n",
    "        iterable, \n",
    "        total: int = 32, \n",
    "        nrow: int = 8, \n",
    "        return_image: bool = False, \n",
    "        show_compression_ratio: bool = False,\n",
    "        label: Optional[Callable] = None,\n",
    "):\n",
    "    samples = []\n",
    "    labels = []\n",
    "    f = ImageFilter()\n",
    "    try:\n",
    "        for image in tqdm(iterable, total=total):\n",
    "            samples.append(image)\n",
    "            if show_compression_ratio:\n",
    "                labels.append(round(f.calc_compression_ratio(image), 3))\n",
    "            elif label is not None:\n",
    "                labels.append(label(image))\n",
    "                \n",
    "            if len(samples) >= total:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    if labels:\n",
    "        image = VF.to_pil_image(make_grid_labeled(samples, nrow=nrow, labels=labels))\n",
    "    else:\n",
    "        image = VF.to_pil_image(make_grid(samples, nrow=nrow))\n",
    "    if return_image:\n",
    "        return image\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae915d9-f6cd-4cff-b64a-4f22f148fed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatternDataset(Dataset):\n",
    "    \n",
    "    shape_types = (\"square\", \"circle\")\n",
    "    fill_types = (\"border\", \"inside\")\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            shape: Tuple[int, int, int],\n",
    "            size: int = 1_000_000,\n",
    "            seed: int = 23,\n",
    "            min_scale: float = 0.5,\n",
    "            max_scale: float = 2.,\n",
    "            min_offset: float = -2.,\n",
    "            max_offset: float = 2.,\n",
    "            dtype: torch.dtype = torch.float,\n",
    "            shape_types: Optional[Iterable[str]] = shape_types,\n",
    "            fill_types: Optional[Iterable[str]] = fill_types,\n",
    "            aa: int = 0,\n",
    "    ):\n",
    "        assert shape[0] in (1, 3), f\"Expecting 1 or 3 color channels, got {shape[0]}\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "        self._size = size\n",
    "        self.seed = seed\n",
    "        self.min_scale = min_scale\n",
    "        self.max_scale = max_scale\n",
    "        self.min_offset = min_offset\n",
    "        self.max_offset = max_offset\n",
    "        self.dtype = dtype\n",
    "        self.aa = aa\n",
    "        self.shape_types = self.__class__.shape_types if shape_types is None else list(shape_types)\n",
    "        self.fill_types = self.__class__.fill_types if fill_types is None else list(fill_types)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self._size\n",
    "        \n",
    "    def __getitem__(self, idx) -> torch.Tensor:\n",
    "        rng = torch.Generator().manual_seed(idx ^ self.seed)\n",
    "\n",
    "        amplitude = torch.sqrt(torch.rand(1, dtype=self.dtype, generator=rng)).item() * 50. \n",
    "        radius = torch.rand(1, dtype=self.dtype, generator=rng).item() * .5 \n",
    "        shape_type = self.shape_types[torch.randint(0, len(self.shape_types), (1,), generator=rng).item()]\n",
    "        fill_type = self.fill_types[torch.randint(0, len(self.fill_types), (1,), generator=rng).item()]\n",
    "        offset = torch.rand(2, dtype=self.dtype, generator=rng) * (self.min_offset - self.max_offset) + self.min_offset\n",
    "        scale = torch.pow(torch.rand(1, dtype=self.dtype, generator=rng)[0], 3.) * (self.max_scale - self.min_scale) + self.min_scale\n",
    "        rotate_2d = torch.rand(1, dtype=self.dtype, generator=rng) * 6.28\n",
    "        \n",
    "        aa = self.aa\n",
    "        if fill_type != \"inside\":\n",
    "            aa = 0\n",
    "\n",
    "        shape = [2, *self.shape[1:]]\n",
    "        if aa > 1:\n",
    "            shape[1] *= aa\n",
    "            shape[2] *= aa\n",
    "            \n",
    "        space = Space2d(\n",
    "            shape=shape,\n",
    "            offset=offset,\n",
    "            scale=scale,\n",
    "            rotate_2d=rotate_2d,\n",
    "            dtype=self.dtype,\n",
    "        ).space()\n",
    "                            \n",
    "        # repeat\n",
    "        image = (space + .5) % 1. - .5\n",
    "        \n",
    "        if shape_type == \"square\":\n",
    "            image = torch.abs(image) - radius\n",
    "            if fill_type == \"border\":\n",
    "                image, _ = torch.min(image, dim=0, keepdim=True)\n",
    "            elif fill_type == \"inside\":\n",
    "                image, _ = torch.min((image < 0.).to(image.dtype), dim=0, keepdim=True)\n",
    "            \n",
    "        elif shape_type == \"circle\":\n",
    "            image = torch.sqrt(torch.sum(torch.square(image), dim=0, keepdim=True))\n",
    "            if fill_type == \"border\":\n",
    "                image, _ = torch.min(torch.abs(image - radius), dim=0, keepdim=True)\n",
    "            elif fill_type == \"inside\":\n",
    "                image, _ = torch.min(((image - radius) <= 0.).to(image.dtype), dim=0, keepdim=True)\n",
    "        \n",
    "        image = (image * amplitude).clamp(0, 1)\n",
    "        \n",
    "        if aa > 1:\n",
    "            image = VF.resize(image, (shape[1] // aa, shape[2] // aa), VF.InterpolationMode.BICUBIC)\n",
    "        \n",
    "        # colors\n",
    "        rgb = torch.rand(3, 1, 1, dtype=self.dtype, generator=rng) * .7 + .3\n",
    "        offset = .5#torch.rand(1, dtype=self.dtype, generator=rng).item() * .7\n",
    "        if self.shape[0] == 3:\n",
    "            image = (image.repeat(3, 1, 1) + offset) * rgb #- offset\n",
    "            \n",
    "        #if torch.rand(1, generator=rng).item() < .5:\n",
    "        #    image = 1. - image\n",
    "        return image.clamp(0, 1)\n",
    "        \n",
    "        \n",
    "dataset = PatternDataset(\n",
    "    (1, 64, 64), aa=2, size=1_000_000, \n",
    "    #shape_types=[\"circle\"],\n",
    "    #min_scale=0.01, max_scale=2.,\n",
    "    #min_offset=-2., max_offset=2.\n",
    ")\n",
    "\n",
    "plot_samples(dataset, show_compression_ratio=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99223dac-872e-4cbb-b88a-82799a64aa74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba5d742-6bd3-4380-b394-8dfebafe509b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e019a-e986-4b27-9f3c-e8cf9ecf8821",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SHAPE = (1, 64, 64)\n",
    "SEED = 5432"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a3aed-923f-43b1-abed-7b302ac3058f",
   "metadata": {},
   "source": [
    "# dataset #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51d1759-24a0-4a35-acd4-2a8818d93d3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_iter_1 = IterableImageFilterDataset(\n",
    "    PatternDataset(\n",
    "        SHAPE, aa=2, size=1_000_000_000, \n",
    "        seed=SEED,\n",
    "        min_scale=1., max_scale=2.,\n",
    "        #min_offset=-2., max_offset=2.,\n",
    "        shape_types=[\"square\"],\n",
    "        fill_types=[\"border\"],\n",
    "    ),\n",
    "    filter=ImageFilter(\n",
    "        min_compression_ratio=.08,\n",
    "        #max_compression_ratio=.9,\n",
    "        #min_scaled_compression_ratio=.7,\n",
    "        #scaled_compression_shape=(64, 64),\n",
    "        #min_blurred_compression_ratio=0.32,\n",
    "        #blurred_compression_sigma=10.,\n",
    "        #blurred_compression_kernel_size=[21, 21],\n",
    "        #compression_format=\"png\",\n",
    "    )\n",
    ")\n",
    "plot_samples(ds_iter_1, show_compression_ratio=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1957c-3682-41c9-a1c6-d03c942607a0",
   "metadata": {},
   "source": [
    "# dataset #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d225cf1-7d2a-4428-abe4-f274920989b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_iter_2 = IterableImageFilterDataset(\n",
    "    PatternDataset(\n",
    "        SHAPE, aa=2, size=1_000_000_000, \n",
    "        seed=SEED + 121,\n",
    "        min_scale=1., max_scale=2.,\n",
    "        #min_offset=-2., max_offset=2.,\n",
    "        shape_types=[\"square\"],\n",
    "        fill_types=[\"inside\"],\n",
    "    ),\n",
    "    filter=ImageFilter(\n",
    "        min_compression_ratio=.08,\n",
    "        #max_compression_ratio=.9,\n",
    "        #min_scaled_compression_ratio=.7,\n",
    "        #scaled_compression_shape=(16, 16),\n",
    "        #min_blurred_compression_ratio=0.32,\n",
    "        #blurred_compression_sigma=10.,\n",
    "        #blurred_compression_kernel_size=[21, 21],\n",
    "        #compression_format=\"png\",\n",
    "    )\n",
    ")\n",
    "plot_samples(ds_iter_2, show_compression_ratio=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f18e1f-f116-454b-b667-a09ecd4ebba9",
   "metadata": {},
   "source": [
    "# dataset #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4d7c9-cb81-4d87-b075-e5e66b0e5932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_iter_3 = IterableImageFilterDataset(\n",
    "    PatternDataset(\n",
    "        SHAPE, aa=2, size=1_000_000_000, \n",
    "        seed=SEED,\n",
    "        min_scale=1., max_scale=2.,\n",
    "        #min_offset=-2., max_offset=2.,\n",
    "        shape_types=[\"circle\"],\n",
    "        fill_types=[\"border\"],\n",
    "    ),\n",
    "    filter=ImageFilter(\n",
    "        min_compression_ratio=.1,\n",
    "        #max_compression_ratio=.9,\n",
    "        #min_scaled_compression_ratio=.7,\n",
    "        #scaled_compression_shape=(16, 16),\n",
    "        #min_blurred_compression_ratio=0.32,\n",
    "        #blurred_compression_sigma=10.,\n",
    "        #blurred_compression_kernel_size=[21, 21],\n",
    "        #compression_format=\"png\",\n",
    "    )\n",
    ")\n",
    "plot_samples(ds_iter_3, show_compression_ratio=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff114a6-1f3b-48c0-be19-8fe7fbc55068",
   "metadata": {},
   "source": [
    "# dataset #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e560282-6846-4785-82ed-89542f2a944a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_iter_4 = IterableImageFilterDataset(\n",
    "    PatternDataset(\n",
    "        SHAPE, aa=2, size=1_000_000_000, \n",
    "        seed=SEED,\n",
    "        min_scale=.5, max_scale=2.,\n",
    "        #min_offset=-2., max_offset=2.,\n",
    "        shape_types=[\"circle\"],\n",
    "        fill_types=[\"inside\"],\n",
    "    ),\n",
    "    filter=ImageFilter(\n",
    "        min_compression_ratio=.07,\n",
    "        #max_compression_ratio=.9,\n",
    "        #min_scaled_compression_ratio=.7,\n",
    "        #scaled_compression_shape=(16, 16),\n",
    "        #min_blurred_compression_ratio=0.32,\n",
    "        #blurred_compression_sigma=10.,\n",
    "        #blurred_compression_kernel_size=[21, 21],\n",
    "        #compression_format=\"png\",\n",
    "    )\n",
    ")\n",
    "plot_samples(ds_iter_4, show_compression_ratio=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa143658-a73a-4495-b175-2d28a2337a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b8cc1-8c16-404b-9b28-0620337ae6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.interleave import InterleaveIterableDataset\n",
    "interleaved_dataset = InterleaveIterableDataset(\n",
    "    datasets=[\n",
    "        ds_iter_1, ds_iter_2, ds_iter_3, ds_iter_4, \n",
    "    ],\n",
    "    counts=[1, 1, 1, 1],\n",
    "    shuffle_datasets=True,\n",
    ")\n",
    "plot_samples(interleaved_dataset, total=16*16, nrow=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8cadb-fab9-48c0-b906-11ad58bff99f",
   "metadata": {},
   "source": [
    "## store 64x64 samples image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0518f15a-f090-41cc-a46d-eebaef495f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = plot_samples(interleaved_dataset, total=64*64, nrow=64, return_image=True)\n",
    "img.save(\"/home/bergi/Pictures/pattern-dataset.png\")\n",
    "img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5dbe2-3c7b-46f9-83d3-99827c59dd22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc7f9f2-ce3f-4045-a782-a96fa36f7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = f\"../datasets/pattern-{SHAPE[-3]}x{SHAPE[-2]}x{SHAPE[-1]}-uint.pt\"\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c76813-e131-4e04-bd72-96e55df6be0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def store_dataset(\n",
    "        images: Iterable,\n",
    "        output_filename,\n",
    "        max_megabyte=512,\n",
    "):\n",
    "    tensor_batch = []\n",
    "    tensor_size = 0\n",
    "    last_print_size = 0\n",
    "    try:\n",
    "        for image in tqdm(images):\n",
    "\n",
    "            image = (image.clamp(0, 1) * 255).to(torch.uint8)\n",
    "\n",
    "            if len(image.shape) < 4:\n",
    "                image = image.unsqueeze(0)\n",
    "            tensor_batch.append(image)\n",
    "            tensor_size += math.prod(image.shape)\n",
    "\n",
    "            if tensor_size - last_print_size > 1024 * 1024 * 100:\n",
    "                last_print_size = tensor_size\n",
    "\n",
    "                print(f\"size: {tensor_size:,}\")\n",
    "\n",
    "            if tensor_size >= max_megabyte * 1024 * 1024:\n",
    "                break\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    tensor_batch = torch.cat(tensor_batch)\n",
    "    torch.save(tensor_batch, output_filename)\n",
    "\n",
    "store_dataset(interleaved_dataset, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ac0e3-ac61-44e8-9214-b8da311cdbc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c39360-643f-409f-9707-8b142300f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TensorDataset(torch.load(dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75290a4b-136d-4ad4-9827-cc860d21bbb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, shuffle=True, batch_size=8*8)\n",
    "for batch in dl:\n",
    "    batch = batch[0]\n",
    "    img = VF.to_pil_image(make_grid(batch, nrow=8))\n",
    "    break\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c1f44-9eb9-4911-bf43-682fe4db7a01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d19e4e1-db5a-44f5-941a-f51b9b91f0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be144306-7aa5-4cd3-b7d1-5b726b80c2eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(VF.to_pil_image(Kali2dDataset((3, 16, 16))[111]))\n",
    "display(VF.to_pil_image(Kali2dDataset((3, 32, 32))[111]))\n",
    "display(VF.to_pil_image(Kali2dDataset((3, 64, 64))[111]))\n",
    "display(VF.to_pil_image(Kali2dDataset((3, 256, 256), aa=10)[111]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fbe2cd-667f-4a89-9f4c-152a3db0e95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e807d15e-2ea8-476c-9225-2c1fff56013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Kali2dDataset((3, 128, 128))[221]\n",
    "VF.to_pil_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111ccbc5-b0b2-484d-ad22-0813ebe4e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def pca_error(img: torch.Tensor, n_components: int = 1) -> float:\n",
    "    h = img.shape[-2]\n",
    "    pca = PCA(n_components=n_components)\n",
    "    data = img.permute(1, 2, 0).reshape(h, -1)\n",
    "    pca.fit(data)\n",
    "    f = pca.transform(data)\n",
    "    r_data = torch.Tensor(pca.inverse_transform(f))\n",
    "    r_img = r_data.reshape(h, img.shape[-1], 3).permute(2, 0, 1)\n",
    "    #return ((img - r_img).abs().sum() / math.prod(img.shape))\n",
    "    #d = (img-r_img).abs()\n",
    "    #return VF.to_pil_image(d / d.max())\n",
    "    return VF.to_pil_image(r_img)\n",
    "    \n",
    "pca_error(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
