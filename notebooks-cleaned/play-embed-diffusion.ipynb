{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06fdab-0195-4968-b0e3-f1951721516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_notebook import *\n",
    "\n",
    "import diffusers\n",
    "\n",
    "from experiments.datasets import *\n",
    "\n",
    "clip_device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ccfb4b-eaf0-4da1-9270-fb49d01890b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMPipelineWithEmbedding(diffusers.DDPMPipeline):\n",
    "\n",
    "    @dataclass\n",
    "    class CallbackArg:\n",
    "        pipeline: \"DDPMPipelineWithEmbedding\"\n",
    "        image: torch.Tensor\n",
    "        iteration: int\n",
    "        timestep: int\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "            self,\n",
    "            embedding: Optional[torch.Tensor] = None,\n",
    "            batch_size: int = 1,\n",
    "            generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "            num_inference_steps: int = 1000,\n",
    "            size: Optional[int] = None,\n",
    "            output_type: Optional[str] = \"pt\",\n",
    "            return_dict: bool = True,\n",
    "            callback: Optional[Callable[[CallbackArg], torch.Tensor]] = None,\n",
    "    ) -> Union[diffusers.ImagePipelineOutput, Tuple]:\n",
    "        from diffusers.utils.torch_utils import randn_tensor\n",
    "\n",
    "        # Sample gaussian noise to begin loop\n",
    "        if isinstance(self.unet.config.sample_size, int):\n",
    "            image_shape = (\n",
    "                batch_size,\n",
    "                self.unet.config.in_channels,\n",
    "                size or self.unet.config.sample_size,\n",
    "                size or self.unet.config.sample_size,\n",
    "            )\n",
    "        else:\n",
    "            image_shape = (batch_size, self.unet.config.in_channels, size or self.unet.config.sample_size[0], self.unet.config.sample_size[1])\n",
    "\n",
    "        if self.device.type == \"mps\":\n",
    "            # randn does not work reproducibly on mps\n",
    "            image = randn_tensor(image_shape, generator=generator)\n",
    "            image = image.to(self.device)\n",
    "        else:\n",
    "            image = randn_tensor(image_shape, generator=generator, device=self.device)\n",
    "\n",
    "        if embedding is None:\n",
    "            embedding = randn_tensor((batch_size, 512), generator=generator, device=self.device)\n",
    "        else:\n",
    "            embedding = embedding.to(self.device)\n",
    "            if embedding.ndim == 1:\n",
    "                embedding = embedding.unsqueeze(0)\n",
    "            if embedding.ndim != 2:\n",
    "                raise ValueError(f\"`embedding` must have 2 dimensions, got {embedding.shape}\")\n",
    "            if embedding.shape[0] < batch_size:\n",
    "                embedding = embedding.repeat(batch_size // embedding.shape[0], 1)\n",
    "            if embedding.shape[0] > batch_size:\n",
    "                embedding = embedding[:batch_size]\n",
    "\n",
    "        return self.run_inference_on(\n",
    "            image=image / 2. + .5,\n",
    "            embedding=embedding,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            output_type=output_type,\n",
    "            return_dict=return_dict,\n",
    "            generator=generator,\n",
    "            callback=callback,\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def run_inference_on(\n",
    "            self,\n",
    "            image: torch.Tensor,\n",
    "            embedding: torch.Tensor,\n",
    "            num_inference_steps: int = 1000,\n",
    "            timestep_offset: int = 0,\n",
    "            timestep_count: Optional[int] = None,\n",
    "            generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "            output_type: Optional[str] = \"pt\",\n",
    "            return_dict: bool = True,\n",
    "            callback: Optional[Callable[[CallbackArg], torch.Tensor]] = None,\n",
    "    ):\n",
    "        image = image.to(self.device)\n",
    "\n",
    "        if image.ndim == 4:\n",
    "            pass\n",
    "        elif image.ndim == 3:\n",
    "            image = image.unsqueeze(0)\n",
    "        else:\n",
    "            raise ValueError(f\"`image` must have 4 (or 3) dimensions, got {image.shape}\")\n",
    "\n",
    "        image = image * 2. - 1.\n",
    "\n",
    "        if embedding.shape[0] != image.shape[0]:\n",
    "            raise ValueError(f\"batch-size of `embedding` must match `image`, expected {image.shape[0]}, got {embedding.shape}\")\n",
    "        embedding = embedding.to(self.device)\n",
    "        \n",
    "        # set step values\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        timesteps = self.scheduler.timesteps[timestep_offset:]\n",
    "        if timestep_count is not None:\n",
    "            timesteps = timesteps[:timestep_count]\n",
    "\n",
    "        for idx, t in enumerate(self.progress_bar(timesteps)):\n",
    "            # 1. predict noise model_output\n",
    "            model_output = self.unet(image, t, embedding).sample\n",
    "\n",
    "            # 2. compute previous image: x_t -> x_t-1\n",
    "            image = self.scheduler.step(model_output, t, image, generator=generator).prev_sample\n",
    "\n",
    "            if callback is not None:\n",
    "                image = callback(self.CallbackArg(\n",
    "                    pipeline=self, image=image, iteration=idx, timestep=t\n",
    "                ))\n",
    "\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        if output_type == \"pt\":\n",
    "            pass\n",
    "        else:\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "            if output_type == \"pil\":\n",
    "                image = self.numpy_to_pil(image)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image,)\n",
    "\n",
    "        return diffusers.ImagePipelineOutput(images=image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105aa717-0a08-4350-8ddf-f2c82286ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe = DDPMPipelineWithEmbedding.from_pretrained(\"../checkpoints/hug-diff/ddpm-07-clip-norm-mult5-clamp0.3/\").to(\"cuda\")  # the good one\n",
    "pipe = DDPMPipelineWithEmbedding.from_pretrained(\"../checkpoints/hug-diff/ddpm-09-clip-norm-mult5-clamp0.3/\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44806a10-2adb-4ab2-99bd-da2d488b55da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ed6c1-cb2b-4ef7-a988-dd201f2da293",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.scheduler = diffusers.schedulers.DDPMScheduler(\n",
    "   # timestep_spacing=\"trailing\", \n",
    "    #thresholding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f5bc0-158f-4071-95f6-38485d3cd16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = PixelartDataset((4, 32, 32))\n",
    "some_images = [\n",
    "    \"/home/bergi/Pictures/csv-turing.png\",\n",
    "    \"/home/bergi/Pictures/Abstract-Radiation-Hazard-Sign.jpg\",\n",
    "    \"/home/bergi/Pictures/diffusion/cthulhu-11.jpeg\",\n",
    "]\n",
    "some_images = [\n",
    "    VF.resize(VF.to_tensor(PIL.Image.open(name).convert(\"RGBA\")), (32, 32), VF.InterpolationMode.BILINEAR, antialias=True)\n",
    "    for name in some_images\n",
    "]\n",
    "\n",
    "VF.to_pil_image(make_grid(some_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5983999-7f49-4d6f-b007-b215fc086e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "VF.to_pil_image(resize(ds[9000][0], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96952a7-e4ea-4f87-a9e6-bf55bd309bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_embedding(embedding: torch.Tensor):\n",
    "    return (embedding / torch.norm(embedding, dim=1, keepdim=True)).clamp(-3., 3.) * 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af9668-52a3-4028-8a53-d51077c02926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt_embeds = ClipSingleton.encode_text([\n",
    "#    \"fire\", \"cobblestone\", \"brick wall\", \"wizard\", \"face\",\n",
    "#], device=\"cpu\") * 2.\n",
    "prompt_embeds = scale_embedding(ClipSingleton.encode_image([\n",
    "    *some_images,\n",
    "    ds[723][0], # green sword\n",
    "    ds[823][0], # green dragon\n",
    "    ds[3023][0], # yellowish rocks\n",
    "    ds[3323][0], # dog?\n",
    "    ds[8229][0], # marble block\n",
    "    ds[8413][0], # cables before white\n",
    "    ds[8523][0], # brick wall, grass\n",
    "    ds[8840][0], # rocks, grass\n",
    "    ds[9000][0], # cobblestone, grass\n",
    "], device=clip_device))\n",
    "#print(prompt_embeds.flatten(1).max(1))\n",
    "#prompt_embeds /= prompt_embeds.abs().max()\n",
    "\n",
    "grid = []\n",
    "try:\n",
    "    for idx in range(prompt_embeds.shape[0]):\n",
    "        images = pipe(embedding=prompt_embeds[None, idx].repeat(6*6, 1), batch_size=6*6, num_inference_steps=10).images\n",
    "        grid.append(make_grid(images, nrow=6))\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "if grid:\n",
    "    display(VF.to_pil_image(make_grid(grid, nrow=3, padding=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc5b77e-f89f-4a3c-8531-a8bd984f4e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embeds = scale_embedding(ClipSingleton.encode_text([\n",
    "    \"cobblestone\", \"brick wall\", \"steel\", \n",
    "    \"sand\", \"grass\", \"water\",\n",
    "    \"fire\", \"ice\", \"air\", \n",
    "    \"sky\", \"earth\", \"mountains\",\n",
    "\n",
    "    \"creature\", \"dragon\", \"dog\",\n",
    "    \"wizard\", \"man\", \"woman\",\n",
    "    \n",
    "    \"face\", \"hand\", \"feet\",\n",
    "    \"sword\", \"axe\", \"armour\",\n",
    "], device=clip_device))\n",
    "\n",
    "grid = []\n",
    "try:\n",
    "    for idx in range(prompt_embeds.shape[0]):\n",
    "        images = pipe(embedding=prompt_embeds[None, idx].repeat(6*6, 1), batch_size=6*6, num_inference_steps=10).images\n",
    "        grid.append(make_grid(images, nrow=6))\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "if grid:\n",
    "    display(VF.to_pil_image(make_grid(grid, nrow=6, padding=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d640772-0178-4326-a2b4-f4568b8608c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embeds = scale_embedding(ClipSingleton.encode_text([\n",
    "    \"woman\",\n",
    "], device=clip_device))\n",
    "\n",
    "grid = []\n",
    "try:\n",
    "    for _ in range(32):\n",
    "        images = pipe(embedding=prompt_embeds.repeat(32, 1), batch_size=32, num_inference_steps=100).images\n",
    "        grid.extend(images)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "if grid:\n",
    "    display(VF.to_pil_image(make_grid(grid, nrow=32, padding=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ff873-359e-4a69-ac49-f4a195243bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f7b3f-edc6-4f27-9fb2-0f7dc819d0da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc6da0b-9a1c-41bc-8726-979dc0483c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wang_patches(corner: bool = False):\n",
    "    \n",
    "    tiles = wangtiles.WangTiles2C() if corner else wangtiles.WangTiles2E()\n",
    "    template = tiles.create_template((32, 32), fade=5., padding=0)\n",
    "    \n",
    "    template.image = torch.concat([template.image, template.image[:1]], dim=0)\n",
    "    template.image = torch.sigmoid((template.image - .985) * 400.)\n",
    "    template.image[:, 32:64, 64:96] = 1\n",
    "    # VF.to_pil_image(wang_template.image)\n",
    "    patches = torch.concat([\n",
    "        i.unsqueeze(0) for i in iter_image_patches(template.image, (32, 32))\n",
    "    ])\n",
    "    return tiles, template, patches\n",
    "\n",
    "wang_tiles, wang_template, wang_patches = get_wang_patches(corner=True)\n",
    "VF.to_pil_image(make_grid(wang_patches, nrow=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f7ff8c-c9c7-4be8-a2ff-55d9ea65f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "VF.to_pil_image(ds[602][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e625a112-02a2-454d-9062-9891f8ecfd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wang_images_from_prompt(\n",
    "    prompt: str, batch_size: int = 1,#wang_patches.shape[0], \n",
    "    size: int = wang_template.shape[-1], steps: int = 100, offset: int = 0,\n",
    "    random_shift: int = 1,\n",
    "    brightness: float = .0,\n",
    "    contrast: float = 1.,\n",
    "    wrap_pad: int = 0,\n",
    "    fac: float = 0.,\n",
    "    blur: float = 0.,\n",
    "):\n",
    "    demo_patch = make_grid([ds[602][0]] * 16, nrow=4, padding=0).to(pipe.device) * 2. - 1.\n",
    "    #demo_patch = .5 * torch.ones(4, 32*4, 32*4).to(pipe.device)\n",
    "    #wang_patches_ = wang_patches.to(pipe.device)\n",
    "    wang_image = wang_template.image.to(pipe.device)\n",
    "    \n",
    "    last_shift = None\n",
    "    def _callback(arg: DDPMPipelineWithEmbedding.CallbackArg):\n",
    "        if arg.iteration < steps - 2:\n",
    "            if blur:\n",
    "                arg.image = arg.image + blur * (VF.gaussian_blur(arg.image, 3, 3) - arg.image)\n",
    "            arg.image = arg.image.clone()\n",
    "            arg.image[0] += fac * (.01 + .99 * wang_image) * (demo_patch - arg.image[0])\n",
    "        \n",
    "        nonlocal last_shift\n",
    "        if random_shift > 0:\n",
    "            if last_shift is None:\n",
    "                last_shift = (\n",
    "                    #(random.randrange(2) - 1) * random_shift, (random.randrange(2) - 1) * random_shift,\n",
    "                    random.randrange(-random_shift, random_shift + 1),\n",
    "                    random.randrange(-random_shift, random_shift + 1),\n",
    "                )\n",
    "                return image_shift(arg.image, *last_shift) \n",
    "            else:\n",
    "                image = image_shift(arg.image, -last_shift[0], -last_shift[1])\n",
    "                last_shift = None\n",
    "                return image\n",
    "        return arg.image\n",
    "\n",
    "    prompt_embeds = scale_embedding(ClipSingleton.encode_text(\n",
    "        [prompt] * batch_size, device=clip_device\n",
    "    ))\n",
    "    \n",
    "    noise = (torch.randn(batch_size, 4, size, size) * contrast + brightness).clamp(0, 1)\n",
    "            \n",
    "    output = pipe.run_inference_on(\n",
    "        noise, embedding=prompt_embeds, num_inference_steps=steps,\n",
    "        timestep_offset=offset,\n",
    "        callback=_callback,\n",
    "    ).images\n",
    "\n",
    "    # output = output.repeat(1, 1, 3, 3)\n",
    "    display(VF.to_pil_image(resize(make_grid(output, nrow=4), 2)))\n",
    "\n",
    "    template = wang_template.with_new_image(make_grid(output, nrow=4, padding=0))\n",
    "    map = wangtiles.wang_map_stochastic_scanline(wang_tiles, (9, 12))\n",
    "    display(VF.to_pil_image(resize(template.render_map(map), 2)))\n",
    "\n",
    "\n",
    "wang_images_from_prompt(\n",
    "    #\"wooden texture\",\n",
    "    #\"blood splattered \"\n",
    "    #\"dark cobblestone\", \n",
    "    #\"black and white\", \n",
    "    #\"pixelart\",\n",
    "    #\"alien\",\n",
    "    #\"alien glittering magnified cobblestone background pattern\",\n",
    "    \"hieroglyphs\",\n",
    "    steps=50, random_shift=0, brightness=0., contrast=4.,\n",
    "    fac=.04,\n",
    "    #blur=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f4a13-26bf-45aa-953b-c495499e86a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wangtiles.wang_map_stochastic_scanline?\n",
    "map = wangtiles.wang_map_stochastic_scanline(wangtiles.WangTiles2C(), (9, 12))\n",
    "VF.to_pil_image(resize(_340.render_map(map), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c8e9df-2e2d-4a26-893e-fbae8f07a58e",
   "metadata": {},
   "source": [
    "## random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34b480-187c-4dd7-a823-8e875d4da1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = .5 + torch.randn(16*16, 4, 32, 32) #.repeat(16, 1, 1, 1) \n",
    "embeddings = torch.randn(16, 512).repeat(16, 1).to(pipe.device) * .5\n",
    "output = pipe.run_inference_on(input, embedding=embeddings, num_inference_steps=20).images\n",
    "VF.to_pil_image(make_grid(output, nrow=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e543c08-8647-45e0-acd8-2935b8f6c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_image = VF.to_tensor(PIL.Image.open(\n",
    "    #\"/home/bergi/Pictures/Abstract-Radiation-Hazard-Sign.jpg\"\n",
    "    \"/home/bergi/Pictures/kali2.png\"\n",
    ").convert(\"RGBA\"))\n",
    "some_image = VF.resize(some_image, (64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709c915b-2a57-44fa-b4ee-e76310185be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_images_prompt(\n",
    "    prompt: str, batch_size: int = 8, size: int = 32, steps: int = 100, offset: int = 0,\n",
    "    random_shift: int = 0,\n",
    "    brightness: float = 0.,\n",
    "):\n",
    "    image_widget = ImageWidget()\n",
    "    display(image_widget)\n",
    "    images = []\n",
    "    \n",
    "    def _callback(arg: DDPMPipelineWithEmbedding.CallbackArg):\n",
    "        if arg.iteration % max(1, steps // 7) == 0:\n",
    "            image = (arg.image * .5 + .5).clamp(0, 1)\n",
    "            images.extend(image.cpu())\n",
    "            image_widget.set_torch(resize(make_grid(images, nrow=batch_size), 2))\n",
    "\n",
    "        if arg.iteration < steps - 1:\n",
    "            arg.image = arg.image + 0\n",
    "            arg.image[:, :3, :, :9] = ds[8523][0][:3, :, :9]\n",
    "        \n",
    "        if random_shift > 0:\n",
    "            return image_shift(\n",
    "                arg.image, \n",
    "                random.randrange(-random_shift, random_shift + 1),\n",
    "                random.randrange(-random_shift, random_shift + 1),\n",
    "            )\n",
    "        return arg.image #* 1.01 - 0.001\n",
    "\n",
    "    prompt_embeds = scale_embedding(ClipSingleton.encode_text(\n",
    "        [prompt] * batch_size,\n",
    "        device=clip_device\n",
    "    ))\n",
    "    \n",
    "    input = noise = torch.randn(batch_size, 4, size, size) + brightness# * 1. + .5\n",
    "    #input = (some_image.unsqueeze(0).repeat(batch_size, 1, 1, 1)).clamp(0, 1)\n",
    "    #input[:, :3] = 1. - input[:, :3]\n",
    "    #input = (input + .1 * (noise - input) + .3).clamp(0, 1)\n",
    "    \n",
    "    if offset > 0:\n",
    "        pipe.scheduler.set_timesteps(steps)\n",
    "        #print(\"X\", steps, pipe.scheduler.timesteps)\n",
    "        #print(\"X\", offset, pipe.scheduler.timesteps[offset])\n",
    "        input = pipe.scheduler.add_noise(\n",
    "            input, torch.randn_like(input) + (.5 * (steps - offset) / steps), \n",
    "            pipe.scheduler.timesteps[offset],\n",
    "            #torch.LongTensor([950])\n",
    "        )\n",
    "        \n",
    "    images.extend(input)\n",
    "    output = pipe.run_inference_on(\n",
    "        input, embedding=prompt_embeds, num_inference_steps=steps,\n",
    "        timestep_offset=offset,\n",
    "        callback=_callback,\n",
    "    ).images\n",
    "    \n",
    "    display(VF.to_pil_image(resize(make_grid(output, nrow=batch_size), 2)))\n",
    "\n",
    "gen_images_prompt(\n",
    "    \"blood stained brick wall\", \n",
    "    size=32, batch_size=12, steps=10, offset=0, random_shift=0, brightness=.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39111c87-f930-4f11-9a1a-78ddc8d5a2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f7712-e691-4601-bd48-54dc06ed9bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0ab86-f750-4a70-ac69-3acdba2bbe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompts(\n",
    "    *prompts: str,\n",
    "    steps: int = 20,\n",
    "    count: int = 6 * 6,\n",
    "    size: int = 32,\n",
    "):\n",
    "    prompt_embeds = scale_embedding(ClipSingleton.encode_text(prompts, device=clip_device))\n",
    "\n",
    "    grid = []\n",
    "    try:\n",
    "        for idx in range(len(prompts)):\n",
    "            images = pipe(\n",
    "                embedding=prompt_embeds[None, idx].repeat(count, 1), \n",
    "                batch_size=count, \n",
    "                num_inference_steps=steps,\n",
    "                size=size,\n",
    "            ).images\n",
    "            grid.append(make_grid(images, nrow=int(math.ceil(math.sqrt(count)))))\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    if grid:\n",
    "        display(VF.to_pil_image(make_grid(grid, nrow=int(math.ceil(math.sqrt(len(prompts)))), padding=5)))\n",
    "\n",
    "gen_prompts(\n",
    "    \"depiction of a sword\",\n",
    "    \"plants\",\n",
    "    size=32,\n",
    "    steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec9b5f-1ff1-417a-af04-f1c4ee959604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9243b-0c89-4569-9021-57abe913e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tilable_images_from_prompt(\n",
    "    prompt: str, batch_size: int = 8, size: int = 32, steps: int = 100, offset: int = 0,\n",
    "    random_shift: int = 1,\n",
    "    brightness: float = .0,\n",
    "    contrast: float = 1.,\n",
    "):\n",
    "    last_shift = None\n",
    "    def _callback(arg: DDPMPipelineWithEmbedding.CallbackArg):\n",
    "        nonlocal last_shift\n",
    "        if random_shift > 0:\n",
    "            if last_shift is None:\n",
    "                last_shift = (\n",
    "                    #(random.randrange(2) - 1) * random_shift, (random.randrange(2) - 1) * random_shift,\n",
    "                    random.randrange(-random_shift, random_shift + 1),\n",
    "                    random.randrange(-random_shift, random_shift + 1),\n",
    "                )\n",
    "                return image_shift(arg.image, *last_shift) \n",
    "            else:\n",
    "                image = image_shift(arg.image, -last_shift[0], -last_shift[1])\n",
    "                last_shift = None\n",
    "                return image\n",
    "        return arg.image\n",
    "\n",
    "    prompt_embeds = scale_embedding(ClipSingleton.encode_text(\n",
    "        [prompt] * batch_size, device=clip_device\n",
    "    ))\n",
    "    \n",
    "    noise = (torch.randn(batch_size, 4, size, size) * contrast + brightness).clamp(0, 1)\n",
    "            \n",
    "    output = pipe.run_inference_on(\n",
    "        noise, embedding=prompt_embeds, num_inference_steps=steps,\n",
    "        timestep_offset=offset,\n",
    "        callback=_callback,\n",
    "    ).images\n",
    "\n",
    "    output = output.repeat(1, 1, 3, 3)\n",
    "    \n",
    "    display(VF.to_pil_image(resize(make_grid(output, nrow=4), 1)))\n",
    "\n",
    "\n",
    "tilable_images_from_prompt(\n",
    "    #\"blood splattered cobblestone\", \n",
    "    \"cobblestone pattern\",\n",
    "    #\"pixelart\",\n",
    "    size=64, batch_size=16, steps=50, random_shift=1, brightness=.4, contrast=1.,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ad9e6-d917-4453-a144-91480f601717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96e761f-0fab-4c30-a600-acd30687ab71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda34f30-05f4-4ac6-8632-38c135837a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def images_from_prompt2(\n",
    "    prompt: str, batch_size: int = 8, size: int = 32, steps: int = 100, offset: int = 0,\n",
    "    random_shift: int = 1,\n",
    "    brightness: float = .0,\n",
    "    contrast: float = 1.,\n",
    "):\n",
    "    last_shift = None\n",
    "    def _callback(arg: DDPMPipelineWithEmbedding.CallbackArg):\n",
    "        arg.image = arg.image + 0\n",
    "        arg.image[:1, 10:20, 10:20] = .5\n",
    "        nonlocal last_shift\n",
    "        if random_shift > 0:\n",
    "            if last_shift is None:\n",
    "                last_shift = (\n",
    "                    #(random.randrange(2) - 1) * random_shift, (random.randrange(2) - 1) * random_shift,\n",
    "                    random.randrange(-random_shift, random_shift + 1),\n",
    "                    random.randrange(-random_shift, random_shift + 1),\n",
    "                )\n",
    "                return image_shift(arg.image, *last_shift) \n",
    "            else:\n",
    "                image = image_shift(arg.image, -last_shift[0], -last_shift[1])\n",
    "                last_shift = None\n",
    "                return image\n",
    "        return arg.image\n",
    "\n",
    "    prompt_embeds = scale_embedding(ClipSingleton.encode_text(\n",
    "        [prompt] * batch_size, device=clip_device\n",
    "    ))\n",
    "    \n",
    "    noise = (torch.randn(batch_size, 4, size, size) * contrast + brightness).clamp(0, 1)\n",
    "            \n",
    "    output = pipe.run_inference_on(\n",
    "        noise, embedding=prompt_embeds, num_inference_steps=steps,\n",
    "        timestep_offset=offset,\n",
    "        callback=_callback,\n",
    "    ).images\n",
    "\n",
    "    # output = output.repeat(1, 1, 3, 3)\n",
    "    display(VF.to_pil_image(resize(make_grid(output, nrow=4), 1)))\n",
    "\n",
    "\n",
    "images_from_prompt2(\n",
    "    #\"blood splattered cobblestone\", \n",
    "    \"cobblestone pattern, high contrast\",\n",
    "    size=128, batch_size=16, steps=50, random_shift=0, brightness=.4, contrast=1.,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a6a97-7e21-4aff-948e-f1507bcd18f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4546cb-16f8-467d-a477-5cc597e8b335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e243f82d-2b55-4b55-9763-ed9ce0dba7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357c5c6-f15a-4fbb-90e7-daaf00b41f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = True\n",
    "embeds = torch.concat([\n",
    "    ClipSingleton.encode_image(VF.to_tensor(PIL.Image.open(\"/home/bergi/Pictures/bob/Bobdobbs.jpg\")), normalize=norm),\n",
    "    ClipSingleton.encode_text([\"bob dobbs\"], normalize=norm),\n",
    "]).cpu()\n",
    "embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e6a17-0eb4-4bab-a6a6-2ca96290d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(embeds.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54be2b2-5298-4838-a041-81c0d5a73f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.models.embeddings import Timesteps\n",
    "ts = Timesteps(512, True, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67e2d85-e56f-4f1e-81b8-a53ccd0cf487",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    te = ts(torch.Tensor([5, 100]))\n",
    "    te = te + embeds.cpu().clamp(-.2, .2) * 10.\n",
    "    display(px.line(te.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6454df95-ba58-46c4-926f-967dc392e993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab4e3e0-05a5-40a2-921b-c67e681e3f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e68cf-c484-4925-aedb-8d4acef2e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = scale_embedding(ClipSingleton.encode_text([\"cobblestone\", \"sword\", \"grass\", \"sand\"], device=clip_device))\n",
    "input = torch.randn(1, 4, 32, 32, generator=torch.manual_seed(23)).clamp(-1, 1).repeat(feature.shape[0], 1, 1, 1)\n",
    "with torch.no_grad():\n",
    "    output = pipe.unet(input.cuda(), torch.LongTensor([500] * feature.shape[0]).cuda(), feature.cuda()).sample.cpu()\n",
    "    restored1 = input - output \n",
    "    output2 = pipe.unet(restored1.cuda(), torch.LongTensor([250] * feature.shape[0]).cuda(), feature.cuda()).sample.cpu()\n",
    "    restored2 = restored1 - output2\n",
    "    output3 = pipe.unet(restored2.cuda(), torch.LongTensor([100] * feature.shape[0]).cuda(), feature.cuda()).sample.cpu()\n",
    "    restored3 = restored2 - output3/2\n",
    "    output4 = pipe.unet(restored3.cuda(), torch.LongTensor([100] * feature.shape[0]).cuda(), feature.cuda()).sample.cpu()\n",
    "    restored4 = restored3 - output4/2\n",
    "\n",
    "grid = (torch.concat([input, output, restored1, output2, restored2, output3, restored3, output4, restored4]) * .5 + .5).clamp(0, 1)\n",
    "VF.to_pil_image(resize(make_grid(grid, nrow=input.shape[0]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919bf4ab-294f-4b7a-bbde-5fe90bfc4d46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd050bd-980e-4bfe-ba8f-b3394bb2d3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
