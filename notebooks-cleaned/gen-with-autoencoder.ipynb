{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ce722-a145-414a-8592-2965fb211ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset, RandomSampler\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import *\n",
    "from src.util import *\n",
    "from src.algo import *\n",
    "from src.models.cnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ddbfa3-0c00-411e-9cc0-5d085ce3c36d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SHAPE = (1, 32, 32)\n",
    "CODE_SIZE = 64\n",
    "dataset = TensorDataset(torch.load(f\"../datasets/fonts-regular-{SHAPE[-2]}x{SHAPE[-1]}.pt\"))\n",
    "#dataset = TransformDataset(dataset, dtype=torch.float, multiply=255.)\n",
    "assert SHAPE == dataset[0][0].shape\n",
    "model = ConvAutoEncoder(SHAPE, channels=[8, 16, 24], kernel_size=7, code_size=CODE_SIZE)\n",
    "model.load_state_dict(torch.load(\"../checkpoints/font1/snapshot.pt\")[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a2503-ecdb-4a2d-bd4a-6263db578e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SHAPE = (3, 64, 64)\n",
    "CODE_SIZE = 64\n",
    "model = ConvAutoEncoder(SHAPE, channels=[32, 32, 23], code_size=CODE_SIZE)\n",
    "model.load_state_dict(torch.load(\"../models/ae/kali-cnn/snapshot.pt\")[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb251f-4238-4416-b8fd-f9e1960da786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = TensorDataset(torch.load(f\"../datasets/kali-uint8-{SHAPE[-2]}x{SHAPE[-1]}.pt\"))\n",
    "dataset = TransformDataset(dataset, dtype=torch.float, multiply=1./255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe40e87-eece-46b7-9240-3448da6c81fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe2c868-d037-45df-b4b8-de824338b188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = torch.cat([dataset[i][0].unsqueeze(0) for i in RandomSampler(dataset, num_samples=16)])\n",
    "repros = model.forward(images).clip(0, 1)\n",
    "VF.to_pil_image(make_grid(torch.cat([images, repros]), nrow=16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98719e6-01a3-4a0a-a9f7-b8e49ab6cc89",
   "metadata": {},
   "source": [
    "# dataset -> features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df73e7-3234-48bf-af0c-77100d062f43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_dataset(dataset):\n",
    "    feature_list = []\n",
    "    for image_batch, in tqdm(DataLoader(dataset, batch_size=50)):\n",
    "        features = model.encoder(image_batch)\n",
    "        feature_list.append(features)\n",
    "        #if len(feature_list) >= 100:\n",
    "        #    break\n",
    "    return torch.cat(feature_list)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = encode_dataset(dataset)\n",
    "print(features.shape)\n",
    "VF.to_pil_image(features[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c3c131-e636-4012-987f-2be26b6cb0c1",
   "metadata": {},
   "source": [
    "# PCA of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68993f47-3086-49ef-84c8-4c3c399f7700",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "pca = PCA(CODE_SIZE)\n",
    "pca.fit(features)\n",
    "#pca.components_\n",
    "pca_weight = torch.Tensor(pca.components_)\n",
    "pca_features = features @ pca_weight.T\n",
    "VF.to_pil_image(pca_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de61ac5-eadf-4e36-8af0-2d2355f20a94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.line(pca_features.std(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce983806-08e7-4dc8-b1a3-98b7c1a72817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(features[0])\n",
    "print(torch.round(features[0] - (pca_features[0] @ pca_weight), decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b587f-7b8e-40d3-9ce3-b2e3a0e2a48e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# generate from PCA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a0005-13b7-42a0-b6f6-a3cf323371f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_pca_samples(features, nrow=16):\n",
    "    features = features @ pca_weight\n",
    "    repros = model.decode(features).clip(0, 1)\n",
    "    \n",
    "    display(VF.to_pil_image(make_grid(repros, nrow=nrow)))\n",
    "    \n",
    "plot_pca_samples(pca_features[:32])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb882d9-b8b5-4b80-9ee9-3c043087a679",
   "metadata": {
    "tags": []
   },
   "source": [
    "# blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d239ec5e-4ea4-47c8-bcd2-dfcdbf595637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_feature_plot(sample1, samples2, nrow=16):\n",
    "    samples = []\n",
    "    for sample2 in samples2:\n",
    "        for i in range(nrow):\n",
    "            t = i / (nrow - 1)\n",
    "            samples.append( (sample1 * (1. - t) + t * sample2).unsqueeze(0) )\n",
    "\n",
    "    features = torch.cat(samples) @ pca_weight\n",
    "    repros = model.decode(features).clip(0, 1)#.view(-1, *SHAPE)\n",
    "    display(VF.to_pil_image(make_grid(repros, nrow=nrow)))\n",
    "    \n",
    "blend_feature_plot(pca_features[4], pca_features[18000:18010])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fda3b1-1f5e-465e-921a-29026002f721",
   "metadata": {
    "tags": []
   },
   "source": [
    "# gen from PCA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635890fe-f20b-4913-a5dc-388c8eb2faa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_pca_sample(pca_sample, nrow=16, comps=5):\n",
    "    modified_samples = pca_sample.view(1, -1).repeat(nrow * comps, 1)\n",
    "    for i, sample in enumerate(modified_samples):\n",
    "        t = ((i % nrow) / 15.) * 2. - 1.\n",
    "        idx = i // nrow\n",
    "        sample[idx] = t * 7.\n",
    "    samples = modified_samples @ pca_weight\n",
    "    repros = model.decoder(samples).clip(0, 1)\n",
    "    display(VF.to_pil_image(make_grid(repros, nrow=nrow)))\n",
    "    \n",
    "plot_pca_sample(pca_features[11007])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3755387c-6cac-4022-8480-c4fe9ab0f334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61085fc-ec16-4633-8ee3-23b3ce791487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9187d194-8694-4cdb-bcd1-ea0e4d2c21dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce24aa9-b4cd-4628-a3a6-d629da6b0008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(\n",
    "        iterable, \n",
    "        total: int = 16, \n",
    "        nrow: int = 16, \n",
    "        return_image: bool = False, \n",
    "):\n",
    "    samples = []\n",
    "    f = ImageFilter()\n",
    "    try:\n",
    "        for image in tqdm(iterable, total=total):\n",
    "            samples.append(image)\n",
    "                \n",
    "            if len(samples) >= total:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    image = VF.to_pil_image(make_grid(samples, nrow=nrow))\n",
    "    if return_image:\n",
    "        return image\n",
    "    display(image)\n",
    "    \n",
    "plot_samples(\n",
    "    ()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54286f81-6d1c-4694-b06b-0789788da73c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
