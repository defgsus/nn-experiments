{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de67573b-3acf-472f-8769-a53ba1089397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset, RandomSampler\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.models as VM \n",
    "from IPython.display import display\n",
    "\n",
    "from src.util.image import *\n",
    "from src.util import *\n",
    "from src.util.embedding import *\n",
    "from src.models.util import *\n",
    "from src.algo import ca1\n",
    "\n",
    "def resize(img, scale: float, mode: VF.InterpolationMode = VF.InterpolationMode.NEAREST):\n",
    "    return VF.resize(img, [max(1, int(s * scale)) for s in img.shape[-2:]], mode, antialias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c45f9cd-7d75-42fd-9407-e5eca48fad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    vgg = VM.vgg19(weights=VM.VGG19_Weights.DEFAULT)\n",
    "    print(f\"params: {num_module_parameters(vgg):,}\")\n",
    "    print(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51abb0eb-2060-42df-b0d1-cd30c8684ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "VF.to_pil_image(get_model_weight_images(vgg, normalize=\"each\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e83e2-5767-4656-8ec6-bf46e0eac037",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    incept = VM.inception_v3(weights=VM.Inception_V3_Weights.DEFAULT)\n",
    "    print(f\"params: {num_module_parameters(incept):,}\")\n",
    "    print(incept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8857b43-9501-4e59-bcf6-123221272832",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    shufflenet = VM.shufflenet_v2_x2_0(weights=VM.ShuffleNet_V2_X2_0_Weights.DEFAULT)\n",
    "    print(f\"params: {num_module_parameters(shufflenet):,}\")    \n",
    "    print(shufflenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045dda5-7067-482c-bd53-030a6f6d8bb8",
   "metadata": {},
   "source": [
    "# find different example patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1907f-fd5b-4316-a0d3-7fa3d176d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = torch.load(\"../datasets/rpg-3x32x32-uint-test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a858d-a33b-413d-89e4-32f6a3c11031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(64)\n",
    "features = pca.fit_transform(patches.flatten(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f054d819-602f-434b-818d-2127f4db86c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "clusterer = KMeans(32, n_init=20)\n",
    "labels = clusterer.fit_predict(features)\n",
    "label_to_index = {}\n",
    "for i, l in enumerate(labels):\n",
    "    label_to_index.setdefault(l, []).append(i)\n",
    "hist = sorted(np.histogram(labels, 32, (0, 31))[0])\n",
    "px.bar(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb7b21-1b7b-46c3-a64a-e5cd16aa47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "image_labels = []\n",
    "num_samples = hist[0]\n",
    "for label in range(32):\n",
    "    for i in range(-num_samples, -1):\n",
    "        idx = label_to_index[label][i]\n",
    "        images.append(patches[idx])\n",
    "        image_labels.append(idx)\n",
    "\n",
    "display(VF.to_pil_image(resize(make_grid_labeled(images, labels=image_labels, nrow=num_samples), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987d1f88-b2d1-4d5f-983a-1fdbdaf00f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_INDICES = [\n",
    "    27, 4, 2, 7, 67, 153, 272, 187, 527, 124, 75, 33, 542, 35, 224, 344, 1644, 2363, 2172,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d7b05-0050-43e3-826d-d85d3e672cbf",
   "metadata": {},
   "source": [
    "# similarity by feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ff9f43-657d-42dc-b091-579c69f751ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.datasets import rpg_tile_dataset_3x32x32\n",
    "\n",
    "def sim_by_feature(\n",
    "        model,\n",
    "        #count: int = 1000,\n",
    "):\n",
    "    features = batch_call(\n",
    "        lambda t: normalize_embedding(model(t.float()).flatten(1)), \n",
    "        patches, verbose=True)\n",
    "    \n",
    "    sim = features @ features.T\n",
    "    indices = sim.argsort(1, descending=True)\n",
    "    #return patches, sim, indices\n",
    "    images = []\n",
    "    image_labels = []\n",
    "    for source_idx in SAMPLE_INDICES:\n",
    "        sim_row = sim[source_idx]\n",
    "        idx_row = sim_row.argsort(descending=True)\n",
    "        for i in itertools.chain(range(32), range(-11, -1)):\n",
    "            idx = idx_row[i]\n",
    "            images.append(patches[idx])\n",
    "            image_labels.append(int(sim_row[idx] * 100))\n",
    "            \n",
    "    display(VF.to_pil_image(make_grid_labeled(images, labels=image_labels, nrow=42)))\n",
    "\n",
    "\n",
    "sim_by_feature(\n",
    "    vgg.features[:6]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5688cf8-ee9a-4ab8-8427-76c7077392ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 20):\n",
    "    m = vgg.features[:i]\n",
    "    print(m)\n",
    "    sim_by_feature(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43977e26-7991-42bf-9909-c8e4ffe729d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "image_labels = []\n",
    "for source_idx in SAMPLE_INDICES:\n",
    "    sim_row = sim[source_idx]\n",
    "    idx_row = sim_row.argsort(descending=True)\n",
    "    for i in itertools.chain(range(32), range(-11, -1)):\n",
    "        idx = idx_row[i]\n",
    "        images.append(patches[idx])\n",
    "        image_labels.append(int(sim_row[idx] * 100))\n",
    "        \n",
    "#images = [\n",
    "#    patches[i] for i in indices[:32, :32].flatten()\n",
    "#]\n",
    "display(VF.to_pil_image(make_grid_labeled(images, labels=image_labels, nrow=42)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb5a2c-bfa8-44af-bafe-a043c34c4c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3133e624-2117-4075-9496-a8d0fdc8faf5",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c15de3-ea6a-47d6-8fe7-7b6d05b2fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg.features:\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c140e8-42c8-41c4-a1bb-29b85f6610cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGFeatures(nn.Module):\n",
    "    def __init__(self, vgg: nn.Module):\n",
    "        super().__init__()\n",
    "        self.layers = vgg.features\n",
    "        self.features = {}\n",
    "        self._layer_map = {}\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                self._layer_map[layer] = f\"conv_{len(self._layer_map) + 1}\"\n",
    "                self.features[self._layer_map[layer]] = None\n",
    "                \n",
    "    def forward(self, image):\n",
    "        x = image\n",
    "        for layer in vgg.features:\n",
    "            x = layer(x)\n",
    "            if layer in self._layer_map:\n",
    "                self.features[self._layer_map[layer]] = x\n",
    "        return x\n",
    "\n",
    "    def features_concat(self, names: Optional[List[str]], gram: bool = True):\n",
    "        features = []\n",
    "        for name, f in self.features.items():\n",
    "            if f is not None and (names is None or name in names):\n",
    "                if gram:\n",
    "                    f = f * f.permute(0, 1, 3, 2)\n",
    "                features.append(f.flatten(-3))\n",
    "        return torch.concat(features, dim=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e796e49e-ae9b-40ad-b9c9-bf90a4ccb011",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelModel(nn.Module):\n",
    "    def __init__(self, image: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.image = nn.Parameter(image)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.image\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
