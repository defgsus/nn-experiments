{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0912fc2f-3f61-4e44-adee-5539bbd330be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset, RandomSampler\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import *\n",
    "from src.util import *\n",
    "from src.algo import *\n",
    "from src.models.cnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a568b6-94ea-4eb8-a3e8-c60c5d07dc70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SHAPE = (3, 128, 128)\n",
    "#dataset = TensorDataset(torch.load(f\"../datasets/kali-uint8-{SHAPE[-2]}x{SHAPE[-1]}.pt\"))\n",
    "\n",
    "SHAPE = (1, 128, 128)\n",
    "dataset = TensorDataset(torch.load(f\"../datasets/pattern-{SHAPE[-3]}x{SHAPE[-2]}x{SHAPE[-1]}-uint.pt\"))\n",
    "\n",
    "assert SHAPE == dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d9e76-1c0c-4873-90e3-5ee4ed03ed6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContrastiveImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns tuple of two image crops and bool if the crops\n",
    "    are from the same image.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            source_dataset: Dataset,\n",
    "            crop_shape: Tuple[int, int],\n",
    "            num_crops: int = 2,\n",
    "            num_contrastive_crops: int = 2,\n",
    "            prob_h_flip: float = .5,\n",
    "            prob_v_flip: float = .5,\n",
    "            prob_hue: float = .0,\n",
    "            prob_saturation: float = .5,\n",
    "            prob_brightness: float = .5,\n",
    "            prob_grayscale: float = 0.,\n",
    "            generator: Optional[torch.Generator] = None\n",
    "    ):\n",
    "        self.source_dataset = source_dataset\n",
    "        self.crop_shape = crop_shape\n",
    "        self.num_contrastive_crops = num_contrastive_crops\n",
    "        self.num_crops = num_crops\n",
    "        self.prob_h_flip = prob_h_flip\n",
    "        self.prob_v_flip = prob_v_flip\n",
    "        self.prob_hue = prob_hue\n",
    "        self.prob_saturation = prob_saturation\n",
    "        self.prob_brightness = prob_brightness\n",
    "        self.prob_grayscale = prob_grayscale\n",
    "        self.generator = torch.Generator() if generator is None else generator\n",
    "\n",
    "        transforms = [self._crop]\n",
    "        if prob_h_flip:\n",
    "            transforms.append(self._h_flip)\n",
    "        if prob_v_flip:\n",
    "            transforms.append(self._v_flip)\n",
    "        if prob_hue:\n",
    "            transforms.append(self._hue)\n",
    "        if prob_saturation:\n",
    "            transforms.append(self._saturation)\n",
    "        if prob_brightness:\n",
    "            transforms.append(self._brightness)\n",
    "        if prob_grayscale:\n",
    "            transforms.append(self._to_grayscale)\n",
    "        self.cropper = VT.Compose(transforms)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.source_dataset) * (self.num_crops + self.num_contrastive_crops)\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[torch.Tensor, torch.Tensor, bool]:\n",
    "        all_crops = self.num_crops + self.num_contrastive_crops\n",
    "\n",
    "        true_index = index // all_crops\n",
    "        crop_index = index % all_crops\n",
    "\n",
    "        image1 = image2 = self._get_image(true_index)\n",
    "        is_same = True\n",
    "\n",
    "        if crop_index >= self.num_crops:\n",
    "            other_index = true_index\n",
    "            while other_index == true_index:\n",
    "                other_index = torch.randint(0, len(self.source_dataset) - 1, (1,), generator=self.generator).item()\n",
    "\n",
    "            image2 = self._get_image(other_index)\n",
    "            is_same = False\n",
    "\n",
    "        return (\n",
    "            self.cropper(image1),\n",
    "            self.cropper(image2),\n",
    "            is_same,\n",
    "        )\n",
    "\n",
    "    def _get_image(self, index: int):\n",
    "        image = self.source_dataset[index]\n",
    "        if isinstance(image, (tuple, list)):\n",
    "            image = image[0]\n",
    "        return image\n",
    "\n",
    "    def _crop(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        h, w = image.shape[-2:]\n",
    "        x = torch.randint(0, h - self.crop_shape[0] + 1, size=(1,), generator=self.generator).item()\n",
    "        y = torch.randint(0, w - self.crop_shape[1] + 1, size=(1,), generator=self.generator).item()\n",
    "\n",
    "        return VF.crop(image, y, x, self.crop_shape[0], self.crop_shape[1])\n",
    "\n",
    "    def _h_flip(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        doit = torch.rand(1, generator=self.generator).item() < self.prob_h_flip\n",
    "        return VF.hflip(image) if doit else image\n",
    "\n",
    "    def _v_flip(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        doit = torch.rand(1, generator=self.generator).item() < self.prob_v_flip\n",
    "        return VF.vflip(image) if doit else image\n",
    "\n",
    "    def _hue(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        amt = torch.rand(1, generator=self.generator).item() - .5\n",
    "        return VF.adjust_hue(image, amt)\n",
    "\n",
    "    def _saturation(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        amt = torch.rand(1, generator=self.generator).item() * 2.\n",
    "        return VF.adjust_saturation(image, amt)\n",
    "\n",
    "    def _brightness(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        amt = torch.rand(1, generator=self.generator).item() + .5\n",
    "        return VF.adjust_brightness(image, amt)\n",
    "\n",
    "    def _to_grayscale(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        doit = torch.rand(1, generator=self.generator).item() < self.prob_grayscale\n",
    "        return VF.rgb_to_grayscale(image, image.shape[0]) if doit else image\n",
    "        \n",
    "                \n",
    "cds = ContrastiveImageDataset(dataset, (64, 64), generator=torch.Generator().manual_seed(23))\n",
    "print(f\"size: {len(cds):,}\")\n",
    "for i in range(30):\n",
    "    i1, i2, is_same = cds[i]\n",
    "    print(is_same)\n",
    "    display(VF.to_pil_image(make_grid([i1, i2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a8c79-b656-44a6-bc35-c71690d7d99b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_positive_negative(cds: ContrastiveImageDataset, total: int = 8*8):\n",
    "    positive, negative = [], []\n",
    "    for i1, i2, is_same in DataLoader(cds, shuffle=True):\n",
    "        the_list = positive if is_same else negative\n",
    "        if len(the_list) < total:\n",
    "            the_list.append(i1.squeeze(0))\n",
    "            the_list.append(i2.squeeze(0))\n",
    "        else:\n",
    "            if len(positive) >= total and len(negative) >= total:\n",
    "                break\n",
    "\n",
    "    display(VF.to_pil_image(make_grid([\n",
    "        make_grid(positive, nrow=8),\n",
    "        make_grid(negative, nrow=8),\n",
    "    ])))\n",
    "    \n",
    "plot_positive_negative(ContrastiveImageDataset(\n",
    "        dataset, crop_shape=(64, 64),\n",
    "        num_crops=1, num_contrastive_crops=1,\n",
    "        prob_h_flip=.5,\n",
    "        prob_v_flip=.5,\n",
    "        prob_hue=.0,\n",
    "        prob_saturation=0.,\n",
    "        prob_brightness=0.9,\n",
    "        prob_grayscale=1.,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8efff6-b381-47d4-8433-f53d26216780",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = dataset[0][0]\n",
    "w = torch.rand((image.shape[0], image.shape[0], 1))\n",
    "#w.shape\n",
    "#image = VF.adjust_hue(image, .4)\n",
    "#image = VF.adjust_saturation(image, 2.)\n",
    "image = VF.adjust_brightness(image, 2)\n",
    "VF.to_pil_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd992a02-53b7-404f-ae39-7073997a9482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = torch.randn(3, 10)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8e4e6-d7bc-40b9-81bc-24ec7cf32457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = features / torch.norm(features, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f04309-28b9-4fc1-bc8b-1d4bd41330db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a767f30-3ca9-400a-9490-c7ca15ee4df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f92283a-39a1-43ae-9af9-68a135be0bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee13f61-f9df-4e3b-a46c-b52e5084ae69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ce777-d4d2-4ba5-8bb1-9a94743166d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36cf87e-b5cc-4bee-af3d-eaaa80317ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = torch.rand(1, 3, 64, 64)\n",
    "nn.Sequential(\n",
    "    nn.Conv2d(3, 5, 7),\n",
    "    nn.MaxPool2d(5),\n",
    ")(i).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f6734-8daf-48d4-8882-da89dd7028f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d23e53-135d-49e9-86ae-2bbb7c926de7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
