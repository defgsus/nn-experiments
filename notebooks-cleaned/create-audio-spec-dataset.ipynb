{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408629e3-067a-4aed-8d55-c08f7f9826e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union, Dict\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset, RandomSampler\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "import torchaudio.transforms as AT\n",
    "import torchaudio.functional as AF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display, Audio\n",
    "import torchaudio\n",
    "from torchaudio.io import StreamReader\n",
    "\n",
    "from src.datasets import *\n",
    "from src.algo import GreedyLibrary\n",
    "from src.util.image import *\n",
    "from src.util import to_torch_device, iter_batches\n",
    "from src.patchdb import PatchDB, PatchDBIndex\n",
    "from src.models.encoder import *\n",
    "from src.models.cnn import *\n",
    "from src.models.util import *\n",
    "from src.models.transform import *\n",
    "from src.util.audio import *\n",
    "from src.util.files import *\n",
    "from src.util.embedding import *\n",
    "from scripts import datasets\n",
    "from src.algo import AudioUnderstander \n",
    "\n",
    "def resize(img, scale: float, mode: VF.InterpolationMode = VF.InterpolationMode.NEAREST):\n",
    "    return VF.resize(img, [max(1, int(s * scale)) for s in img.shape[-2:]], mode, antialias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0bdac4-5425-423e-a908-4584bddbaa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sr = torchaudio.load(\"/home/bergi/Music/Scirocco/07 Bebop.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849eff2d-0c17-48b1-9a27-b3271bb9ad68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 44_100\n",
    "SPEC_SHAPE = (64, 64)\n",
    "\n",
    "speccer = AT.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=1024 * 2,\n",
    "    win_length=SAMPLE_RATE // SPEC_SHAPE[-1],\n",
    "    hop_length=SAMPLE_RATE // SPEC_SHAPE[-1],\n",
    "    n_mels=SPEC_SHAPE[-2],\n",
    "    power=1.,\n",
    ")\n",
    "spec = speccer(audio[:, 40*sr:41*sr].mean(0))\n",
    "VF.to_pil_image(resize(VF.vflip(spec.unsqueeze(0)) / spec.max(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6274b9-14a5-44c9-96e5-690a461b2604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slice_ds = datasets.audio_slice_dataset(\n",
    "    path=\"~/Music/\", recursive=True,\n",
    "    interleave_files=1000,\n",
    "    mono=True,\n",
    "    #shuffle_slices=1_000,\n",
    "    #shuffle_files=True,\n",
    "    with_filename=True,\n",
    "    with_position=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c439e8-e457-416c-bd19-d74ab73e7c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "speccer = speccer.cuda()\n",
    "\n",
    "spec_map = {}\n",
    "num_bytes = 0\n",
    "last_num_bytes = 0\n",
    "max_second = 0.\n",
    "try:\n",
    "    with torch.inference_mode():\n",
    "        for i, (audio, fn, pos) in enumerate(tqdm(slice_ds)):\n",
    "            fn = str(fn)\n",
    "            spec = speccer(audio.cuda()).cpu().squeeze(0)[:, :SPEC_SHAPE[-1]].to(torch.half)\n",
    "            if i == 0:\n",
    "                spec_size_bytes = math.prod(spec.shape) * 2\n",
    "                print(f\"audio shape: {audio.shape}\")\n",
    "                print(f\"spec shape:  {spec.shape} ({spec_size_bytes} bytes)\")\n",
    "                display(VF.to_pil_image(VF.vflip(spec) / spec.max()))\n",
    "            \n",
    "            if fn not in spec_map:\n",
    "                spec_map[fn] = {}\n",
    "            spec_map[fn][str(pos)] = spec\n",
    "            \n",
    "            max_second = max(max_second, pos / SAMPLE_RATE)\n",
    "            num_bytes += spec_size_bytes\n",
    "            if num_bytes - last_num_bytes > 1024**2 * 500:\n",
    "                last_num_bytes = num_bytes\n",
    "                print(f\"bytes: {num_bytes:,} files: {len(spec_map):,} max-sec: {max_second:.2f}\")\n",
    "                \n",
    "            if num_bytes >= 1024**3 * 4:\n",
    "                break\n",
    "                \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e760e-5056-48a3-8d0d-3b98843eaef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(spec_map, \"../datasets/audio-file-pos-spec-1sec-dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c69db1-2bd5-44cf-9ae7-d0930ad8c811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a27660-3fa2-4255-93a3-f46c15f685a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eed5b9-1baf-4e95-8976-fb68f6fc1fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AudioSpecIterableDataset(IterableDataset):\n",
    "\n",
    "    _spec_maps = {}\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            path: Union[str, Path] = \"~/Music/\",\n",
    "            recursive: bool = False,\n",
    "            sample_rate: int = 44100,\n",
    "            slice_size: int = 44100,\n",
    "            stride: Optional[int] = None,\n",
    "            interleave_files: Optional[int] = None,\n",
    "            shuffle_files: bool = False,\n",
    "            shuffle_slices: Optional[int] = None,\n",
    "            mono: bool = False,\n",
    "            seek_offset: float = 0.,\n",
    "            with_filename: bool = False,\n",
    "            with_position: bool = False,\n",
    "            \n",
    "            spec_slice_size: int = 44100,\n",
    "            spec_shape: Tuple[int, int] = (64, 64),\n",
    "            spec_stride: int = 1,\n",
    "    ):\n",
    "        self.spec_shape = spec_shape\n",
    "        self.spec_slice_size = spec_slice_size\n",
    "        self.spec_stride = spec_stride        \n",
    "        self.sample_rate = sample_rate\n",
    "        self.with_filename = with_filename\n",
    "        self.with_position = with_position \n",
    "        self.slice_ds = datasets.audio_slice_dataset(\n",
    "            path=path, \n",
    "            recursive=True,\n",
    "            interleave_files=interleave_files,\n",
    "            slice_size=slice_size,\n",
    "            stride=stride,\n",
    "            mono=mono,\n",
    "            shuffle_files=shuffle_files,\n",
    "            shuffle_slices=shuffle_slices,\n",
    "            seek_offset=seek_offset,\n",
    "            with_position=True,\n",
    "            with_filename=True,\n",
    "        )\n",
    "\n",
    "        self.speccer = AT.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=1024 * 2,\n",
    "            win_length=self.spec_slice_size // spec_shape[-1],\n",
    "            hop_length=self.spec_slice_size // spec_shape[-1],\n",
    "            n_mels=spec_shape[-2],\n",
    "            power=1.,\n",
    "        )\n",
    "\n",
    "    def __iter__(self):\n",
    "        for audio, fn, pos in self.slice_ds:\n",
    "            spec = self.speccer(audio)\n",
    "            \n",
    "            #audio_slice_width = int(self.spec_slice_size / self.spec_shape[-1] * audio.shape[-1])\n",
    "            for offset in range(0, spec.shape[-1], self.spec_stride):\n",
    "                # print(offset, self.spec_shape[-1], spec.shape[-1]) \n",
    "                if offset + self.spec_shape[-1] <= spec.shape[-1]:\n",
    "                    audio_offset = int(offset / spec.shape[-1] * audio.shape[-1])\n",
    "                    # print(\"X\", audio_offset, audio_slice_width, audio.shape[-1])\n",
    "                    if audio_offset + self.spec_slice_size <= audio.shape[-1]:\n",
    "                        spec_slice = spec[..., offset:offset + self.spec_shape[-1]]\n",
    "                        audio_slice = audio[..., audio_offset:audio_offset + self.spec_slice_size]\n",
    "\n",
    "                        yield audio_slice, spec_slice, fn, pos + audio_offset\n",
    "\n",
    "            #for offset in range(0, audio.shape[-1], audio.shape[-1] // 50):\n",
    "            #    if offset + self.sample_rate <= audio.shape[-1]:\n",
    "            #        audio_slice = audio[offset: offset + self.sample_rate]\n",
    "            #        yield audio, offset\n",
    "\n",
    "SAMPLE_RATE = 44_100\n",
    "SPEC_SHAPE = (64, 64)\n",
    "ds = AudioSpecIterableDataset(\n",
    "    \"~/Music\", recursive=True,\n",
    "    slice_size=SAMPLE_RATE * 4,\n",
    "    stride=SAMPLE_RATE * 2,\n",
    "    spec_shape=SPEC_SHAPE,\n",
    "    spec_slice_size=SAMPLE_RATE // 4,\n",
    "    spec_stride=1,\n",
    "    interleave_files=1000,\n",
    "    mono=True,\n",
    ")\n",
    "#ds = IterableShuffle(ds, 100_000)\n",
    "if 1:\n",
    "    audios, specs = [], []\n",
    "    for i, (audio, spec, filename, pos) in zip(range(16), ds):\n",
    "        print(audio.shape, spec.shape, pos, str(filename)[-30:])\n",
    "        audios.append(plot_audio(audio / audio.abs().max(), tensor=True, shape=SPEC_SHAPE))\n",
    "        specs.append(VF.vflip(spec.mean(0).unsqueeze(0)) / spec.max())\n",
    "    display(VF.to_pil_image(make_grid(audios)))\n",
    "    display(VF.to_pil_image(make_grid(specs)))\n",
    "else:\n",
    "    for audio, spec, filename, pos in tqdm(ds):    \n",
    "        #print(audio.shape, spec.shape); break\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c658122-fef1-4dd6-8fb2-b9212fd1fa60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db204eb0-7e1c-474b-8487-639bf4acc9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e495cefb-e893-4924-b2b1-8ca15ae13964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57679d5d-0a5f-4955-a17f-7d5e41b8f01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc5030-0997-40fa-907a-0cce3ed2f93b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1f8607-50bc-41e2-82b3-1a3ee73d1b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef474a-4f42-4748-bc46-7cfacc9274a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f565ae-ea08-425f-a592-e9e1ff2f5bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24975197-789f-4316-b2f6-a5d0e5627516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "449c9848-17ad-4f44-87bd-8de41aa07971",
   "metadata": {},
   "source": [
    "# dev spec to audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fde756-2660-493c-924e-e767673827b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AUDIO_SIZE = SAMPLE_RATE // 4\n",
    "if 1:\n",
    "    decoder = nn.Sequential(\n",
    "        nn.Flatten(1),\n",
    "        nn.Linear(math.prod(SPEC_SHAPE), math.prod(SPEC_SHAPE)),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(math.prod(SPEC_SHAPE), AUDIO_SIZE),\n",
    "        nn.Tanh(),\n",
    "        Reshape((1, AUDIO_SIZE)),\n",
    "    )\n",
    "    decoder.load_state_dict(torch.load(\"../checkpoints/sta6/best.pt\")[\"state_dict\"])\n",
    "else:\n",
    "    from scripts.train_spec_to_audio import SpecToWave\n",
    "    decoder = SpecToWave(SPEC_SHAPE, AUDIO_SIZE, 10)\n",
    "    decoder.load_state_dict(torch.load(\"../checkpoints/sta5-sr4/best.pt\")[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca68aa7-ad67-4f5d-836c-551e40441d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio, _ = torchaudio.load(\"/home/bergi/Music/Scirocco/07 Bebop.mp3\")\n",
    "#audio, _ = torchaudio.load(\"/home/bergi/Music/Ray Kurzweil The Age of Spiritual Machines/(audiobook) Ray Kurzweil - The Age of Spiritual Machines - 1 of 4.mp3\")\n",
    "speccer = AT.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=1024 * 2,\n",
    "    win_length=SAMPLE_RATE // SPEC_SHAPE[-1],\n",
    "    hop_length=SAMPLE_RATE // SPEC_SHAPE[-1],\n",
    "    n_mels=SPEC_SHAPE[-2],\n",
    "    power=1.,\n",
    ")\n",
    "spec = speccer(audio.mean(0)[SAMPLE_RATE * 38:SAMPLE_RATE * 52])\n",
    "print(spec.shape)\n",
    "VF.to_pil_image(VF.vflip(spec) / spec.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd8cafe-d1d4-419b-b81d-4ac60aa2f67b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_spec = spec[..., :SPEC_SHAPE[-1]].unsqueeze(0)\n",
    "recon = decoder(in_spec).squeeze(0)\n",
    "display(plot_audio(recon, (128, 1000)))\n",
    "out_spec = speccer(recon)\n",
    "display(VF.to_pil_image(in_spec / in_spec.max()))\n",
    "VF.to_pil_image(out_spec / out_spec.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8467f4a3-f651-4bea-aaa3-4c75cf2c08e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def spec_to_audio(decoder, spec, spec_shape, audio_size, stride=1, sub_sample: float = 2):\n",
    "    def _yield_patches():\n",
    "        audio_x = 0\n",
    "        spec_x = 0\n",
    "        while spec_x + spec_shape[-1] <= spec.shape[-1]:\n",
    "            yield (\n",
    "                spec[:, spec_x:spec_x + spec_shape[-1]], \n",
    "                audio_x,\n",
    "            )\n",
    "            spec_x += stride\n",
    "            audio_x += max(1, int(audio_size / sub_sample))\n",
    "                \n",
    "    result = torch.zeros(int(spec.shape[-1] / spec_shape[-1] * audio_size) * 2)\n",
    "    result_sum = torch.zeros_like(result)\n",
    "    \n",
    "    for spec_batch, pos_batch in iter_batches(_yield_patches(), 32):\n",
    "        audio_batch = decoder(spec_batch).mean(1)\n",
    "        for pos, audio_slice in zip(pos_batch, audio_batch):\n",
    "            end = pos + audio_slice.shape[0]\n",
    "            if end > result.shape[-1]:\n",
    "                result = torch.concat([result, torch.zeros(end - result.shape[-1])])\n",
    "                result_sum = torch.concat([result_sum, torch.zeros(end - result_sum.shape[-1])]) \n",
    "            s = slice(pos, end)\n",
    "            \n",
    "            result[s] = result[s] + audio_slice\n",
    "            result_sum[s] = result_sum[s] + 1.\n",
    "    \n",
    "    mask = result_sum > 0\n",
    "    result[mask] = result[mask] / result_sum[mask]\n",
    "    return result\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    wave = spec_to_audio(decoder, spec, SPEC_SHAPE, AUDIO_SIZE, stride=30, sub_sample=1)\n",
    "print(\n",
    "    f\"spec {spec.shape[-1]}/{SPEC_SHAPE[-1]}={spec.shape[-1]//SPEC_SHAPE[-1]}\"\n",
    "    f\" audio-size {AUDIO_SIZE} wave: {wave.shape}\")\n",
    "display(Audio(wave, rate=SAMPLE_RATE))\n",
    "plot_audio(wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4b20a-5dfe-4ddb-a7f1-fbddf46f92da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
