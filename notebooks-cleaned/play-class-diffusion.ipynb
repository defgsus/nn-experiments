{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06fdab-0195-4968-b0e3-f1951721516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_notebook import *\n",
    "\n",
    "import diffusers\n",
    "\n",
    "from experiments.datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ccfb4b-eaf0-4da1-9270-fb49d01890b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMPipelineWithClasses(diffusers.DDPMPipeline):\n",
    "\n",
    "    @dataclass\n",
    "    class CallbackArg:\n",
    "        pipeline: \"DDPMPipelineWithClasses\"\n",
    "        image: torch.Tensor\n",
    "        iteration: int\n",
    "        timestep: int\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "            self,\n",
    "            classes: Iterable[int] = (0,),\n",
    "            batch_size: int = 1,\n",
    "            generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "            num_inference_steps: int = 1000,\n",
    "            output_type: Optional[str] = \"pt\",\n",
    "            return_dict: bool = True,\n",
    "            callback: Optional[Callable[[CallbackArg], torch.Tensor]] = None,\n",
    "    ) -> Union[diffusers.ImagePipelineOutput, Tuple]:\n",
    "        from diffusers.utils.torch_utils import randn_tensor\n",
    "\n",
    "        # Sample gaussian noise to begin loop\n",
    "        if isinstance(self.unet.config.sample_size, int):\n",
    "            image_shape = (\n",
    "                batch_size,\n",
    "                self.unet.config.in_channels,\n",
    "                self.unet.config.sample_size,\n",
    "                self.unet.config.sample_size,\n",
    "            )\n",
    "        else:\n",
    "            image_shape = (batch_size, self.unet.config.in_channels, *self.unet.config.sample_size)\n",
    "\n",
    "        if self.device.type == \"mps\":\n",
    "            # randn does not work reproducibly on mps\n",
    "            image = randn_tensor(image_shape, generator=generator)\n",
    "            image = image.to(self.device)\n",
    "        else:\n",
    "            image = randn_tensor(image_shape, generator=generator, device=self.device)\n",
    "\n",
    "        class_labels = list(classes)\n",
    "        while len(class_labels) < batch_size:\n",
    "            class_labels.extend(class_labels)\n",
    "        class_labels = torch.LongTensor(class_labels[:batch_size]).to(self.device)\n",
    "\n",
    "        return self.run_inference_on(\n",
    "            image=image / 2. + .5,\n",
    "            class_labels=class_labels,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            output_type=output_type,\n",
    "            return_dict=return_dict,\n",
    "            generator=generator,\n",
    "            callback=callback,\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def run_inference_on(\n",
    "            self,\n",
    "            image: torch.Tensor,\n",
    "            class_labels: Union[int, Iterable, torch.LongTensor],\n",
    "            num_inference_steps: int = 1000,\n",
    "            timestep_offset: int = 0,\n",
    "            timestep_count: Optional[int] = None,\n",
    "            generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "            output_type: Optional[str] = \"pt\",\n",
    "            return_dict: bool = True,\n",
    "            callback: Optional[Callable[[CallbackArg], torch.Tensor]] = None,\n",
    "    ):\n",
    "        image = image.to(self.device)\n",
    "        \n",
    "        if image.ndim == 4:\n",
    "            pass\n",
    "        elif image.ndim == 3:\n",
    "            image = image.unsqueeze(0)\n",
    "        else:\n",
    "            raise ValueError(f\"`image` must have 4 (or 3) dimensions, got {image.shape}\")\n",
    "\n",
    "        image = image * 2. - 1.\n",
    "\n",
    "        if isinstance(class_labels, int):\n",
    "            class_labels = torch.LongTensor([class_labels]).to(self.device)\n",
    "        elif not isinstance(class_labels, torch.Tensor):\n",
    "            class_labels = torch.LongTensor(class_labels).to(self.device)\n",
    "    \n",
    "        if class_labels.shape[0] != image.shape[0]:\n",
    "            raise ValueError(f\"batch-size of `class_labels` must match `image`, expected {image.shape[0]}, got {class_labels.shape}\")\n",
    "\n",
    "        # set step values\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        timesteps = timestepsX = self.scheduler.timesteps[timestep_offset:]\n",
    "        if timestep_count is not None:\n",
    "            timesteps = timesteps[:timestep_count]\n",
    "        # print(timestep_count, timestep_offset, timesteps, timestepsX)\n",
    "        for idx, t in enumerate(self.progress_bar(timesteps)):\n",
    "            # 1. predict noise model_output\n",
    "            model_output = self.unet(image, t, class_labels).sample\n",
    "\n",
    "            # 2. compute previous image: x_t -> x_t-1\n",
    "            image = self.scheduler.step(model_output, t, image, generator=generator).prev_sample\n",
    "\n",
    "            if callback is not None:\n",
    "                image = callback(self.CallbackArg(\n",
    "                    pipeline=self, image=image, iteration=idx, timestep=t\n",
    "                ))\n",
    "\n",
    "        image = (image / 2 + 0.5).clamp(0, 1)\n",
    "        if output_type == \"pt\":\n",
    "            pass\n",
    "        else:\n",
    "            image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
    "            if output_type == \"pil\":\n",
    "                image = self.numpy_to_pil(image)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image,)\n",
    "\n",
    "        return diffusers.ImagePipelineOutput(images=image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105aa717-0a08-4350-8ddf-f2c82286ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline = DDPMPipelineWithClasses(model, diffusers.DDPMScheduler(num_train_timesteps=100))\n",
    "pipe = DDPMPipelineWithClasses.from_pretrained(\"../ddpm-test-01/03\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af9668-52a3-4028-8a53-d51077c02926",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = []\n",
    "try:\n",
    "    for klass in range(3):\n",
    "        images = pipe(classes=[klass], batch_size=6*6, num_inference_steps=30).images\n",
    "        grid.append(make_grid(images, nrow=6))\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "if grid:\n",
    "    display(VF.to_pil_image(make_grid(grid, nrow=6, padding=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc5b77e-f89f-4a3c-8531-a8bd984f4e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf194c3-a632-4443-877e-8778806eec39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34b480-187c-4dd7-a823-8e875d4da1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(16*16, 4, 32, 32) + .5\n",
    "label = PixelartDataset.LABELS.index(\"other\")\n",
    "output = pipe.run_inference_on(input, class_labels=[label] * input.shape[0], num_inference_steps=20).images\n",
    "VF.to_pil_image(make_grid(output, nrow=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e543c08-8647-45e0-acd8-2935b8f6c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_image = VF.to_tensor(PIL.Image.open(\"/home/bergi/Pictures/csv-turing.png\").convert(\"RGBA\"))\n",
    "some_image = resize(some_image, 1/32)\n",
    "print(some_image.shape)\n",
    "VF.to_pil_image(some_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709c915b-2a57-44fa-b4ee-e76310185be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_images(label: int, batch_size: int = 8, size: int = 32, steps: int = 100, offset: int = 0):\n",
    "    image_widget = ImageWidget()\n",
    "    display(image_widget)\n",
    "    images = []\n",
    "    \n",
    "    def _callback(arg: DDPMPipelineWithClasses.CallbackArg):\n",
    "        if arg.iteration % max(1, steps // 7) == 0:\n",
    "            image = (arg.image * .5 + .5).clamp(0, 1)\n",
    "            images.extend(image.cpu())\n",
    "            image_widget.set_torch(resize(make_grid(images, nrow=batch_size), 2))\n",
    "        \n",
    "        return arg.image #* 1.01 - 0.001\n",
    "    \n",
    "    #input = torch.randn(batch_size, 4, size, size) * 1. + .5\n",
    "    input = (some_image.unsqueeze(0).repeat(batch_size, 1, 1, 1)).clamp(0, 1)\n",
    "    input[:, :3] = 1. - input[:, :3]\n",
    "\n",
    "    if offset > 0:\n",
    "        pipe.scheduler.set_timesteps(steps)\n",
    "        #print(\"X\", steps, pipe.scheduler.timesteps)\n",
    "        #print(\"X\", offset, pipe.scheduler.timesteps[offset])\n",
    "        input = pipe.scheduler.add_noise(\n",
    "            input, torch.randn_like(input), \n",
    "            #pipe.scheduler.timesteps[offset],\n",
    "            torch.LongTensor([150])\n",
    "        )\n",
    "        \n",
    "    images.extend(input)\n",
    "    output = pipe.run_inference_on(\n",
    "        input, class_labels=[label] * input.shape[0], num_inference_steps=steps,\n",
    "        timestep_offset=offset,\n",
    "        callback=_callback,\n",
    "    ).images\n",
    "    \n",
    "    display(VF.to_pil_image(resize(make_grid(output), 1)))\n",
    "\n",
    "gen_images(label=PixelartDataset.LABELS.index(\"wall\"), size=32, batch_size=8, steps=20, offset=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39111c87-f930-4f11-9a1a-78ddc8d5a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "VF.to_pil_image(resize(make_grid(output), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f7712-e691-4601-bd48-54dc06ed9bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ad9e6-d917-4453-a144-91480f601717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f55d12-45ff-46ad-b52d-dfa4304091f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = some_image.unsqueeze(0).repeat(8, 1, 1, 1) - .3\n",
    "output = pipe.run_inference_on(\n",
    "    input, \n",
    "    class_labels=[0] * input.shape[0],\n",
    "    num_inference_steps=20,\n",
    ")\n",
    "VF.to_pil_image(resize(make_grid(output.images), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24630293-f3e3-4603-88a6-146dc8d9d6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a00619-d86a-4c4c-9da7-7b24c744f7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccb1d9f0-568d-443c-a405-66679b946c22",
   "metadata": {},
   "source": [
    "### process big image in patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac30fcb-9e84-45e0-bc93-65298dfab509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image_patches(\n",
    "    image: torch.Tensor,\n",
    "    label: int, batch_size: int = 16, size: int = 32, steps: int = 30\n",
    "):\n",
    "    image_widget = ImageWidget()\n",
    "    display(image_widget)\n",
    "    image_widget.set_torch(image)\n",
    "    \n",
    "    def _callback(arg: DDPMPipelineWithClasses.CallbackArg):\n",
    "        if arg.iteration % max(1, steps // 7) == 0:\n",
    "            image = (arg.image * .5 + .5).clamp(0, 1)\n",
    "            images.extend(image)\n",
    "            image_widget.set_torch(resize(make_grid(images, nrow=batch_size), 2))\n",
    "        \n",
    "        return arg.image #* 1.01 - 0.001\n",
    "\n",
    "    image = image.to(pipe.device)\n",
    "    pipe.set_progress_bar_config(disable=True)\n",
    "    for timestep_offset in tqdm(range(0, steps, 2)):\n",
    "        for patches, positions in iter_image_patches(\n",
    "                image,\n",
    "                #count=50,\n",
    "                shape=(size, size),\n",
    "                stride=(size // 2, size // 2),\n",
    "                batch_size=batch_size,\n",
    "                with_pos=True,\n",
    "                verbose=False,\n",
    "        ):\n",
    "            patches = pipe.run_inference_on(\n",
    "                patches, class_labels=[label] * patches.shape[0], \n",
    "                num_inference_steps=steps,\n",
    "                timestep_offset=timestep_offset,\n",
    "                timestep_count=5,\n",
    "                #callback=_callback,\n",
    "            ).images\n",
    "\n",
    "            for patch, pos in zip(patches, positions):\n",
    "                s1, s2 = slice(pos[-2], pos[-2] + patch.shape[-2]), slice(pos[-1], pos[-1] + patch.shape[-1])\n",
    "                image[:, s1, s2] += .2 * (patch - image[:, s1, s2])\n",
    "\n",
    "        image_widget.set_torch(image.clamp(0, 1).cpu())\n",
    "        \n",
    "    return image.clamp(0, 1).cpu()\n",
    "    #display(VF.to_pil_image(resize(make_grid(output), 1)))\n",
    "    \n",
    "output = process_image_patches(\n",
    "    torch.randn(4, 256, 256) + .4,\n",
    "    label=PixelartDataset.LABELS.index(\"wall\"), \n",
    "    size=32, batch_size=16, steps=20,\n",
    ")\n",
    "VF.to_pil_image(resize(output, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcfa04f-2f9c-4c43-b316-8f7a7e0d64c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "VF.to_pil_image(resize(output, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
