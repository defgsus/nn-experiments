{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d82c9a-daae-4a20-9236-ade1313a72a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import * \n",
    "from src.util import ImageFilter\n",
    "from src.algo import Space2d, IFS\n",
    "from src.datasets.generative import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdbdb54-d98a-4041-9bd0-cc697984b963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_samples(\n",
    "        iterable, \n",
    "        total: int = 32, \n",
    "        nrow: int = 8, \n",
    "        return_image: bool = False, \n",
    "        show_compression_ratio: bool = False,\n",
    "        label: Optional[Callable] = None,\n",
    "):\n",
    "    samples = []\n",
    "    labels = []\n",
    "    f = ImageFilter()\n",
    "    try:\n",
    "        for entry in tqdm(iterable, total=total):\n",
    "            image = entry\n",
    "            if isinstance(entry, (list, tuple)):\n",
    "                image = entry[0]\n",
    "            if image.ndim == 4:\n",
    "                image = image.squeeze(0)\n",
    "            samples.append(image)\n",
    "            if show_compression_ratio:\n",
    "                labels.append(round(f.calc_compression_ratio(image), 3))\n",
    "            elif label is not None:\n",
    "                labels.append(label(entry))\n",
    "                \n",
    "            if len(samples) >= total:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    if labels:\n",
    "        image = VF.to_pil_image(make_grid_labeled(samples, nrow=nrow, labels=labels))\n",
    "    else:\n",
    "        image = VF.to_pil_image(make_grid(samples, nrow=nrow))\n",
    "    if return_image:\n",
    "        return image\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8bccd4-8684-4a2f-92ff-6b844aba117a",
   "metadata": {},
   "source": [
    "# base dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab88ee6-9a48-43a8-9e1f-dfb3b10aad87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = IFSDataset(\n",
    "    shape=(128, 128), num_iterations=2000, alpha=.5,\n",
    "    num_parameters=3,\n",
    "    patch_size=3,\n",
    "    num_classes=1000,\n",
    "    start_seed=0, \n",
    ")\n",
    "plot_samples(dataset, label=lambda l: l[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd1e657-c943-41a7-8535-2b4c99789b7f",
   "metadata": {},
   "source": [
    "# CLIP guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba01260-2fff-4acd-8c9c-063ed3dfdd39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import clip as cliplib\n",
    "CODE_SIZE = 512\n",
    "class ToRGB(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.repeat(1, 3, 1, 1)\n",
    "class ToDevice(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.half().cuda()\n",
    "class FromDevice(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.cpu().to(torch.float32)\n",
    "clip, preproc = cliplib.load(\"ViT-B/32\")\n",
    "encoder = nn.Sequential(\n",
    "    VT.Resize((224, 224), VF.InterpolationMode.BICUBIC),\n",
    "    ToRGB(),\n",
    "    preproc.transforms[-1],\n",
    "    ToDevice(),\n",
    "    clip.visual,\n",
    "    FromDevice(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29454dd0-eca2-427d-b5cb-3b1af0c756af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def encode_texts(texts: List[str]) -> torch.Tensor:\n",
    "    features = clip.encode_text(cliplib.tokenize(texts).cuda()).cpu().float()\n",
    "    return features / features.norm(dim=-1, keepdim=True)\n",
    "@torch.no_grad()\n",
    "def encode_images(images: torch.Tensor) -> torch.Tensor:\n",
    "    features = encoder(images)\n",
    "    return features / features.norm(dim=-1, keepdim=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9078e1cf-5b0f-44b4-b574-79f92f96d649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_features(ds: IFSClassIterableDataset) -> torch.Tensor:\n",
    "    features_list = []\n",
    "    seed_list = []\n",
    "    try:\n",
    "        for images, seeds in tqdm(DataLoader(ds, batch_size=4)):\n",
    "            features_list.append(encode_images(images))\n",
    "            for s in seeds.tolist():\n",
    "                seed_list.append(s)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    return torch.cat(features_list), seed_list\n",
    "\n",
    "ds_features, ifs_seeds = get_dataset_features(dataset)\n",
    "ds_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a23aedd-5a6a-4560-b675-27c1c38324eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    target_features = clip.encode_text(cliplib.tokenize([\n",
    "        #\"very detailed structures\",\n",
    "        \"clustered\",\n",
    "        #\"leave\", #\"bird\"\n",
    "    ]).cuda()).cpu().float()\n",
    "    \n",
    "    target_features /= target_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    for seed in tqdm(range(1000)):\n",
    "        ifs = IFS(seed=seed)\n",
    "        image = torch.Tensor(ifs.render_image((128, 128), 1000, alpha=.5))\n",
    "        features = encoder(image.unsqueeze(0))\n",
    "        features /= features.norm(dim=-1, keepdim=True)\n",
    "        dots = features @ target_features.T\n",
    "        if torch.any(dots > .22):\n",
    "            #image = torch.Tensor(ifs.render_image((128, 128), 10000, alpha=.2))\n",
    "            #print(dots)\n",
    "            #display(VF.to_pil_image(image))\n",
    "            images.append(image)\n",
    "            labels.append(\" \".join(str(round(float(d), 3)) for d in dots[0]))\n",
    "        if len(images) >= 8:\n",
    "            display(VF.to_pil_image(make_grid_labeled(images, labels=labels)))\n",
    "            images.clear()\n",
    "            labels.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9dde27-4ab3-4825-960a-1d3e7f8ef5bf",
   "metadata": {},
   "source": [
    "# dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fea1477-2579-48c9-b544-ecdbb1ac7894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e041889-5678-42d0-b285-4135805b2219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "128*128*16*1000 / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a8817c-3567-4cae-8198-f729335d16fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VF.to_pil_image(torch.Tensor(IFS(seed=955).render_image((512, 512), 400_000, alpha=0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d26a19b-c3d1-4d10-9512-94ced70a2146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = IFSClassIterableDataset(\n",
    "    num_classes=1, num_instances_per_class=8, seed=955,\n",
    "    shape=(128, 128), num_iterations=10_000, alpha=.2,\n",
    "    #shape=(32, 32), num_iterations=1_000, alpha=1,\n",
    "    image_filter=ImageFilter(\n",
    "        min_mean=0.2,\n",
    "        max_mean=0.27,\n",
    "        #min_blurred_compression_ratio=.6,\n",
    "    ),\n",
    "    filter_shape=(32, 32),\n",
    "    filter_num_iterations=1000,\n",
    "    filter_alpha=1.\n",
    ")\n",
    "plot_samples(\n",
    "    ds, total=len(ds), nrow=8, \n",
    "    label=lambda i: str(i[1]), # show seed\n",
    "    #label=lambda i: round(float(i[0].mean()), 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23397396-4c69-48c2-a2e1-f88af51161e4",
   "metadata": {},
   "source": [
    "# store dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1840dec5-af31-40ec-8dd6-b151d0921efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d578435-0465-4d22-bbf4-31a26e795737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = IFSClassIterableDataset(\n",
    "    num_classes=100, num_instances_per_class=8, seed=3746385,\n",
    "    shape=(128, 128), num_iterations=30_000, alpha=.15,\n",
    "    #shape=(32, 32), num_iterations=1_000, alpha=1,\n",
    "    parameter_variation=0.05,\n",
    "    alpha_variation=0.12,\n",
    "    patch_size_variations=[1, 1, 1, 3, 3, 5],\n",
    "    num_iterations_variation=50_000,\n",
    "    image_filter=ImageFilter(\n",
    "        min_mean=0.2,\n",
    "        max_mean=0.27,\n",
    "        #min_blurred_compression_ratio=.6,\n",
    "    ),\n",
    "    filter_shape=(32, 32),\n",
    "    filter_num_iterations=1000,\n",
    "    filter_alpha=1.\n",
    ")\n",
    "plot_samples(dataset, total=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24617b13-c1cd-408c-8293-08030d42b02e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = \"../datasets/ifs-1x128x128-uint8-200x3\"\n",
    "\n",
    "def store_dataset(\n",
    "        images: Iterable,\n",
    "        output_filename,\n",
    "        max_megabyte=4096,\n",
    "):\n",
    "    tensor_batch = []\n",
    "    label_batch = []\n",
    "    tensor_size = 0\n",
    "    last_print_size = 0\n",
    "    try:\n",
    "        for image, label in tqdm(images):\n",
    "\n",
    "            image = (image.clamp(0, 1) * 255).to(torch.uint8)\n",
    "\n",
    "            if len(image.shape) < 4:\n",
    "                image = image.unsqueeze(0)\n",
    "            tensor_batch.append(image)\n",
    "            label_batch.append(torch.Tensor([label]).to(torch.int64))\n",
    "            \n",
    "            tensor_size += math.prod(image.shape)\n",
    "\n",
    "            if tensor_size - last_print_size > 1024 * 1024 * 100:\n",
    "                last_print_size = tensor_size\n",
    "\n",
    "                print(f\"size: {tensor_size:,}\")\n",
    "\n",
    "            if tensor_size >= max_megabyte * 1024 * 1024:\n",
    "                break\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    tensor_batch = torch.cat(tensor_batch)\n",
    "    torch.save(tensor_batch, f\"{output_filename}.pt\")\n",
    "    label_batch = torch.cat(label_batch)\n",
    "    torch.save(label_batch, f\"{output_filename}-labels.pt\")\n",
    "\n",
    "store_dataset(dataset, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b2873a-3b62-482a-8d5e-207e085cbe86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = \"../datasets/ifs-1x128x128-uint8-200x32-seed3482374923\"\n",
    "ds = TensorDataset(\n",
    "    torch.load(f\"{dataset_name}.pt\"),\n",
    "    torch.load(f\"{dataset_name}-labels.pt\"),\n",
    ")\n",
    "print(\"label:\", ds[0][1])\n",
    "VF.to_pil_image(ds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc6ef9f-e3d1-4c3f-8471-8e7b4336d782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_samples(DataLoader(ds, shuffle=False), total=32*200, nrow=32, label=lambda e: int(e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01dc31d-9669-4da0-b86b-0a4b8c8de51d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STORE BIG ONE\n",
    "\n",
    "grid = plot_samples(DataLoader(ds, shuffle=True), total=64*64, nrow=64, label=lambda e: int(e[1]), return_image=True)\n",
    "#grid.save(\"/home/bergi/Pictures/ifs-database-shuffled.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c867a2-b6d5-4707-bc89-5a73c5a27a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de95a48-1866-4e07-a409-f5a6f67080ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = torch.load(f\"../datasets/ifs-1x128x128-uint8-1000x32.pt\")\n",
    "images = torch.load(f\"../datasets/kali-uint8-128x128.pt\")[:10_000]\n",
    "#labels = torch.load(f\"../datasets/ifs-1x128x128-uint8-1000x32-labels.pt\")\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3df1fe-0a9f-49e0-86fa-2ec93726412e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot_samples(images, nrow=32, total=32*32)\n",
    "plot_samples(DataLoader(TensorDataset(images), shuffle=True), nrow=16, total=16*16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3292846a-7814-48ca-a098-a13e118235d7",
   "metadata": {},
   "source": [
    "# show PCA weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7c6f7-6949-4647-a700-a230eb8ccb2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "pca = PCA(256)\n",
    "pca.fit(images.numpy().reshape(images.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f43459-c1f5-439f-9871-af8c9eafd925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = torch.Tensor(pca.components_)\n",
    "mi, ma = weights.min(), weights.max()\n",
    "weights = (weights - mi) / (ma - mi)\n",
    "VF.to_pil_image(make_grid([\n",
    "    w.reshape(images[0].shape) for w in weights\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2915fa90-b841-4874-b9b3-95df6408a593",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = torch.Tensor(pca.eigenvectors_).permute(1, 0).reshape(pca.n_components, 100, 100)\n",
    "VF.to_pil_image(make_grid(\n",
    "    [w.unsqueeze(0) for w in weights]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0463e527-eada-4a8e-b100-99ab0c00cd65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = torch.randn(3, 4)\n",
    "b = torch.rand(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2d6d3-59a0-4e00-b340-d3d44e113326",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "F.kl_div(torch.ones(3, 4)+10, torch.ones(3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe0049-7e1c-4e1d-bcd3-3606d30715a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.log(torch.tensor(.3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
