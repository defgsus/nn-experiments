{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203ce722-a145-414a-8592-2965fb211ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import clip\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset, RandomSampler\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import *\n",
    "from src.util import *\n",
    "from src.algo import *\n",
    "from src.models.cnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ddbfa3-0c00-411e-9cc0-5d085ce3c36d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scripts.train_contrastive import EncoderConv, EncoderTrans\n",
    "\n",
    "SHAPE = (3, 64, 64)\n",
    "if 0:\n",
    "    dataset = TensorDataset(torch.load(f\"../datasets/pattern-1x{SHAPE[-2]}x{SHAPE[-1]}-uint.pt\"))\n",
    "    dataset = TransformDataset(dataset, dtype=torch.float, multiply=1./255., transforms=[lambda i: i.repeat(3, 1, 1)])\n",
    "elif 1:\n",
    "    dataset = TensorDataset(torch.load(f\"../datasets/kali-uint8-{SHAPE[-2]}x{SHAPE[-1]}.pt\"))\n",
    "    dataset = TransformDataset(dataset, dtype=torch.float, multiply=1./255.)\n",
    "else:\n",
    "    dataset = TensorDataset(torch.load(f\"../datasets/photos-{SHAPE[-2]}x{SHAPE[-1]}-bcr03.pt\"))\n",
    "\n",
    "assert SHAPE == dataset[0][0].shape\n",
    "print(f\"{len(dataset):,}\")\n",
    "VF.to_pil_image(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceadc787-f146-4046-89ec-2eec10c6ac1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, preproc = clip.load(\n",
    "    \"RN50\"\n",
    "    #\"ViT-B/32\"\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d12c81b-89db-4d93-85f1-4019aa769067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98719e6-01a3-4a0a-a9f7-b8e49ab6cc89",
   "metadata": {},
   "source": [
    "# dataset -> features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf09caa-01f6-44d0-9a17-bb314a35b696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    f = model.visual(torch.zeros(3, 3, 224, 224).half().cuda())\n",
    "f.min(), f.max(), f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df73e7-3234-48bf-af0c-77100d062f43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_dataset(dataset, max_count=10_000):\n",
    "    feature_list = []\n",
    "    count = 0\n",
    "    try:\n",
    "        for image_batch in tqdm(DataLoader(dataset, batch_size=3)):\n",
    "            image_batch = image_batch[0]\n",
    "            image_batch = VF.resize(image_batch, (224, 224))\n",
    "            features = model.visual(image_batch.half().to(\"cuda\"))\n",
    "            #features = features / torch.norm(features, dim=-1, keepdim=True)\n",
    "            feature_list.append(features)\n",
    "            count += features.shape[0]\n",
    "            if count >= max_count:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    return torch.cat(feature_list)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = encode_dataset(dataset).cpu().float()\n",
    "\n",
    "features_n = features / features.norm(dim=-1, keepdim=True)\n",
    "print(\"shape:\", features.shape)\n",
    "print(\"min/max:\", features.min(), features.max(), features.mean())\n",
    "VF.to_pil_image(features[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808de33e-e727-42bf-b314-c3e8c77e2ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.line(features_n[:10].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b971ec-9e2e-4bd0-a781-9237af6369cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VF.to_pil_image(make_grid([dataset[i][0] for i in range(10)], nrow=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef56914-1d38-4959-8c9e-b55d2552c847",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.line(features.std(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107d823-d4f8-489a-a580-91ee651b46c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "    x=features_n[:1000, 0] * torch.linspace(0.5, 1, 1000), \n",
    "    y=features_n[:1000, 1] * torch.linspace(0.5, 1, 1000), \n",
    "    width=400, height=400, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8892ce75-2c79-4243-89d8-7c96a50e80b7",
   "metadata": {},
   "source": [
    "# sort features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9187d194-8694-4cdb-bcd1-ea0e4d2c21dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "reduction = TSNE(1, verbose=1)\n",
    "positions = torch.Tensor(reduction.fit_transform(features_n)).reshape(-1)\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be754078-f2a0-4f4a-8ab4-c76fc58af218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, indices = torch.sort(positions)\n",
    "images = [\n",
    "    VF.resize(dataset[i][0], (32, 32), VF.InterpolationMode.NEAREST)\n",
    "    for i in itertools.chain(indices[:500], indices[-500:])\n",
    "]\n",
    "VF.to_pil_image(make_grid(images, nrow=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75a3e06-e759-4b5f-8e23-bba9d8983605",
   "metadata": {},
   "source": [
    "# save full sorted image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f48a90a-2258-4982-9aa7-82105156d9d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = [\n",
    "    dataset[i][0]\n",
    "    #VF.resize(dataset[i][0], (32, 32), VF.InterpolationMode.NEAREST)\n",
    "    for i in indices\n",
    "]\n",
    "big_image = VF.to_pil_image(make_grid(images, nrow=64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a4b9fd-4948-4ffe-830a-0bb1bccc60da",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_image.save(Path(\"~/Pictures/kali-of-tsne1d-of-clip-rn50.png\").expanduser())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc37650-8bcf-4bc0-8e73-c63bd3e40a76",
   "metadata": {},
   "source": [
    "# plot similars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c6b9a-daf9-4b4d-a51d-9815354e2d38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_similar_indices(feat, count: int = 10):\n",
    "    #feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "    dot = feat @ features_n.T\n",
    "    _, indices = torch.sort(dot, descending=True)\n",
    "    return indices[:, :count]\n",
    "\n",
    "def plot_similar(indices: Iterable[int], count: int = 10):\n",
    "    indices = list(indices)\n",
    "    sim_indices = get_similar_indices(torch.cat([\n",
    "        features_n[i].unsqueeze(0) for i in indices\n",
    "    ]), count=count)\n",
    "    images = [dataset[i][0] for i in sim_indices.T.reshape(-1)] \n",
    "    display(VF.to_pil_image(make_grid(images, nrow=len(indices))))\n",
    "    \n",
    "#get_similar_indices(features[0:2])\n",
    "plot_similar(list(range(2000, 2020)), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d0f11-e9ed-40ae-a149-c182dbe9688b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54286f81-6d1c-4694-b06b-0789788da73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.load(\"../datasets/kali-uint8-64x64-CLIP.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec11e5-87db-4b15-afc9-9e7a10649d63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.line(pd.DataFrame({\n",
    "    \"min\": f.min(dim=0)[0],\n",
    "    \"max\": f.max(dim=0)[0],\n",
    "}))\n",
    "#px.bar(f.max(dim=0)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
