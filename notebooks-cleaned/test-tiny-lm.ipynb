{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cd1ab0-c15c-4c27-ae0f-3564e5265793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c24cf-e911-49e7-8998-63b4c1384617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-33M')\n",
    "#model = AutoModelForCausalLM.from_pretrained('roneneldan/TinyStories-Instruct-33M')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"blueapple8259/TinyStories-Alpaca\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393a8691-d828-4183-9424-6b8030ea3fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d6513-b1ed-498b-8ab5-ec124a5ee039",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def iter_auto_complete(\n",
    "    prompt: str = \"Once upon a time there was\",\n",
    "    temperature: float = 1.,\n",
    "    max_length: int = 100,\n",
    "    num_beams: int = 1,\n",
    "):\n",
    "    #prompt += tokenizer.\n",
    "    print(prompt)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    try:\n",
    "        for i in range(max_length):\n",
    "            next_logits = model(input_ids)[\"logits\"]\n",
    "            print(input_ids.shape, next_logits.shape)\n",
    "            #print(input_ids)\n",
    "            #print(next_logits.argmax(dim=-1))\n",
    "            #print(tokenizer.decode(input_ids[0]))\n",
    "            #print(tokenizer.decode(next_logits.argmax(dim=-1)[0]))\n",
    "\n",
    "            next_ids = next_logits.argmax(dim=-1)\n",
    "            print(input_ids[0, 1:])\n",
    "            print(next_ids[0, :-1])\n",
    "            num_correct = (next_logits.argmax(dim=-1)[0, :-1] == input_ids[0, 1:]).sum() #/ input_ids.shape[-1]\n",
    "            print(\"CORR\", num_correct)\n",
    "            \n",
    "            next_logits = next_logits[:, -1]\n",
    "            next_id = next_logits.argmax(dim=-1)#.unsqueeze(-1)\n",
    "            yield tokenizer.decode(next_id)\n",
    "            input_ids = torch.concat([input_ids, next_id.unsqueeze(0)], dim=-1)\n",
    "        \n",
    "    #    output_ids = model.generate(input_ids, max_length=1, num_beams=num_beams, temperature=temperature, do_sample=temperature != 1)\n",
    "    #    output_ids = output_ids[:, input_ids.shape[-1]:]\n",
    "    #    input_ids = torch.concat([input_ids, output_ids], dim=-1)\n",
    "    #    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    #    yield output_text\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "        \n",
    "\n",
    "for token in iter_auto_complete(\n",
    "    #\"When Lucy was 28 years old,\"\n",
    "    #\"The blood-splattered cyberpunk said:\",\n",
    "    #\"After ripping off her head\",\n",
    "    #\"One day I went to the park with my big new car\",\n",
    "    #\"The scray monster said, \\\"I will ear your brains\",\n",
    "    #\"It's important to take serious risk\", \n",
    "    #\"Never think before acting\",\n",
    "    #\"Summary: Two astoroids collide\\nFeatures: explosion, devastation\\nSentence: This tore apart the whole galaxy\\nWords: cyber, space\\nStory: In a future life,\",\n",
    "    #\"Summary: A story that teaches children to always follow their leader\\nFeatures: a crazy bear\\nSentence: Always follow your leader!\\nWords: nice, vanilla, nuclear\\nStory: \",\n",
    "    #\"A story about the\", \n",
    "    #\"What is a collateral biscuit?\",\n",
    "    #\"A consecutive sequence: 1, 2, 3, 4\",\n",
    "):\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a85bf85-c0e2-44c7-adb7-356abc4f73e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc0aa0-dc8f-4465-9610-a8bb76c5edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def iter_auto_complete_prev(\n",
    "    prompt: str = \"Once upon a time there was\",\n",
    "    temperature: float = 1.,\n",
    "    max_length: int = 100,\n",
    "    num_beams: int = 1,\n",
    "):\n",
    "    #prompt += tokenizer.\n",
    "    print(prompt)\n",
    "\n",
    "    past_key_values = None\n",
    "    next_id = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    try:\n",
    "        for i in range(max_length):\n",
    "            next_logits, past_key_values = model(next_id, use_cache=True, past_key_values=past_key_values).to_tuple()\n",
    "            next_logits = next_logits[:, -1:]\n",
    "            next_id = next_logits.argmax(dim=-1)#.unsqueeze(-1)\n",
    "            yield tokenizer.decode(next_id.item())\n",
    "            # input_ids = torch.concat([input_ids, next_id.unsqueeze(0)], dim=-1)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "        \n",
    "\n",
    "for token in iter_auto_complete_prev():\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0207a3-4bb3-4577-93eb-b05eb865952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def iter_auto_complete_sample(\n",
    "    prompt: str = \"1, 2, 3, 4,\",  #\"Once upon a time there was\",\n",
    "    max_length: int = 100,\n",
    "    top_k: int = 5,\n",
    "):\n",
    "    #prompt += tokenizer.\n",
    "    print(prompt)\n",
    "\n",
    "    past_key_values = None\n",
    "    next_id = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    try:\n",
    "        for i in range(max_length):\n",
    "            next_logits, past_key_values = model(next_id, use_cache=True, past_key_values=past_key_values).to_tuple()\n",
    "            next_logits = next_logits[:, -1:]\n",
    "\n",
    "            sorted_logit_ids = torch.argsort(next_logits, dim=-1, descending=True)\n",
    "\n",
    "            idx = int(math.pow(random.random() * random.random(), 5) * top_k)\n",
    "            #print(sorted_logit_ids)\n",
    "            next_id = sorted_logit_ids[:, :, idx]\n",
    "            #next_id = next_logits.argmax(dim=-1)\n",
    "            \n",
    "            yield tokenizer.decode(next_id.item(), skip_special_tokens=True)\n",
    "            # input_ids = torch.concat([input_ids, next_id.unsqueeze(0)], dim=-1)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "        \n",
    "\n",
    "for token in iter_auto_complete_sample():\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2cebc7-7144-45ba-8241-bba0305ca9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b448aa-2654-43d0-bc57-09aa2efc6dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6780f203-d866-4b0c-b739-4a6dd19e3bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"  /\\\\  \\n /  \\\\ \\n/____\\\\\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b41adc4-fc71-487f-8f2d-3749576d44c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbd41dd-54a4-41c8-99a4-45582f01daa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8142f684-40f2-4e78-a491-1f0f7f4009bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44a5475-4ebe-4844-822f-e3441432e959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
