{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294b2c6-5915-4277-a418-546e0f27921e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaf6a44-c6a2-481e-a1dc-d239d29d3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = VF.to_tensor(PIL.Image.open(\"/home/bergi/Pictures/__diverse/capitalism2.jpg\"))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966adeec-331a-427d-bcda-e71f24cc2c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import soft_histogram\n",
    "images = torch.cat([img[None, :, :100, :100], img[None, :, 100:200, :100]])\n",
    "images = images.view(2 * 3, 100, 100)\n",
    "h = soft_histogram(images, 128, 0, 1, sigma=100)\n",
    "h = h.view(2, 3, 128).mean(0)\n",
    "px.line(h.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2481e81-4a29-4b19-9c93-28e72e35061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.datasets.classic import _dataset\n",
    "from src.datasets import WrapDataset\n",
    "import torchvision\n",
    "from functools import partial\n",
    "\n",
    "def flowers102_dataset(\n",
    "        train: bool,\n",
    "        shape: Tuple[int, int, int] = (3, 96, 96),\n",
    "        interpolation: bool = True,\n",
    ") -> Dataset:\n",
    "    ds = torchvision.datasets.Flowers102(\n",
    "        \"~/prog/data/datasets/\", split=\"train\" if train else \"test\", #download=True,\n",
    "    )\n",
    "    def cropper(item):\n",
    "        return image_resize_crop(\n",
    "            item, \n",
    "            shape=shape[-2:], \n",
    "            interpolation=VF.InterpolationMode.BILINEAR if interpolation else VF.InterpolationMode.NEAREST,\n",
    "        )\n",
    "        \n",
    "    return (\n",
    "        WrapDataset(ds)\n",
    "        .transform([\n",
    "            VF.to_tensor,\n",
    "            cropper,\n",
    "        ])\n",
    "    )\n",
    "\n",
    "ds = flowers102_dataset(True)\n",
    "VF.to_pil_image(ds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2f5cdf-85f3-4d4d-8646-54af05187706",
   "metadata": {},
   "outputs": [],
   "source": [
    "VF.to_pil_image(make_grid(\n",
    "    [ds[i][0] for i in range(8*8)]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddfedd2-7b70-4480-8da0-1514a4ef05bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbce2740-76f5-461b-91a9-9b6dd3faf7df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46022bfb-3d91-4e23-abc1-89784a527b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgf = torch.fft.fft2(img)\n",
    "imgf.shape, imgf.real.min(), imgf.real.max(), imgf.imag.min(), imgf.imag.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13374a7f-56e9-4466-ad5a-4c370202f904",
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = ds[0][0]\n",
    "img2f = torch.fft.fft2(img2)\n",
    "img2f.shape, img2f.real.min(), img2f.real.max(), img2f.imag.min(), img2f.imag.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b195a9c-7c61-4bb0-a28a-25f30ed4e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(px.line((img2f[0, :10].real.abs().pow(1/4) * img2f[0, :10].real.sign()).T))\n",
    "#imgf[0, 10].real.abs().log()\n",
    "display(px.line((img2f[0, :10].real).T))\n",
    "#imgf[0, 10].real.abs().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ae4368-1b49-464e-b862-c61d87919f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doit(img):\n",
    "    y = torch.fft.fft2(img) #.abs() #.log()\n",
    "    S = 3.\n",
    "    print(y.shape, y.real.min(), y.real.max(), y.imag.min(), y.imag.max())\n",
    "    n = img.shape[-2] ** 2\n",
    "    y.real = (y.real / n).abs().pow(1./S) * y.real.sign()\n",
    "    #mi = y.real.min().clamp_min(y.imag.min())\n",
    "    #n = math.prod(img2f.shape[-2:])\n",
    "    #y = (y + mi) / n \n",
    "    print(y.shape, y.real.min(), y.real.max(), y.imag.min(), y.imag.max())\n",
    "    display(px.imshow(y[0].real))\n",
    "    display(px.line(y[0, :10].real.T))\n",
    "    y.real = y.real.abs().pow(S) * y.real.sign() * n\n",
    "    r = torch.fft.ifft2(y).real #.exp()\n",
    "    print(r.shape, r.min(), r.max())\n",
    "    display(VF.to_pil_image(r))\n",
    "\n",
    "doit(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26165e7-d38f-475a-b1e0-8b42bcfa0abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_fft2(\n",
    "    image: torch.Tensor,\n",
    "    exponent: float = 3.,\n",
    "):\n",
    "    size = image.shape[-1] * image.shape[-2]\n",
    "    \n",
    "    y = torch.fft.fft2(image)\n",
    "    y_r = (y.real / size).abs().pow(1. / exponent) * y.real.sign()\n",
    "    y_i = (y.imag / size).abs().pow(1. / exponent) * y.imag.sign()\n",
    "\n",
    "    return torch.cat([y_r, y_i], dim=-3)\n",
    "\n",
    "\n",
    "def normalized_ifft2(\n",
    "    y: torch.Tensor,\n",
    "    exponent: float = 3.,\n",
    "):\n",
    "    assert y.shape[-3] % 2 == 0, f\"Expected channels divisable by 2, got {y.shape}\"\n",
    "    \n",
    "    chan = y.shape[-3] // 2\n",
    "    size = y.shape[-1] * y.shape[-2]\n",
    "\n",
    "    y_sign = y.sign()\n",
    "    y = y.abs().pow(exponent) * size\n",
    "    y = torch.complex(\n",
    "        y[..., :chan, :, :] * y_sign[..., :chan, :, :],\n",
    "        y[..., chan:, :, :] * y_sign[..., chan:, :, :],\n",
    "    )\n",
    "    image = torch.fft.ifft2(y).real\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "f = normalized_fft2(img)\n",
    "i = normalized_ifft2(f)      \n",
    "VF.to_pil_image(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cba1e6-8a03-40d6-90c2-25375a8eed8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
