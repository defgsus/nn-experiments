{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a980ef25-94f3-4a62-87fa-6285622045a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d6c949-75fd-4bff-b511-c158771fa473",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = PIL.Image.open(\n",
    "    #\"/home/bergi/Pictures/WEEK-51-1-euchlid-consortium.png\"\n",
    "    #\"/home/bergi/Pictures/__diverse/00-them.jpg\"\n",
    "    #\"/home/bergi/Pictures/__diverse/bbjp20250614a0030.jpg\"\n",
    "    \"/home/bergi/Pictures/__diverse/Ivy_Lee.jpg\"\n",
    ")\n",
    "image = image.convert(\"RGB\")\n",
    "image = VF.to_tensor(image)\n",
    "image = resize(image, .2)\n",
    "VF.to_pil_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58347ba9-8b86-4993-ac4b-1a6a625cecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complexity_map(\n",
    "        image: torch.Tensor, \n",
    "        stride: Union[float, int] = 1./4, \n",
    "        fill_value: Union[float, str] = \"mean\", \n",
    "        format: str = \"png\",\n",
    "        recursive: int = 0,\n",
    "        recursive_stride: float = 1./4.,\n",
    "        verbose: bool = True,\n",
    "):\n",
    "    def _get_compressed_size(image: torch.Tensor) -> int:\n",
    "        fp = io.BytesIO()\n",
    "        VF.to_pil_image(image).save(fp, format=format)\n",
    "        return fp.tell()\n",
    "        \n",
    "    h, w = image.shape[-2:]\n",
    "    if isinstance(stride, float):\n",
    "        stride = int(stride * min(w, h))\n",
    "    num_y, num_x = (h ) // stride, (w ) // stride\n",
    "    image = image[:, :num_y * stride, :num_x * stride]\n",
    "    base_size = _get_compressed_size(image)\n",
    "    rows = []\n",
    "    with tqdm(total=num_y * num_x, disable=not verbose) as progress:\n",
    "        for j in range(num_y):\n",
    "            row = []\n",
    "            rows.append(row)\n",
    "            for i in range(num_x):\n",
    "                masked = image.clone()\n",
    "                slice_y = slice(j * stride, (j + 1) * stride)\n",
    "                slice_x = slice(i * stride, (i + 1) * stride)\n",
    "                if isinstance(fill_value, float):\n",
    "                    fill_v = fill_value\n",
    "                elif fill_value == \"mean\":\n",
    "                    fill_v = image[:, slice_y, slice_x].mean(dim=-1).mean(dim=-1)[..., None, None]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unrecognized fill_value {repr(fill_value)}\")\n",
    "                masked[:, slice_y, slice_x] = fill_v\n",
    "                size = _get_compressed_size(masked)\n",
    "                row.append(1. - size / base_size)\n",
    "                progress.update()\n",
    "\n",
    "    cm = torch.Tensor(rows)\n",
    "    if cm.max() == 0:\n",
    "        print(\"XX\", image.shape, image.min(), image.max())\n",
    "    cm /= cm.max() + 0.0000001\n",
    "    \n",
    "    if recursive > 0:\n",
    "        rec_rows = []\n",
    "        with tqdm(total=num_y * num_x, disable=not verbose) as progress:\n",
    "            for j, orig_row in enumerate(cm):\n",
    "                row = []\n",
    "                for i, orig_value in enumerate(orig_row):\n",
    "                    patch = image[:, j * stride: (j + 1) * stride, i * stride: (i + 1) * stride]\n",
    "                    row.append(complexity_map(\n",
    "                        image=patch, \n",
    "                        stride=recursive_stride,\n",
    "                        fill_value=fill_value,\n",
    "                        format=format,\n",
    "                        recursive=recursive-1,\n",
    "                        recursive_stride=recursive_stride,\n",
    "                        verbose=False,\n",
    "                    ))\n",
    "                    progress.update()\n",
    "                rec_rows.append(torch.concat(row, dim=-1) * orig_value)\n",
    "\n",
    "        cm = torch.concat(rec_rows, dim=-2)\n",
    "\n",
    "    if verbose:\n",
    "        cm_img = (cm - cm.min()) / (cm.max() - cm.min())\n",
    "        cm_img = .2 + .8 * VF.resize(cm_img.unsqueeze(0), image.shape[-2:], VF.InterpolationMode.NEAREST)\n",
    "        image2 = image * cm_img#.pow(5)\n",
    "        display(VF.to_pil_image(image2))\n",
    "\n",
    "    return cm\n",
    "\n",
    "cm = complexity_map(image, stride=1./30., recursive=0, recursive_stride=1./4., format=\"png\")\n",
    "px.imshow(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ac99d1-1e0d-4a24-8f70-99364657807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = complexity_map(image, stride=1./10., recursive=0, recursive_stride=1./4., format=\"jpeg\")\n",
    "px.imshow(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dfcb5b-7a10-4f2c-b3a8-f31087fd9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complexity_map_2(\n",
    "        image: torch.Tensor, \n",
    "        stride: Union[float, int] = 1./4, \n",
    "        format: str = \"png\",\n",
    "        verbose: bool = True,\n",
    "):\n",
    "    def _get_compressed_size(image: torch.Tensor) -> int:\n",
    "        fp = io.BytesIO()\n",
    "        VF.to_pil_image(image).save(fp, format=format, compression=9)\n",
    "        return fp.tell()\n",
    "        \n",
    "    h, w = image.shape[-2:]\n",
    "    if isinstance(stride, float):\n",
    "        stride = int(stride * min(w, h))\n",
    "    num_y, num_x = (h ) // stride, (w ) // stride\n",
    "    image = image[:, :num_y * stride, :num_x * stride]\n",
    "    base_size = _get_compressed_size(image)\n",
    "    rows = []\n",
    "    with tqdm(total=num_y * num_x, disable=not verbose) as progress:\n",
    "        for j in range(num_y):\n",
    "            row = []\n",
    "            rows.append(row)\n",
    "            for i in range(num_x):\n",
    "                patch = image[:, j * stride: (j + 1) * stride, i * stride: (i + 1) * stride] \n",
    "                size = _get_compressed_size(patch)\n",
    "                row.append(1. - size / base_size)\n",
    "                progress.update()\n",
    "\n",
    "    cm = torch.Tensor(rows)\n",
    "    cm /= cm.max() + 0.0000001    \n",
    "    return cm\n",
    "\n",
    "cm = complexity_map_2(image, stride=1./10., format=\"png\")\n",
    "px.imshow(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e40f8-d7fa-4984-b768-1fd40fe7d4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complexity_map_3(\n",
    "        image: torch.Tensor, \n",
    "        stride: Union[float, int] = 1./4,\n",
    "        overlap: float = 1./2.,\n",
    "        fill_value: Union[float, str] = \"mean\", \n",
    "        format: str = \"png\",\n",
    "        verbose: bool = True,\n",
    "):\n",
    "    def _get_compressed_size(image: torch.Tensor) -> int:\n",
    "        fp = io.BytesIO()\n",
    "        VF.to_pil_image(image).save(fp, format=format)\n",
    "        return fp.tell()\n",
    "        \n",
    "    h, w = image.shape[-2:]\n",
    "    if isinstance(stride, float):\n",
    "        stride = int(stride * min(w, h))\n",
    "    num_y, num_x = (h ) // stride, (w ) // stride\n",
    "\n",
    "    patches = []\n",
    "    y = 0\n",
    "    max_y, max_x = 0, 0\n",
    "    while y + stride <= image.shape[-2]:\n",
    "        max_y = max(max_y, y)\n",
    "        x = 0\n",
    "        while x + stride <= image.shape[-1]:\n",
    "            max_x = max(max_x, x)\n",
    "            patches.append({\n",
    "                \"x\": x, \"y\": y,\n",
    "                \"slice_x\": slice(x, x + stride),\n",
    "                \"slice_y\": slice(y, y + stride),\n",
    "            })\n",
    "            x += int(stride * (1. - overlap))\n",
    "        y += int(stride * (1. - overlap))\n",
    "\n",
    "    image = image[:, :max_y + stride, :max_x + stride]\n",
    "    base_size = _get_compressed_size(image)\n",
    "\n",
    "    cm_img = torch.zeros_like(image)\n",
    "    cm_count = torch.zeros_like(image)\n",
    "    for patch in tqdm(patches):\n",
    "        masked = image.clone()\n",
    "        slice_x, slice_y = patch[\"slice_x\"], patch[\"slice_y\"]\n",
    "        if isinstance(fill_value, float):\n",
    "            fill_v = fill_value\n",
    "        elif fill_value == \"mean\":\n",
    "            fill_v = image[:, slice_y, slice_x].mean(dim=-1).mean(dim=-1)[..., None, None]\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized fill_value {repr(fill_value)}\")\n",
    "        masked[:, slice_y, slice_x] = fill_v\n",
    "        size = _get_compressed_size(masked)\n",
    "        cm_img[:, slice_y, slice_x] += size\n",
    "        cm_count[:, slice_y, slice_x] += 1\n",
    "\n",
    "    mask = cm_count > 0\n",
    "    cm_img[mask] /= cm_count[mask]\n",
    "    cm_img = base_size - cm_img\n",
    "    print(\"C\", cm_img.shape, cm_img.min(), cm_img.max())\n",
    "    if verbose:\n",
    "        cm = (cm_img - cm_img.min()) / (cm_img.max() - cm_img.min())\n",
    "        cm = .2 + .8 * cm\n",
    "        image2 = image * cm\n",
    "        display(VF.to_pil_image(image2))\n",
    "\n",
    "    return cm_img[0]\n",
    "\n",
    "cm = complexity_map_3(image, stride=1./10., overlap=1-1/4, format=\"png\")\n",
    "px.imshow(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db08f717-f8eb-4c38-9bac-37351e34942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image2 = image[:, :cm.shape[-2], :cm.shape[-1]]\n",
    "image2 = image2 * (cm / cm.max()).pow(10)\n",
    "display(VF.to_pil_image(image2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ed74c5-4da4-4b03-9275-9fd87429151e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
