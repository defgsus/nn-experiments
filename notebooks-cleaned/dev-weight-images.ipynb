{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d062b4f7-c0d9-4db9-8032-cd4916827549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union, Dict\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import *\n",
    "from src.util import *\n",
    "from src.algo import *\n",
    "from src.models.encoder import *\n",
    "from src.models.decoder import *\n",
    "from src.models.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef584c-2b95-45de-b067-da6162b30186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = EncoderConv2d((3, 32, 32), 128, channels=(8, 12, 14), kernel_size=(3, 4, 5))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fe9e68-92da-4072-9ee3-e166541650ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2 = VQVAE()\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b1b25-3ca7-4aab-b002-e4662fe4f108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_model_weight_images(\n",
    "        model: nn.Module,\n",
    "        grad_only: bool = True,\n",
    "        max_channels: int = 16,\n",
    "        min_size: int = 2,\n",
    "        max_size: int = 128,\n",
    "        normalize: str = \"all\",  # \"each\", \"shape\", \"all\", \"none\"\n",
    "        size_to_scale: Dict[int, float] = {10: 4, 20: 2},\n",
    "):\n",
    "    from torchvision.utils import make_grid\n",
    "    from src.util.image import signed_to_image\n",
    "\n",
    "    # yield 2d shapes\n",
    "    def _iter_params():\n",
    "        for param in model.parameters():\n",
    "            if not param.requires_grad and grad_only:\n",
    "                continue\n",
    "            if param.ndim == 2:\n",
    "                yield param\n",
    "            elif param.ndim == 4:\n",
    "                for ch in range(min(max_channels, param.shape[0])):\n",
    "                    yield param[ch, 0]\n",
    "                for ch in range(min(max_channels, param.shape[1])):\n",
    "                    yield param[0, ch]\n",
    "\n",
    "    shape_dict = {}\n",
    "    for param in _iter_params():\n",
    "        if any(s < min_size for s in param.shape):\n",
    "            continue\n",
    "        param = param[:max_size, :max_size]\n",
    "        \n",
    "        scale = None\n",
    "        for key in sorted(size_to_scale):\n",
    "            value = size_to_scale[key]\n",
    "            if all(s <= key for s in param.shape):\n",
    "                scale = value\n",
    "                break\n",
    "        \n",
    "        if scale:\n",
    "            param = VF.resize(\n",
    "                param.unsqueeze(0),\n",
    "                [s * scale for s in param.shape], VF.InterpolationMode.NEAREST, antialias=False\n",
    "            ).squeeze(0)\n",
    "\n",
    "        if param.shape not in shape_dict:\n",
    "            shape_dict[param.shape] = []\n",
    "        shape_dict[param.shape].append(param)\n",
    "\n",
    "    grids = []\n",
    "    for shape in sorted(shape_dict):\n",
    "        params = shape_dict[shape]\n",
    "        nrow = max(1, int(math.sqrt(len(params)) * 2))\n",
    "        if normalize == \"each\":\n",
    "            grids.append(make_grid([signed_to_image(p) for p in params], nrow=nrow))\n",
    "        else:\n",
    "            grids.append(make_grid([p.unsqueeze(0) for p in params], nrow=nrow))\n",
    "        \n",
    "    max_width = max(g.shape[-1] for g in grids)\n",
    "\n",
    "    for image_idx, image in enumerate(grids):\n",
    "        if image.shape[-1] < max_width:\n",
    "            grids[image_idx] = VF.pad(image, [0, 0, max_width - image.shape[-1], 0])\n",
    "\n",
    "    if normalize == \"shape\":\n",
    "        grids = [signed_to_image(g) for g in grids]\n",
    "\n",
    "    grids = torch.concat([\n",
    "        VF.pad(grid, [0, 0, 0, 2])\n",
    "        for grid in grids\n",
    "    ], dim=-2)\n",
    "\n",
    "    if normalize == \"all\":\n",
    "        grids = signed_to_image(grids)\n",
    "\n",
    "    return grids\n",
    "    \n",
    "    \n",
    "model3 = EncoderConv2d((3, 32, 32), code_size=2, channels=(24, 32, 48), kernel_size=11)\n",
    "VF.to_pil_image(get_model_weight_images(model3, normalize=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17ea33-bdad-40cc-ae89-9769822318c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VF.pad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1717e1-948a-4ccc-ad10-5407443443d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f6f08-754a-4936-903f-abfe5a113f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d07965-019b-4d1e-b4c0-3c80b5af84d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from experiments import datasets\n",
    "ds1 = datasets.rpg_tile_dataset_3x32x32((3, 32, 32))\n",
    "ds2 = datasets.mnist_dataset((3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd17734-2ac1-4a80-90fa-5e597f896d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CombineImageAugmentIterableDataset(IterableDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset: Union[Dataset, IterableDataset],\n",
    "            ratio: float = .5,\n",
    "            crop_ratio: Union[float, Tuple[float, float]] = .5,\n",
    "            batch_size: int = 128,\n",
    "    ):\n",
    "        assert batch_size > 1\n",
    "        #num_aug = int(batch_size * ratio)\n",
    "        #if num_aug < 1:\n",
    "        #    raise ValueError(f\"`batch_size` * `ratio` must be >= 1\")\n",
    "            \n",
    "        self.dataset = dataset\n",
    "        self.ratio = ratio\n",
    "        self.batch_size = batch_size\n",
    "        self.crop_ratio = (crop_ratio, crop_ratio) if isinstance(crop_ratio, (float, int)) else tuple(crop_ratio)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        num_aug = int(self.batch_size * self.ratio)\n",
    "        \n",
    "        for batch in iter_batches(self.dataset, self.batch_size):\n",
    "            yield from random_combine_image_crops(batch)\n",
    "\n",
    "            break\n",
    "\n",
    "            \n",
    "for batch in DataLoader(CombineImageAugmentIterableDataset(ds2, ratio=.2), batch_size=64):\n",
    "    images = batch[0]\n",
    "    is_aug = batch[1]\n",
    "    #print(batch.shape)\n",
    "    display(VF.to_pil_image(\n",
    "        make_grid_labeled(images, nrow=8, labels=[\"X\" if a else \"\" for a in is_aug])\n",
    "    ))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e54b4-9108-41b1-904f-c221afc1fb50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747e82b4-0872-46c2-9bbc-6510f4a17887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b5a74-69e6-4608-8127-c0abab1c71c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_combine_image_crops(\n",
    "        images: torch.Tensor,\n",
    "        ratio: float = .5,\n",
    "        crop_ratio: Union[float, Tuple[float, float]] = .5,\n",
    "):\n",
    "    crop_ratio = (crop_ratio, crop_ratio) if isinstance(crop_ratio, (float, int)) else tuple(crop_ratio)\n",
    "    ret_images = []\n",
    "    \n",
    "    for image_idx, image in enumerate(images): \n",
    "        if random.random() > ratio:\n",
    "            ret_images.append(image)\n",
    "        else:\n",
    "            while True:\n",
    "                other_idx = random.randrange(images.shape[0])\n",
    "                if other_idx != image_idx:\n",
    "                    break\n",
    "            other_image = images[other_idx]\n",
    "\n",
    "            crop_size = [\n",
    "                random.uniform(*crop_ratio)\n",
    "                for i in range(2)\n",
    "            ]\n",
    "            crop_size = [\n",
    "                max(1, min(int(c * image.shape[i + 1]), image.shape[i + 1] - 1))\n",
    "                for i, c in enumerate(crop_size)\n",
    "            ]\n",
    "            source_pos = [random.randrange(0, s - crop_size[i]) for i, s in enumerate(other_image.shape[-2:])]\n",
    "            target_pos = [random.randrange(0, s - crop_size[i]) for i, s in enumerate(other_image.shape[-2:])]\n",
    "\n",
    "            image[:, target_pos[0]: target_pos[0] + crop_size[0], target_pos[1]: target_pos[1] + crop_size[1]] = \\\n",
    "                other_image[:, source_pos[0]: source_pos[0] + crop_size[0], source_pos[1]: source_pos[1] + crop_size[1]]\n",
    "\n",
    "            ret_images.append(image)\n",
    "    \n",
    "    return torch.concat([i.unsqueeze(0) for i in ret_images], dim=0)\n",
    "\n",
    "images = next(iter(DataLoader(ds, batch_size=64)))\n",
    "VF.to_pil_image(make_grid(random_combine_image_crops(images[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca1821-fb67-42ad-888f-4c3d5fb910be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = torch.Tensor([0, 1, 2, 3, 4])\n",
    "b = a.clone()\n",
    "b[1] = 5\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba2f97f-befd-4279-b92a-c222ba1fd238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
