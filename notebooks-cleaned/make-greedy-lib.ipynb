{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3767f8d3-dbba-4884-8c96-eb602cd2548a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union, Dict\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset, RandomSampler\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.algo import GreedyLibrary\n",
    "from src.util.image import *\n",
    "from src.util import to_torch_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f8172-3610-4a43-8831-bdea147f9210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#lib = GreedyLibrary.from_torch(\"../models/greedylib-1x31x31-388-photos-6M.pt\")\n",
    "#lib.plot_entries(signed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc6d4c1-74e8-4362-8dc3-dddb399d2d71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -l ../datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02856b82-56e1-49a3-b954-6e7e40e9e17f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    # \"../datasets/ca-64x64-i10-p05.pt\",\n",
    "    \"../datasets/diverse-32x32-std01.pt\",\n",
    "    #\"../datasets/fonts-regular-32x32.pt\",\n",
    "    \"../datasets/ifs-1x128x128-uint8-1000x16.pt\",\n",
    "    \"../datasets/kali-uint8-128x128.pt\",\n",
    "    \"../datasets/pattern-1x128x128-uint.pt\",\n",
    "    \"../datasets/photos-64x64-bcr03.pt\",\n",
    "]\n",
    "\n",
    "SHAPE = (1, 31, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16dab9f-7909-477c-bab9-d5436621e261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_all = None\n",
    "for name in DATASETS:\n",
    "    tensor = torch.load(name)\n",
    "    ds = TensorDataset(tensor)\n",
    "    if tensor.dtype == torch.uint8:\n",
    "        ds = TransformDataset(ds, dtype=torch.float, multiply=1. / 255.)\n",
    "    ds = TransformDataset(ds, transforms=[VT.Grayscale(),])\n",
    "    print(ds[0][0].mean(), name)\n",
    "    if ds_all is None:\n",
    "        ds_all = ds\n",
    "    else:\n",
    "        ds_all = ds_all + ds\n",
    "\n",
    "print(len(ds_all), \"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd41f8-faf9-4c88-b84a-fc0c7f64db17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ALTERNATIVE DATASET\n",
    "            \n",
    "ds_all = TransformIterableDataset(\n",
    "    ImageFolderIterableDataset(\"/home/bergi/Pictures/photos/\", recursive=True),\n",
    "    transforms=[\n",
    "        lambda x: x.to(torch.float) / 255. if x.dtype != torch.float else x,\n",
    "        VT.Grayscale()\n",
    "    ]\n",
    ")\n",
    "VF.to_pil_image(next(iter(ds_all))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a2c63-6100-4c91-9c41-02ad002bc757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_crop = ImagePatchIterableDataset(\n",
    "    ds_all, \n",
    "    shape=SHAPE[-2:],\n",
    "    stride=SHAPE[-1] // 3,\n",
    ")\n",
    "ds_crop = IterableImageFilterDataset(ds_crop, ImageFilter(min_std=0.03)) \n",
    "ds_crop = IterableShuffle(ds_crop, max_shuffle=100_000)\n",
    "\n",
    "for batch, in DataLoader(ds_crop, batch_size=30*30):\n",
    "    print(batch.shape)\n",
    "    #batch = batch.unsqueeze(1)\n",
    "    img = VF.to_pil_image(make_grid(batch, nrow=30))\n",
    "    img = VF.resize(img, (img.height * 2, img.width * 2), VF.InterpolationMode.NEAREST)\n",
    "    break\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba6cac9-ec0a-44c4-b9b1-3139ab5eb242",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for batch, in tqdm(DataLoader(ds_crop, batch_size=100, num_workers=2)):\n",
    "#    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c558ed8-3298-4a02-89c7-4d8a80637fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lib = GreedyLibrary(1, SHAPE, std=.001, mean=0., device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e59028-0391-4c3f-90f2-b6e4a2735809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    lib.entries = create_entries_2d(lib.shape, rotation_steps=8).to(lib.device)\n",
    "    lib.entries -= lib.entries.mean()\n",
    "    lib.entries *= .3\n",
    "    lib.n_entries = lib.entries.shape[0]\n",
    "    lib.hits = [0] * lib.n_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab20ba8c-96e1-4aec-bf8d-7406448f03d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --------- TRAIN ---------\n",
    "\n",
    "try:\n",
    "    count, last_count = 0, 0\n",
    "    max_dist = math.prod(lib.shape) * .21\n",
    "    #max_dist = -.001\n",
    "    \n",
    "    try:\n",
    "        total = len(ds_crop)\n",
    "    except TypeError:\n",
    "        total = None\n",
    "    with tqdm(total=total) as progress:    \n",
    "        for batch, in DataLoader(ds_crop, batch_size=100, shuffle=not isinstance(ds_crop, IterableDataset), num_workers=2):\n",
    "            if batch.ndim == 3:\n",
    "                batch = batch.unsqueeze(1)\n",
    "            lib.fit(batch, lr=1., skip_top_entries=0, zero_mean=True, grow_if_distance_above=max_dist, max_entries=1000)\n",
    "            #lib.fit(batch, lr=.1, skip_top_entries=0, zero_mean=True, grow_if_distance_above=max_dist, max_entries=1000, metric=\"corr\")\n",
    "            #lib.fit(batch, lr=1., skip_top_entries=0, zero_mean=True)\n",
    "            progress.desc = f\"{lib.n_entries} entries\"\n",
    "            progress.update(batch.shape[0])\n",
    "            \n",
    "            count += batch.shape[0]\n",
    "            if count > last_count + 200000:\n",
    "                last_count = count\n",
    "\n",
    "                n_entries = lib.n_entries\n",
    "                lib.drop_entries(hits_lt=2, inplace=True)\n",
    "                if lib.n_entries < 1000:\n",
    "                    print(f\"entries: {lib.n_entries}, dropped {n_entries - lib.n_entries}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "(lib\n",
    " #.drop_entries(hits_lt=10, inplace=True)\n",
    " .sort_entries(inplace=True, reverse=True)\n",
    ")\n",
    "display(lib.plot_entries(min_size=600, sort_by=\"hits\"))\n",
    "print(sorted(lib.hits, reverse=True))\n",
    "display(lib.plot_entries(min_size=600, sort_by=\"tsne\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a640c634-2280-498a-b152-0ba508d21885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lib.n_entries, lib.entries.min(), lib.entries.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855be97-ae71-4ccc-8f68-efb2d7548345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lib.sort_entries(reverse=True).save_torch(f\"../models/greedylib-{lib.shape[0]}x{lib.shape[1]}x{lib.shape[2]}-{lib.n_entries}-photos-6M.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f460ce42-59c1-4a0d-ab39-89eae036ca5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RELOAD\n",
    "\n",
    "lib = GreedyLibrary.from_torch(\"../models/greedylib-1x31x31-388-photos-6M.pt\").cuda()\n",
    "# lib.plot_entries(signed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadbeaf1-33c2-4c14-ada8-325fc9f28b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = next(iter(ds_all))[0]\n",
    "image -= image.mean()\n",
    "VF.to_pil_image(make_grid(F.max_pool2d(lib.convolve(image, stride=SHAPE[-1]), 1).unsqueeze(1)).clamp(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcff79c-b168-43e7-9fa5-e37b2ac21e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d131e81-eb31-4213-a033-63bfde249a47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_labels(image):\n",
    "    c = lib.convolve(image - image.mean(), padding=max(0, lib.shape[-1] - 2), stride=1)\n",
    "    c = F.max_pool2d(c, lib.shape[-2:])\n",
    "    #print(c.shape)\n",
    "    return c.permute(1, 2, 0).argmax(dim=2)\n",
    "    \n",
    "images = []\n",
    "for image, in ds_all:\n",
    "    labels = image_labels(image)\n",
    "    \n",
    "    repro = make_grid([lib.entries[i] for i in labels.flatten()], nrow=labels.shape[-1], normalize=False)\n",
    "    repro = signed_to_image(repro)\n",
    "    s = (min(image.shape[-2], repro.shape[-2]), min(image.shape[-1], repro.shape[-1]))\n",
    "    repro[:, :s[-2], :s[-1]] = repro[:, :s[-2], :s[-1]] * .3 + .7 * image[:, :s[-2], :s[-1]].to(lib.device)\n",
    "    images.append(repro)\n",
    "    \n",
    "    if len(images) >= 8*8:\n",
    "        break\n",
    "VF.to_pil_image(make_grid(images, nrow=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d485fa41-5c28-422a-a93a-91dd6e3459b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = VF.pil_to_tensor(PIL.Image.open(\n",
    "    \"/home/bergi/Pictures/photos/katjacam/101MSDCF/DSC00012.JPG\"\n",
    ")).to(torch.float) / 255.\n",
    "print(image.shape)\n",
    "image = VF.resize(image, (image.shape[-2] // 4, image.shape[-1] // 4))\n",
    "print(image.shape)\n",
    "VF.to_pil_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd0108-8dc4-43c3-991a-2ef6b244332c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = lib.cpu().drop_entries(hits_lt=10).convolve(VT.Grayscale()(image), stride=10)\n",
    "c = F.max_pool2d(c, 3)\n",
    "labels = c.permute(1, 2, 0).argmax(dim=2)\n",
    "repro = make_grid([lib.entries[i] for i in labels.flatten()], nrow=labels.shape[-1], normalize=False)\n",
    "repro = signed_to_image(repro)\n",
    "VF.to_pil_image(repro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8a714-7d3a-444a-b4ba-77750cffb0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54128c22-9eef-420f-b5ae-e7f54b62280d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f9f5a4-735c-4545-9f07-bb11fd332125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = lib.cpu().convolve(VT.Grayscale()(image - image.mean()), stride=3)\n",
    "c = F.max_pool2d(c, 3)\n",
    "c2 = lib.convolve(c[:1])\n",
    "VF.to_pil_image(c2[100:103].clamp(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ab8ec1-3bbf-484f-8eef-d55e60a515eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SHAPE2 = [1, 7, 7]\n",
    "lib2 = GreedyLibrary(1, SHAPE2, std=.001, mean=0., device=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93009e53-1d00-4671-95d8-9cbde373e5cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --------- TRAIN ---------\n",
    "\n",
    "try:\n",
    "    count, last_count = 0, 0\n",
    "    max_dist = math.prod(lib2.shape) * 2.\n",
    "    #max_dist = -1\n",
    "    \n",
    "    print(\"max_dist\", max_dist)\n",
    "    try:\n",
    "        total = len(ds_crop)\n",
    "    except TypeError:\n",
    "        total = None\n",
    "    with tqdm(total=total) as progress:    \n",
    "        for image, in DataLoader(ds_all, batch_size=1, shuffle=not isinstance(ds_crop, IterableDataset), num_workers=2):\n",
    "            image = image[0]\n",
    "            \n",
    "            conv = lib.convolve(image - image.mean(), stride=SHAPE[-1])\n",
    "            for conv_layer in conv:\n",
    "                #print(conv_layer.min(), conv_layer.max())\n",
    "                for patches in iter_image_patches(conv_layer.unsqueeze(0), shape=SHAPE2[-2:], stride=SHAPE2[-1], batch_size=1000):\n",
    "                    ids, distances = lib2.fit(patches, lr=1., skip_top_entries=0, zero_mean=False, grow_if_distance_above=max_dist, max_entries=1000, metric=\"l2\")\n",
    "                    #print(distances.min(), distances.max())\n",
    "                    #lib2.fit(batch, lr=.1, skip_top_entries=0, zero_mean=True, grow_if_distance_above=max_dist, max_entries=1000, metric=\"corr\")\n",
    "                    #lib2.fit(batch, lr=1., skip_top_entries=0, zero_mean=True)\n",
    "                # break\n",
    "                \n",
    "            progress.desc = f\"{lib2.n_entries} entries\"\n",
    "            progress.update(1)\n",
    "            \n",
    "            count += 1\n",
    "            if count > last_count + 100:\n",
    "                last_count = count\n",
    "\n",
    "                n_entries = lib2.n_entries\n",
    "                lib2.drop_entries(hits_lt=2, inplace=True)\n",
    "                if lib2.n_entries < 1000:\n",
    "                    print(f\"entries: {lib2.n_entries}, dropped {n_entries - lib2.n_entries}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "(lib2\n",
    " #.drop_entries(hits_lt=10, inplace=True)\n",
    " .sort_entries(inplace=True, reverse=True)\n",
    ")\n",
    "display(lib2.plot_entries(min_size=600, sort_by=\"hits\", signed=True))\n",
    "print(sorted(lib2.hits, reverse=True))\n",
    "display(lib2.plot_entries(min_size=600, sort_by=\"hits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80445daa-038d-4532-9e30-a066eb87e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(lib2.cpu().drop_entries(hits_lt=100).plot_entries(min_size=600, sort_by=\"hits\", signed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb596ad-47fb-4595-adfe-ae9f4bbbf048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30727fb4-5f06-4346-be38-7e96f3433f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.algo import Space2d\n",
    "\n",
    "def create_entries_2d(\n",
    "        shape: Iterable[int], \n",
    "        rotation_steps: int = 4,\n",
    "):\n",
    "    shape = tuple(shape)\n",
    "    assert len(shape) == 3, f\"Expected shape of ndim=3, got {len(shape)}\"\n",
    "    \n",
    "    entries = None\n",
    "    def _add_entry(entry, zero_mean=True):\n",
    "        nonlocal entries\n",
    "        if zero_mean:\n",
    "            entry = entry.clamp(0, 1)\n",
    "            entry -= entry.mean()\n",
    "            \n",
    "        entry = entry.reshape(1, *shape)\n",
    "                \n",
    "        if entries is None:\n",
    "            entries = entry\n",
    "        else:\n",
    "            dist = (\n",
    "                (entries - entry.repeat(entries.shape[0], *(1 for _ in shape))).abs()\n",
    "                .reshape(entries.shape[0], -1).sum(dim=1)\n",
    "                / math.prod(shape)\n",
    "            )\n",
    "            if not torch.any(dist < 0.001):\n",
    "                entries = torch.concat([entries, entry])\n",
    "            \n",
    "    def _shape(mode, space):\n",
    "        if mode == \"bar\":\n",
    "            return 1. - (space[0] - 0.).abs()\n",
    "        elif mode == \"edge\":\n",
    "            return 1. - (space[0] + 0.5)\n",
    "        elif mode == \"circle\":\n",
    "            return 1. - ((space[0] + 1.) ** 2 + space[1] ** 2).sqrt() * .5\n",
    "    \n",
    "    def _iter_spaces(mode):\n",
    "        for rotation_idx in range(rotation_steps // (2 if mode == \"bar\" else 1)):\n",
    "            rotation = np.pi * 2. * rotation_idx / rotation_steps\n",
    "            yield Space2d((2, *shape[1:]), rotate_2d=rotation).space()\n",
    "    \n",
    "    for i in torch.linspace(-1, 1, 8):\n",
    "        _add_entry(torch.ones(shape) * i, zero_mean=False)\n",
    "\n",
    "    for mode1 in (\"edge\", \"circle\", \"bar\"):        \n",
    "        for space in _iter_spaces(mode1):\n",
    "            entry = _shape(mode1, space)\n",
    "            _add_entry(entry)\n",
    "        \n",
    "    for mode1 in (\"edge\", \"circle\", \"bar\"):        \n",
    "        for space in _iter_spaces(mode1):\n",
    "            entry = _shape(mode1, space)\n",
    "\n",
    "            for mode2 in (\"edge\", \"circle\", \"bar\"):\n",
    "                for space2 in _iter_spaces(mode2):\n",
    "                    entry2 = _shape(mode2, space2)\n",
    "                    \n",
    "                    for mix_mode in (\"min\", \"max\", \"mix\"):\n",
    "                        if mix_mode == \"min\":\n",
    "                            mixed_entry = torch.maximum(entry, entry2)\n",
    "                        elif mix_mode == \"max\":\n",
    "                            mixed_entry = torch.minimum(entry, entry2)\n",
    "                        elif mix_mode == \"mix\":\n",
    "                            mixed_entry = (entry + entry2) * .5\n",
    "\n",
    "                        _add_entry(mixed_entry)\n",
    "                    \n",
    "        return entries \n",
    "        \n",
    "VF.to_pil_image(signed_to_image(make_grid(create_entries_2d((1, 31, 31), rotation_steps=8), nrow=32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bdd3cf-3fbe-4a19-aecb-889def4cf926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VF.to_pil_image(signed_to_image(make_grid(create_entries_2d((1, 31, 31), rotation_steps=8), nrow=32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9059e5-79c4-46d4-bf6e-9603dd2a4245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a2d49b-a68d-473a-bc91-2694b6c4acb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0796d5-1cd8-4ca6-973c-fbdf4fe66b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e3878-b5d7-4f48-b27b-6a0f8b98274c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04392ffa-0bfa-4340-8914-a423c0bc8407",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LocalGreedyLibrary:\n",
    "    \"\"\"\n",
    "    Collection of library patches of ndim=>1.\n",
    "\n",
    "    Learns by adjusting the best matching patch to match the input patch\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_entries: int,\n",
    "            shape: Iterable[int],\n",
    "            mean: float = 0.,\n",
    "            std: float = 0.01,\n",
    "            device: Union[None, str, torch.device] = \"cpu\",\n",
    "    ):\n",
    "        self.device = to_torch_device(device)\n",
    "        self.shape = tuple(shape)\n",
    "        self.n_entries = n_entries\n",
    "        self.entries = mean + std * torch.randn(n_entries, *self.shape).to(self.device)\n",
    "        self.hits = [0] * n_entries\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({self.n_entries}, {self.shape})\"\n",
    "\n",
    "    def __copy__(self):\n",
    "        return self.copy()\n",
    "\n",
    "    @property\n",
    "    def ndim(self) -> int:\n",
    "        return len(self.shape)\n",
    "\n",
    "    @property\n",
    "    def max_hits(self) -> int:\n",
    "        \"\"\"Maximum number of hits of all entries\"\"\"\n",
    "        return max(*self.hits) if self.hits else 0\n",
    "\n",
    "    def copy(self) -> \"GreedyLibrary\":\n",
    "        d = self.__class__(0, self.shape, device=self.device)\n",
    "        d.n_entries = self.n_entries\n",
    "        d.entries = deepcopy(self.entries)\n",
    "        d.hits = self.hits.copy()\n",
    "        return d\n",
    "\n",
    "    def to(self, device: Union[str, torch.device], inplace: bool = False) -> \"GreedyLibrary\":\n",
    "        if not inplace:\n",
    "            lib = self.copy()\n",
    "            return lib.to(device, inplace=True)\n",
    "\n",
    "        self.device = to_torch_device(device)\n",
    "        self.entries = self.entries.to(self.device)\n",
    "        return self\n",
    "\n",
    "    def cpu(self, inplace: bool = False) -> \"GreedyLibrary\":\n",
    "        return self.to(\"cpu\", inplace=inplace)\n",
    "\n",
    "    def cuda(self, inplace: bool = False) -> \"GreedyLibrary\":\n",
    "        return self.to(\"cuda\", inplace=inplace)\n",
    "\n",
    "    def save_torch(self, f: torch.serialization.FILE_LIKE, **kwargs):\n",
    "        torch.save(self._save_data(), f, **kwargs)\n",
    "\n",
    "    def load_torch(self, f: torch.serialization.FILE_LIKE, **kwargs):\n",
    "        data = torch.load(f, **kwargs)\n",
    "        self._load_data(data)\n",
    "\n",
    "    @classmethod\n",
    "    def from_torch(cls, f: torch.serialization.FILE_LIKE, device: Union[None, str, torch.device] = \"cpu\") -> \"GreedyLibrary\":\n",
    "        lib = cls(n_entries=0, shape=tuple(), device=device)\n",
    "        lib.load_torch(f)\n",
    "        return lib\n",
    "\n",
    "    def _save_data(self) -> dict:\n",
    "        return {\n",
    "            \"entries\": self.entries,\n",
    "            \"hits\": self.hits,\n",
    "        }\n",
    "\n",
    "    def _load_data(self, data: dict):\n",
    "        self.entries = data[\"entries\"]\n",
    "        self.hits = data[\"hits\"]\n",
    "        self.n_entries = len(self.hits)\n",
    "        self.shape = tuple(self.entries.shape[1:])\n",
    "        self.to(self.device)\n",
    "\n",
    "    def top_entry_index(self) -> Optional[int]:\n",
    "        \"\"\"Returns index of entry with most hits\"\"\"\n",
    "        top_idx, top_hits = None, None\n",
    "        for i, hits in enumerate(self.hits):\n",
    "            if top_idx is None or hits > top_hits:\n",
    "                top_idx, top_hits = i, hits\n",
    "        return top_idx\n",
    "\n",
    "    def entry_ranks(self, reverse: bool = False) -> List[int]:\n",
    "        \"\"\"\n",
    "        Returns a list of ranks for each entry,\n",
    "        where rank means the index sorted by number of hits.\n",
    "        \"\"\"\n",
    "        entry_ids = list(range(self.n_entries))\n",
    "        entry_ids.sort(key=lambda i: self.hits[i], reverse=reverse)\n",
    "        return [entry_ids.index(i) for i in range(self.n_entries)]\n",
    "\n",
    "    def entry_hits(self, reverse: bool = False) -> Dict[int, int]:\n",
    "        \"\"\"\n",
    "        Returns a dict of `entry-index` -> `number-of-hits`.\n",
    "\n",
    "        Sorted by number of hits.\n",
    "        \"\"\"\n",
    "        entry_ids = list(range(self.n_entries))\n",
    "        entry_ids.sort(key=lambda i: self.hits[i], reverse=reverse)\n",
    "        return {\n",
    "            i: self.hits[i]\n",
    "            for i in entry_ids\n",
    "        }\n",
    "\n",
    "    def sort_entries(\n",
    "            self,\n",
    "            by: str = \"hits\",\n",
    "            reverse: bool = False,\n",
    "            inplace: bool = False,\n",
    "    ):\n",
    "        if not inplace:\n",
    "            lib = self.copy()\n",
    "            return lib.sort_entries(by=by, reverse=reverse, inplace=True)\n",
    "\n",
    "        sorted_ids = self.sorted_entry_indices(by=by, reverse=reverse)\n",
    "        self.entries = self.entries[sorted_ids]\n",
    "        self.hits = [self.hits[sorted_ids[i]] for i in range(len(self.hits))]\n",
    "        return self\n",
    "\n",
    "    def sorted_entry_indices(\n",
    "            self,\n",
    "            by: str = \"hits\",\n",
    "            reverse: bool = False,\n",
    "    ) -> List[int]:\n",
    "        if not self.n_entries:\n",
    "            return []\n",
    "        entry_ids = list(range(self.n_entries))\n",
    "        if self.n_entries < 2:\n",
    "            return entry_ids\n",
    "\n",
    "        if by == \"hits\":\n",
    "            entry_ids.sort(key=lambda i: self.hits[i], reverse=reverse)\n",
    "\n",
    "        elif by == \"tsne\":\n",
    "            from sklearn.manifold import TSNE\n",
    "            tsne = TSNE(1, perplexity=min(30, self.n_entries - 1))\n",
    "            positions = tsne.fit_transform(self.entries.reshape(self.entries.shape[0], -1).cpu().numpy())\n",
    "            entry_ids.sort(key=lambda i: positions[i], reverse=reverse)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported sort by '{by}'\")\n",
    "\n",
    "        return entry_ids\n",
    "\n",
    "    def fit(\n",
    "            self,\n",
    "            batch: torch.Tensor,\n",
    "            lr: float = 1.,\n",
    "            zero_mean: bool = False,\n",
    "            skip_top_entries: Union[bool, int] = False,\n",
    "            grow_if_distance_above: Optional[float] = None,\n",
    "            max_entries: int = 1000,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Partially fit a batch of patches.\n",
    "\n",
    "        :param batch: Tensor of N patches of shape matching the library's shape\n",
    "        :param lr: learning rate, range [0, 1]\n",
    "        :param zero_mean: True to subtract the mean from each patch in the batch\n",
    "        :param skip_top_entries: bool or int,\n",
    "            Do not match the top N entries (1 for True), sorted by number of hits\n",
    "        \"\"\"\n",
    "        batch = batch.to(self.device)\n",
    "\n",
    "        if zero_mean:\n",
    "            batch_mean = batch\n",
    "            for i in range(self.ndim):\n",
    "                batch_mean = batch_mean.mean(dim=i+1, keepdim=True)\n",
    "            batch = batch - batch_mean\n",
    "\n",
    "        best_entry_ids, distances = self.best_entries_for(batch, skip_top_entries=skip_top_entries)\n",
    "        print(best_entry_ids, distances)\n",
    "        for i in range(batch.shape[0]):\n",
    "            entry_id = best_entry_ids[i]\n",
    "\n",
    "            if grow_if_distance_above is not None:\n",
    "                if distances[i] > grow_if_distance_above:\n",
    "                    self.entries = torch.concat(\n",
    "                        self.entries,\n",
    "                        torch.randn(1, *self.shape).to(self.device) * 0.001 + self.entries.mean()\n",
    "                    )\n",
    "                    self.hits.append(0)\n",
    "                    entry_id = self.n_entries\n",
    "                    self.n_entries += 1\n",
    "\n",
    "            weight = 1. / (1 + self.hits[entry_id])\n",
    "            self.entries[entry_id] += lr * weight * (batch[i] - self.entries[entry_id])\n",
    "            self.hits[entry_id] += 1\n",
    "\n",
    "    def convolve(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "            stride: Union[int, Iterable[int]] = 1,\n",
    "            padding: Union[int, Iterable[int]] = 0,\n",
    "    ) -> torch.Tensor:\n",
    "        func = getattr(F, f\"conv{self.ndim - 1}d\", None)\n",
    "        if not callable(func):\n",
    "            raise NotImplementedError(f\"{self.ndim - 1}-d convolution not supported\")\n",
    "\n",
    "        return func(x.to(self.device), self.entries, stride=stride, padding=padding)\n",
    "\n",
    "    def best_entries_for(\n",
    "            self,\n",
    "            batch: torch.Tensor,\n",
    "            skip_top_entries: Union[bool, int] = False,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns the index of the best matching entry for each patch in the batch.\n",
    "\n",
    "        :param batch: Tensor of N patches of shape matching the library's shape\n",
    "        :param skip_top_entries: bool or int,\n",
    "            Do not match the top N entries (1 for True), sorted by number of hits\n",
    "        :return: tuple of\n",
    "            - Tensor of int64: entry indices\n",
    "            - Tensor of float: distances\n",
    "        \"\"\"\n",
    "        assert batch.ndim == len(self.shape) + 1, f\"Got {batch.shape}\"\n",
    "        assert batch.shape[1:] == self.shape, f\"Got {batch.shape}\"\n",
    "        ones = tuple(1 for _ in self.shape)\n",
    "\n",
    "        repeated_entries = self.entries.repeat(batch.shape[0], *ones)\n",
    "        repeated_batch = batch.to(self.device).repeat(1, self.n_entries, *ones[1:]).reshape(-1, *self.shape)\n",
    "\n",
    "        dist = (repeated_entries - repeated_batch).abs()\n",
    "\n",
    "        # TODO: correlation, about like: dist = (repeated_entries * repeated_batch).sum(??)\n",
    "\n",
    "        while dist.ndim > 1:\n",
    "            dist = dist.sum(1)\n",
    "        dist = dist.reshape(batch.shape[0], -1)\n",
    "        if not skip_top_entries:\n",
    "            indices = torch.argmin(dist, 1)\n",
    "            return indices, dist.flatten()[indices + torch.linspace(0, indices.shape[0] - 1, indices.shape[0]).to(torch.int64) * l.n_entries]\n",
    "\n",
    "        skip_top_entries = int(skip_top_entries)\n",
    "        sorted_indices = torch.argsort(dist, 1)\n",
    "        entry_ranks = self.entry_ranks(reverse=True)\n",
    "        best_entries = []\n",
    "        for indices in sorted_indices:\n",
    "            idx = 0\n",
    "            while idx + 1 < len(indices) and entry_ranks[indices[idx]] < skip_top_entries:\n",
    "                idx += 1\n",
    "            best_entries.append(indices[idx])\n",
    "        print(best_entries)\n",
    "        indices = torch.Tensor(best_entries).to(torch.int64)\n",
    "        return indices, dist.flatten()[indices + torch.linspace(0, indices.shape[0] - 1, indices.shape[0]).to(torch.int64) * l.n_entries]\n",
    "\n",
    "    def drop_unused(self, inplace: bool = False) -> \"GreedyLibrary\":\n",
    "        return self.drop_entries(hits_lt=1, inplace=inplace)\n",
    "\n",
    "    def drop_entries(\n",
    "            self,\n",
    "            hits_lt: Optional[int] = None,\n",
    "            inplace: bool = False,\n",
    "    ) -> \"GreedyLibrary\":\n",
    "        if not inplace:\n",
    "            lib = self.copy()\n",
    "            return lib.drop_entries(\n",
    "                hits_lt=hits_lt,\n",
    "                inplace=True,\n",
    "            )\n",
    "\n",
    "        drop_idx = set()\n",
    "        if hits_lt is not None:\n",
    "            for i, hits in enumerate(self.hits):\n",
    "                if hits <= hits_lt:\n",
    "                    drop_idx.add(i)\n",
    "\n",
    "        if drop_idx:\n",
    "            entries = []\n",
    "            hits = []\n",
    "            for i, (entry, h) in enumerate(zip(self.entries, self.hits)):\n",
    "                if i not in drop_idx:\n",
    "                    entries.append(entry.unsqueeze(0))\n",
    "                    hits.append(h)\n",
    "            self.entries = torch.concat(entries) if entries else torch.Tensor()\n",
    "            self.hits = hits\n",
    "            self.n_entries = len(self.hits)\n",
    "            self.to(self.device, inplace=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def plot_entries(\n",
    "            self,\n",
    "            min_size: int = 300,\n",
    "            with_hits: bool = True,\n",
    "            sort_by: Optional[str] = None,\n",
    "    ) -> PIL.Image.Image:\n",
    "        if len(self.shape) == 1:\n",
    "            entries = self.entries.reshape(-1, 1, 1, *self.shape)\n",
    "        elif len(self.shape) == 2:\n",
    "            entries = self.entries.reshape(-1, 1, *self.shape)\n",
    "        elif len(self.shape) == 3:\n",
    "            entries = self.entries\n",
    "        else:\n",
    "            raise RuntimeError(f\"Can't plot entries with shape {self.shape} (ndim>3)\")\n",
    "        if entries.shape[0]:\n",
    "\n",
    "            e_min, e_max = entries.min(), entries.max()\n",
    "            if e_min != e_max:\n",
    "                entries = (entries - e_min) / (e_max - e_min)\n",
    "\n",
    "            if with_hits:\n",
    "                max_hits = max(1, self.max_hits)\n",
    "                entry_list = []\n",
    "                for entry, hits in zip(entries, self.hits):\n",
    "                    if entry.shape[0] == 1:\n",
    "                        entry = entry.repeat(3, *(1 for _ in entry.shape[1:]))\n",
    "                    elif entry.shape[0] == 3:\n",
    "                        pass\n",
    "                    else:\n",
    "                        raise ValueError(f\"Can't plot entries with {entry.shape[0]} channels\")\n",
    "\n",
    "                    background = torch.Tensor([0, hits / max_hits, 0])\n",
    "                    background = background.reshape(3, *((1,) * (len(entry.shape) - 1)))\n",
    "                    background = background.repeat(1, *(s + 2 for s in entry.shape[1:]))\n",
    "                    background[:, 1:-1, 1:-1] = entry\n",
    "                    entry_list.append(background)\n",
    "                entries = entry_list\n",
    "\n",
    "            if sort_by:\n",
    "                if not isinstance(entries, list):\n",
    "                    entries = list(entries)\n",
    "                entry_ids = self.sorted_entry_indices(by=sort_by, reverse=True)\n",
    "                entries = [entries[i] for i in entry_ids]\n",
    "\n",
    "            grid = make_grid(entries, nrow=max(1, int(np.sqrt(self.n_entries))), normalize=False)\n",
    "            if grid.shape[-1] < min_size:\n",
    "                grid = VF.resize(\n",
    "                    grid,\n",
    "                    [\n",
    "                        int(grid.shape[-2] * min_size / grid.shape[-1]),\n",
    "                        min_size,\n",
    "                    ],\n",
    "                    VF.InterpolationMode.NEAREST\n",
    "                )\n",
    "        else:\n",
    "            grid = torch.zeros(1, min_size, min_size)\n",
    "        return VF.to_pil_image(grid.cpu())\n",
    "\n",
    "l = LocalGreedyLibrary(3, (1,))\n",
    "l.entries = torch.Tensor([[1], [2], [3]])\n",
    "l.fit(torch.Tensor([[0], [1.7], [3.1], [3.2]]))\n",
    "l.fit(torch.Tensor([[0], [1.7], [3.1], [3.2]]), skip_top_entries=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a72575-b1be-4010-96ba-6b22a82e056b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices = torch.Tensor([0, 1, 2, 2]).to(torch.int64)\n",
    "dist = torch.Tensor(\n",
    "       [[1.0000, 2.0000, 3.0000],\n",
    "        [0.7000, 0.3000, 1.3000],\n",
    "        [2.1000, 1.1000, 0.1000],\n",
    "        [2.2000, 1.2000, 0.2000]])\n",
    "\n",
    "dist.flatten()[indices + torch.linspace(0, 3, 4).to(torch.int64) * l.n_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b623a23-c3f7-49b3-9c00-67b93eb6a78f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
