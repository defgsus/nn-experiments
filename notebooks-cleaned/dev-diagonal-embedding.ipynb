{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a98f45b-328b-4714-9729-5e875d5a2ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50398913-9148-4579-bdae-95544b7d2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagonalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            channels_in: int,\n",
    "            channels_out: int,\n",
    "            diagonal: bool = True,\n",
    "            symmetric: bool = True,\n",
    "            fft: bool = False,\n",
    "            fft_concat_dim: int = -1,\n",
    "            requires_grad: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Wrapper around torch.nn.Embedding\n",
    "\n",
    "        :param channels_in: int, vocabulary size\n",
    "        :param channels_out: int, internal representation size\n",
    "        :param diagonal: bool, if True, the embedding weights are initialized with\n",
    "            a diagonal matrix, e.g. if channels_in==channels_out, the representation\n",
    "            matches the input\n",
    "        :param symmetric: bool, if True, the embedding weights and readout weights are shared.\n",
    "            If False, the readout has its own set of weights.\n",
    "        :param fft: bool, If True, the representation is the concatenation of the\n",
    "            real and imaginary FFT transform\n",
    "        :param fft_concat_dim: int, either -1 or -2,\n",
    "            if -1, fft real and imaginary output is concatenated along the sequence dimension\n",
    "            if -2, it's concatenated along the channel dimensions and `channels_out` is divided by 2!\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._diagonal = diagonal\n",
    "        self._symmetric = symmetric\n",
    "        self._fft = fft\n",
    "        self._fft_concat_dim = fft_concat_dim\n",
    "\n",
    "        if fft:\n",
    "            assert sum(int(x) for x in list(bin(channels_out)[2:])) == 1, \\\n",
    "                f\"`channels_in` must be a power of 2 when using `fft`\"\n",
    "            assert fft_concat_dim in (-1, -2), f\"`fft_concat_dim` must be -1 or -2, got '{fft_concat_dim}'\"\n",
    "            if fft_concat_dim == -2:\n",
    "                channels_out //= 2\n",
    "\n",
    "        self.input = nn.Embedding(channels_in, channels_out)\n",
    "        with torch.no_grad():\n",
    "            if diagonal:\n",
    "                self.input.weight[:] = create_diagonal_matrix(self.input.weight.shape)\n",
    "            elif fft:\n",
    "                self.input.weight[:] = F.softmax(self.input.weight, dim=-1)\n",
    "            if not requires_grad:\n",
    "                self.input.weight.requires_grad = False\n",
    "    \n",
    "            if not symmetric:\n",
    "                self.output = nn.Linear(channels_out, channels_in)\n",
    "                self.output.weight[:] = self.input.weight.T\n",
    "                self.output.bias[:] = 0.\n",
    "                if not requires_grad:\n",
    "                    self.output.weight.requires_grad = False\n",
    "                    self.output.bias.requires_grad = False\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return (\n",
    "            f\"diagonal={self._diagonal}, symmetric={self._symmetric}\"\n",
    "            f\", fft={self._fft}, fft_concat_dim={self._fft_concat_dim}\"\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "            reverse: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Converts token indices to representation or representation to token class logits\n",
    "\n",
    "        :param x: torch.Tensor,\n",
    "            if reverse==False, the token indices of shape [B, L] (where L is sequence length),\n",
    "            if reverse==True and fft==False, the representation of shape [B, C, L]\n",
    "            if reverse==True and fft==True, the representation\n",
    "                of shape [B, C, L] if `fft_concat_dim==-2` or [B, C, L*2] if `fft_concat_dim==-1`\n",
    "        :param reverse: bool, if True, reverses the embedding\n",
    "        :return: torch.Tensor,\n",
    "            if reverse==False and fft==False, the representation of shape [B, C, L]\n",
    "            if reverse==False and fft==True, the representation\n",
    "                of shape [B, C, L] if `fft_concat_dim==-2` or [B, C, L*2] if `fft_concat_dim==-1`\n",
    "            if reverse==True, the token class logits of shape [B, L, V] (where V is vocab_size)\n",
    "        \"\"\"\n",
    "        if not reverse:\n",
    "\n",
    "            outp = self.input(x).permute(0, 2, 1)\n",
    "            if self._fft:\n",
    "                outp = torch.fft.fft(outp, dim=-2)\n",
    "                outp = torch.concat([outp.real, outp.imag], dim=self._fft_concat_dim)\n",
    "            return outp\n",
    "\n",
    "        else:\n",
    "            if self._fft:\n",
    "                x = torch.complex(*torch.split(x, x.shape[self._fft_concat_dim] // 2, dim=self._fft_concat_dim))\n",
    "                x = torch.fft.ifft(x, dim=-2).real\n",
    "\n",
    "            if self._symmetric:\n",
    "                return (self.input.weight @ x).permute(0, 2, 1).contiguous()\n",
    "            else:\n",
    "                return self.output(x.permute(0, 2, 1))\n",
    "\n",
    "\n",
    "emb = DiagonalEmbedding(16, 16, symmetric=False, fft=True)\n",
    "inp = torch.randint(0, 9, (2, 5), generator=torch.Generator().manual_seed(23))\n",
    "outp = emb(inp)\n",
    "display(inp)\n",
    "print(\"output:\")\n",
    "print(outp.shape)\n",
    "display(outp)\n",
    "print(\"reverse:\")\n",
    "inp2 = emb(outp, reverse=True)\n",
    "display(inp2.argmax(dim=-1))\n",
    "display(inp2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a67416c-c878-45cc-a8a2-fc5cbeb079f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d401d863-4a65-4195-b370-9de0b3f11134",
   "metadata": {},
   "source": [
    "# try without weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6239b-c23a-4f12-8e62-f88cefe4c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagonalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            channels_in: int,\n",
    "            channels_out: int,\n",
    "            diagonal: bool = True,\n",
    "            symmetric: bool = True,\n",
    "            no_weights: bool = False,\n",
    "            fft: bool = False,\n",
    "            fft_concat_dim: int = -1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Wrapper around torch.nn.Embedding\n",
    "\n",
    "        :param channels_in: int, vocabulary size\n",
    "        :param channels_out: int, internal representation size\n",
    "        :param diagonal: bool, if True, the embedding weights are initialized with\n",
    "            a diagonal matrix, e.g. if channels_in==channels_out, the representation\n",
    "            matches the input\n",
    "        :param symmetric: bool, if True, the embedding weights and readout weights are shared.\n",
    "            If False, the readout has its own set of weights.\n",
    "        :param fft: bool, If True, the representation is the concatenation of the\n",
    "            real and imaginary FFT transform\n",
    "        :param fft_concat_dim: int, either -1 or -2,\n",
    "            if -1, fft real and imaginary output is concatenated along the sequence dimension\n",
    "            if -2, it's concatenated along the channel dimensions and `channels_out` is divided by 2!\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._diagonal = diagonal\n",
    "        self._symmetric = symmetric\n",
    "        self._no_weights = no_weights\n",
    "        self._fft = fft\n",
    "        self._fft_concat_dim = fft_concat_dim\n",
    "\n",
    "        if fft:\n",
    "            assert sum(int(x) for x in list(bin(channels_out)[2:])) == 1, \\\n",
    "                f\"`channels_in` must be a power of 2 when using `fft`\"\n",
    "            assert fft_concat_dim in (-1, -2), f\"`fft_concat_dim` must be -1 or -2, got '{fft_concat_dim}'\"\n",
    "            if fft_concat_dim == -2:\n",
    "                channels_out //= 2\n",
    "\n",
    "        \n",
    "        if self._no_weights:\n",
    "            assert channels_in == channels_out, \\\n",
    "                f\"In and out channels must match when no_weights is True, got {channels_in} and {channels_out}\"\n",
    "            self.input = None\n",
    "            self._channels = channels_in\n",
    "        else:\n",
    "            self.input = nn.Embedding(channels_in, channels_out)\n",
    "            with torch.no_grad():\n",
    "                if diagonal:\n",
    "                    self.input.weight[:] = create_diagonal_matrix(self.input.weight.shape)\n",
    "                elif fft:\n",
    "                    self.input.weight[:] = F.softmax(self.input.weight, dim=-1)\n",
    "\n",
    "        if not symmetric:\n",
    "            self.output = nn.Linear(channels_out, channels_in)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return (\n",
    "            f\"diagonal={self._diagonal}, symmetric={self._symmetric}, no_weights={self._no_weights}\"\n",
    "            f\", fft={self._fft}, fft_concat_dim={self._fft_concat_dim}\"\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor,\n",
    "            reverse: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Converts token indices to representation or representation to token class logits\n",
    "\n",
    "        :param x: torch.Tensor,\n",
    "            if reverse==False, the token indices of shape [B, L] (where L is sequence length),\n",
    "            if reverse==True and fft==False, the representation of shape [B, C, L]\n",
    "            if reverse==True and fft==True, the representation\n",
    "                of shape [B, C, L] if `fft_concat_dim==-2` or [B, C, L*2] if `fft_concat_dim==-1`\n",
    "        :param reverse: bool, if True, reverses the embedding\n",
    "        :return: torch.Tensor,\n",
    "            if reverse==False and fft==False, the representation of shape [B, C, L]\n",
    "            if reverse==False and fft==True, the representation\n",
    "                of shape [B, C, L] if `fft_concat_dim==-2` or [B, C, L*2] if `fft_concat_dim==-1`\n",
    "            if reverse==True, the token class logits of shape [B, L, V] (where V is vocab_size)\n",
    "        \"\"\"\n",
    "        if not reverse:\n",
    "\n",
    "            if self.input is not None:\n",
    "                outp = self.input(x).permute(0, 2, 1)\n",
    "            else:\n",
    "                # TODO: NOT RIGHT\n",
    "                B, L = x.shape\n",
    "                outp = torch.zeros(B, self._channels, L)\n",
    "                if x.dtype != torch.int64:\n",
    "                    x = x.to(torch.int64)\n",
    "                print(x.shape, outp.shape)\n",
    "                outp = torch.cat([\n",
    "                    outp[..., i].scatter(-1, x[..., i, None], 1)\n",
    "                    for i in range(L)\n",
    "                ]).reshape(B, -1, L)#.permute(0, 2, 1)\n",
    "        \n",
    "            if self._fft:\n",
    "                outp = torch.fft.fft(outp, dim=-2)\n",
    "                outp = torch.concat([outp.real, outp.imag], dim=self._fft_concat_dim)\n",
    "            return outp\n",
    "\n",
    "        else:\n",
    "            if self._fft:\n",
    "                x = torch.complex(*torch.split(x, x.shape[self._fft_concat_dim] // 2, dim=self._fft_concat_dim))\n",
    "                x = torch.fft.ifft(x, dim=-2).real\n",
    "\n",
    "            if self._symmetric:\n",
    "                return (self.input.weight @ x).permute(0, 2, 1).contiguous()\n",
    "            else:\n",
    "                return self.output(x.permute(0, 2, 1))\n",
    "\n",
    "emb = DiagonalEmbedding(10, 10, no_weights=False)\n",
    "inp = torch.randint(0, 9, (2, 5), generator=torch.Generator().manual_seed(23))\n",
    "outp = emb(inp)\n",
    "display(inp)\n",
    "print(outp.shape)\n",
    "display(outp)\n",
    "emb = DiagonalEmbedding(10, 10, no_weights=True)\n",
    "outp2 = emb(inp)\n",
    "print(outp2.shape)\n",
    "display(outp2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf20615-ef7c-435b-8537-4c92df2cb93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90bce1-2aeb-4e9b-9e69-6f12178fa320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0560832-f951-4139-8048-276949366a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf8d186-f70a-4b5e-ad4a-ceb602936c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
