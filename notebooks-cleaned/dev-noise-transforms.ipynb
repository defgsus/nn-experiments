{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19898e36-3fea-48c1-9a82-eb80dc94648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import *\n",
    "from src.util import *\n",
    "from src.algo import *\n",
    "from src.models.decoder import *\n",
    "from src.models.transform import *\n",
    "from src.models.util import *\n",
    "from experiments import datasets\n",
    "\n",
    "def resize(img, scale: float, mode: VF.InterpolationMode = VF.InterpolationMode.NEAREST):\n",
    "    return VF.resize(img, [max(1, int(s * scale)) for s in img.shape[-2:]], mode, antialias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8992296a-fcf3-4f42-8234-d20c1a644c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.rpg_tile_dataset_3x32x32(shape=(3, 64, 64))\n",
    "#dataset = datasets.fmnist_dataset(train=True)\n",
    "dataset = datasets.kali_patch_dataset((3, 64, 64))\n",
    "\n",
    "patches = next(iter(DataLoader(dataset, batch_size=64)))\n",
    "print(patches.shape)\n",
    "VF.to_pil_image(resize(make_grid(patches), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a2f6d-64bf-492c-9181-c69d5f959465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883685e2-5bf7-4617-bd55-1ed90f346787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCropHalfImage(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            prob: float = 1.,\n",
    "            null_value: float = 0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.prob = prob\n",
    "        self.null_value = null_value\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        if input.ndim == 3:\n",
    "            return self._crop_half(input)\n",
    "        elif input.ndim == 4:\n",
    "            return torch.concat([\n",
    "                self._crop_half(img).unsqueeze(0)\n",
    "                for img in input\n",
    "            ])\n",
    "        else:\n",
    "            raise ValueError(f\"input must have 3 or 4 dimensions, got {input}\")\n",
    "\n",
    "    def _crop_half(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        if random.uniform(0, 1) >= self.prob:\n",
    "            return image\n",
    "            \n",
    "        lrtb = random.randrange(4)\n",
    "        if lrtb == 0:\n",
    "            slices = slice(None, None), slice(None, image.shape[-1] // 2) \n",
    "        elif lrtb == 1:\n",
    "            slices = slice(None, None), slice(image.shape[-1] // 2, None) \n",
    "        elif lrtb == 2:\n",
    "            slices = slice(None, image.shape[-2] // 2), slice(None, None)\n",
    "        else:\n",
    "            slices = slice(image.shape[-2] // 2, None), slice(None, None) \n",
    "        \n",
    "        new_image = image + 0\n",
    "        new_image[:, slices[0], slices[1]] = self.null_value\n",
    "        return new_image\n",
    "        \n",
    "\n",
    "noise_patches = RandomCropHalfImage(prob=1.)(patches)\n",
    "VF.to_pil_image(resize(make_grid(noise_patches), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d33c2a-e891-42f1-8f60-16e0bcd5b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNoise(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            amt_min: float = .01,\n",
    "            amt_max: float = .15,\n",
    "            amt_power: float = 2.,\n",
    "            grayscale_prob: float = .1,\n",
    "            prob: float = 1.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.amt_min = amt_min\n",
    "        self.amt_max = amt_max\n",
    "        self.amt_power = amt_power\n",
    "        self.grayscale_prob = grayscale_prob\n",
    "        self.prob = prob\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        if input.ndim == 3:\n",
    "            return self._noise(input)\n",
    "        elif input.ndim == 4:\n",
    "            return torch.concat([\n",
    "                self._noise(img).unsqueeze(0)\n",
    "                for img in input\n",
    "            ])\n",
    "        else:\n",
    "            raise ValueError(f\"input must have 3 or 4 dimensions, got {input}\")\n",
    "\n",
    "    def _noise(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        if random.uniform(0, 1) >= self.prob:\n",
    "            return image\n",
    "            \n",
    "        amt = math.pow(random.uniform(0, 1), self.amt_power)\n",
    "        amt = self.amt_min + (self.amt_max - self.amt_min) * amt\n",
    "\n",
    "        if random.uniform(0, 1) < self.grayscale_prob:\n",
    "            noise = torch.randn_like(image[..., :1, :, :]).repeat(\n",
    "                *(1 for _ in range(image.ndim - 3)),\n",
    "                image.shape[-3], 1, 1\n",
    "            )\n",
    "        else:\n",
    "            noise = torch.randn_like(image)\n",
    "\n",
    "        return (image + amt * noise).clamp(0, 1)\n",
    "\n",
    "\n",
    "noise_patches = ImageNoise(amt_min=.1, amt_max=.1, prob=1., grayscale_prob=.5)(patches)\n",
    "VF.to_pil_image(resize(make_grid(noise_patches), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa5c08-7816-431c-892e-421e3c532ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageMultiNoise(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            amt_min: float = .01,\n",
    "            amt_max: float = .15,\n",
    "            amt_power: float = 2.,\n",
    "            blur_sigma_min: float = 0.,\n",
    "            blur_sigma_max: float = 1.,\n",
    "            prob: float = 1.,\n",
    "            channel_modes: Optional[List[str]] = None,\n",
    "            distribution_modes: Optional[List[str]] = None,\n",
    "    ):\n",
    "        super().__init__()    \n",
    "        self.amt_min = amt_min\n",
    "        self.amt_max = amt_max\n",
    "        self.amt_power = amt_power\n",
    "        self.blur_sigma_min = blur_sigma_min\n",
    "        self.blur_sigma_max = blur_sigma_max\n",
    "        self.prob = prob\n",
    "        if channel_modes is None:\n",
    "            channel_modes = [\"white\", \"color\"]\n",
    "        self.channel_modes = channel_modes\n",
    "        if distribution_modes is None:\n",
    "            distribution_modes = [\"gauss\", \"positive\", \"negative\", \"positive-negative\"]\n",
    "        self.distribution_modes = distribution_modes\n",
    "        \n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        if input.ndim == 3:\n",
    "            return self._noise(input)\n",
    "        elif input.ndim == 4:\n",
    "            return torch.concat([\n",
    "                self._noise(img).unsqueeze(0)\n",
    "                for img in input\n",
    "            ])\n",
    "        else:\n",
    "            raise ValueError(f\"input must have 3 or 4 dimensions, got {input}\")\n",
    "\n",
    "    def _noise(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        if random.uniform(0, 1) >= self.prob:\n",
    "            return image\n",
    "            \n",
    "        amt = math.pow(random.uniform(0, 1), self.amt_power)\n",
    "        amt = self.amt_min + (self.amt_max - self.amt_min) * amt\n",
    "\n",
    "        channel_mode = random.choice(self.channel_modes)\n",
    "        distribution_mode = random.choice(self.distribution_modes)\n",
    "\n",
    "        if distribution_mode == \"gauss\":\n",
    "            rand_func = torch.randn_like\n",
    "        elif distribution_mode in (\"positive\", \"negative\"):\n",
    "            rand_func = torch.rand_like\n",
    "        elif distribution_mode == \"positive-negative\":\n",
    "            rand_func = lambda x: torch.rand_like(x) - torch.rand_like(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown distribution_mode '{distribution_mode}'\")\n",
    "            \n",
    "        if channel_mode == \"white\":\n",
    "            noise = rand_func(image[..., :1, :, :]).repeat(\n",
    "                *(1 for _ in range(image.ndim - 3)),\n",
    "                image.shape[-3], 1, 1\n",
    "            )\n",
    "        elif channel_mode == \"color\":\n",
    "            noise = rand_func(image)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown channel_mode '{channel_mode}'\")\n",
    "\n",
    "        blur_sigma = random.uniform(self.blur_sigma_min, self.blur_sigma_max)\n",
    "        if blur_sigma > 0.:\n",
    "            noise = VF.gaussian_blur(noise, 5, blur_sigma)\n",
    "        \n",
    "        if distribution_mode == \"negative\":\n",
    "            noise = -noise\n",
    "\n",
    "        return (image + amt * noise).clamp(0, 1)\n",
    "\n",
    "\n",
    "noise_patches = ImageMultiNoise(amt_min=.0, amt_max=.3)(patches)\n",
    "VF.to_pil_image(resize(make_grid(noise_patches), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e507fd6-7875-4f06-b5dd-026f79f469c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "noises = [\n",
    "    torch.rand(10000) - torch.rand(10000),\n",
    "    torch.randn(10000),\n",
    "]\n",
    "    \n",
    "for data in noises:\n",
    "    display(px.histogram(data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
