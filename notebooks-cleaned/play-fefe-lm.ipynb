{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b23180-fbdb-4a9f-bfd7-fdc99a696a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_notebook import *\n",
    "from src.train.experiment import load_experiment_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f2f45-7ab0-4ad7-ba9e-5e7ffb3b5d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = load_experiment_trainer(\"../experiments/minimind/fefe2.yml\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37532b2-074e-4de1-90c4-84238217a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input_text: str):\n",
    "    tokenizer = trainer._tokenizer\n",
    "    input_tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    print(input_text, end=\"\")\n",
    "    num_printed = 0\n",
    "    for tokens in trainer.model.generate(\n",
    "        input_tokens.input_ids, \n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        stream=True,\n",
    "        temperature=.7,\n",
    "    ):\n",
    "        tokens = tokens[0, num_printed:]\n",
    "        print(tokenizer.decode(tokens).replace(\"⬇\", \" \").replace(\"⬅\", \"\\n\"), end=\"\")\n",
    "        num_printed += tokens.shape[0]\n",
    "        #print(token)\n",
    "\n",
    "generate(\n",
    "    #\"!!1!\",\n",
    "    \"Die Erklärung lautet: \",\n",
    "    #\"Faschisten\",\n",
    "    #\"Cinderella hat ein U-Boot gesprengt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b5a000-412a-455c-b3f5-923383c82426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad622e5-c618-412a-b5a1-6f853d59ff64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92756ebb-771d-4519-b208-3f0cf7b9e2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.minimind.tokenizedataset import TokenizeDataset\n",
    "from src.datasets import FefePostIterableDataset\n",
    "from typing import Sequence\n",
    "dataset = FefePostIterableDataset().freeze() \n",
    "\n",
    "class TokenizeDataset(BaseIterableDataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            texts: Sequence[str],\n",
    "            tokenizer,\n",
    "            max_seq_length: int,\n",
    "            min_seq_length: int = None,\n",
    "            batch_size: int = None,\n",
    "            method: str = \"concat\",\n",
    "    ):\n",
    "        self._texts = texts\n",
    "        self._tokenizer = tokenizer\n",
    "        self._min_seq_length = min_seq_length\n",
    "        self._max_seq_length = max_seq_length\n",
    "        self._batch_size = batch_size\n",
    "        self._method = method\n",
    "\n",
    "#    def __len__(self):\n",
    "#        return len(self._texts)\n",
    "\n",
    "    def __iter__(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        #from tqdm import tqdm\n",
    "        if self._method == \"truncate\":\n",
    "            iterable = self._iter_truncated()\n",
    "        elif self._method == \"fragments\":\n",
    "            iterable = self._iter_fragments()\n",
    "        elif self._method == \"concat\":\n",
    "            iterable = self._iter_concat()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method `{self._method}`\")\n",
    "            \n",
    "        for input_ids in iterable:\n",
    "            if input_ids.shape[0]:\n",
    "                loss_mask = (input_ids != self._tokenizer.pad_token_id)\n",
    "    \n",
    "                X = input_ids[:-1]\n",
    "                Y = input_ids[1:]\n",
    "                loss_mask = loss_mask[1:]\n",
    "                yield X, Y, loss_mask\n",
    "\n",
    "    def _iter_truncated(self):\n",
    "        seq_length = self._max_seq_length\n",
    "        for i, text in enumerate(self._texts):\n",
    "\n",
    "            if self._min_seq_length:\n",
    "                assert self._batch_size, \"Must define `batch_size` when defining `min_seq_length`\"\n",
    "                if i % self._batch_size == 0:\n",
    "                    seq_length = random.randint(self._min_seq_length, self._max_seq_length)\n",
    "\n",
    "            encoding = self._tokenizer(\n",
    "                text,\n",
    "                max_length=seq_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            yield encoding.input_ids.squeeze()\n",
    "\n",
    "    def _iter_fragments(self):\n",
    "        seq_length = self._max_seq_length\n",
    "        count = 0\n",
    "        for text in self._texts:\n",
    "            encoding = self._tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = encoding.input_ids.squeeze()\n",
    "            if input_ids.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            while True:\n",
    "                if self._min_seq_length is not None:\n",
    "                    assert self._batch_size, \"Must define `batch_size` when defining `min_seq_length`\"\n",
    "                    if count % self._batch_size == 0:\n",
    "                        seq_length = random.randint(self._min_seq_length, self._max_seq_length)\n",
    "                count += 1\n",
    "\n",
    "                if input_ids.shape[0] == seq_length:\n",
    "                    yield input_ids\n",
    "                    break\n",
    "\n",
    "                elif input_ids.shape[0] < seq_length:\n",
    "                    yield torch.cat([\n",
    "                        torch.ones((seq_length - input_ids.shape[0], ), dtype=input_ids.dtype) * self._tokenizer.pad_token_id,\n",
    "                        input_ids\n",
    "                    ])\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    yield input_ids[:seq_length]\n",
    "                    input_ids = input_ids[seq_length // 2:]\n",
    "\n",
    "    def _iter_concat(self):\n",
    "        seq_length = self._max_seq_length\n",
    "        count = 0\n",
    "        current_ids = None\n",
    "        for text in self._texts:\n",
    "            encoding = self._tokenizer(\n",
    "                text,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = encoding.input_ids.squeeze()\n",
    "            if input_ids.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            while True:\n",
    "                if self._min_seq_length is not None:\n",
    "                    assert self._batch_size, \"Must define `batch_size` when defining `min_seq_length`\"\n",
    "                    if count % self._batch_size == 0:\n",
    "                        seq_length = random.randint(self._min_seq_length, self._max_seq_length)\n",
    "\n",
    "                if input_ids.shape[0] == seq_length:\n",
    "                    yield input_ids\n",
    "                    count += 1\n",
    "                    break\n",
    "\n",
    "                elif input_ids.shape[0] < seq_length:\n",
    "                    if current_ids is None:\n",
    "                        current_ids = input_ids\n",
    "                    else:\n",
    "                        current_ids = torch.cat([\n",
    "                            current_ids,\n",
    "                            torch.ones((1, ), dtype=input_ids.dtype) * self._tokenizer.sep_token_id,\n",
    "                            input_ids\n",
    "                        ])\n",
    "                    \n",
    "                    if current_ids.shape[0] >= seq_length:\n",
    "                        yield current_ids[:seq_length]\n",
    "                        count += 1\n",
    "                        current_ids = current_ids[seq_length:]\n",
    "                    break\n",
    "                \n",
    "                else:\n",
    "                    yield input_ids[:seq_length]\n",
    "                    count += 1\n",
    "                    input_ids = input_ids[seq_length // 2:]\n",
    "\n",
    "ds = TokenizeDataset(\n",
    "    dataset.skip(1000),\n",
    "    trainer._tokenizer,\n",
    "    min_seq_length=256,\n",
    "    max_seq_length=256,\n",
    "    batch_size=16,\n",
    "    method=\"concat\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e1fe4-ca16-439b-aaba-4b9f06a4926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (X, Y, _) in enumerate(tqdm(ds)):\n",
    "    if i % 16 == 0:\n",
    "        bs = X.shape[0]\n",
    "    else:\n",
    "        assert bs == X.shape[0], f\"@{i} {bs} != {X.shape[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8638948-b166-4d80-9f60-fe4026fcc365",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (X, Y, _) in enumerate(tqdm(ds)):\n",
    "    print()\n",
    "    print(trainer._tokenizer.decode(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed45347-636b-4013-8cd8-e6a9d06baf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(DataLoader(ds, batch_size=16, num_workers=1)) as progress:\n",
    "    for i, _ in enumerate(progress):\n",
    "        progress.set_postfix({\"step\": i * 16}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90ba49-a8a9-4c76-9b5b-fcf34ee4a160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
