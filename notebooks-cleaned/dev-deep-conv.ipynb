{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db4088-ebb5-4a57-b418-5f25e91c3c1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset, RandomSampler\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import *\n",
    "from src.util import *\n",
    "from src.algo import *\n",
    "from src.models.cnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82de8aad-c70c-479b-b265-b12d5aa811d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SHAPE = (3, 64, 64)\n",
    "dataset = TensorDataset(torch.load(f\"../datasets/kali-uint8-{SHAPE[-2]}x{SHAPE[-1]}.pt\"))\n",
    "dataset = TransformDataset(dataset, dtype=torch.float, multiply=1./255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e813b4f-4136-4b0a-8da9-afb39b9d1476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = None\n",
    "        self.stride = stride\n",
    "\n",
    "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
    "            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n",
    "            self.downsample = nn.Sequential(OrderedDict([\n",
    "                (\"-1\", nn.AvgPool2d(stride)),\n",
    "                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n",
    "                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.relu2(self.bn2(self.conv2(out)))\n",
    "        out = self.avgpool(out)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu3(out)\n",
    "        return out\n",
    "\n",
    "m = Bottleneck(3, 4, 1)\n",
    "m(torch.rand(1, 3, 4, 6)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45650e0-335b-48cb-a9cd-f37dc932bcb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VF.to_pil_image(\n",
    "#dataset[12][0]\n",
    "m(dataset[12][0].unsqueeze(0))[0, :3].clip(0,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856eb704-3289-4d4f-941c-b13e56908d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4697a5-d52e-4d0b-b1d8-d4e42cb419dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54fb19-8e85-4c74-bc0a-942ff4beadee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099967a4-075a-4d5a-9f3c-85233966d43b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "    \n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "            (\"gelu\", QuickGELU()),\n",
    "            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
    "        ]))\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.attn_mask = attn_mask\n",
    "\n",
    "    def attention(self, x: torch.Tensor):\n",
    "        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
    "        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layers = layers\n",
    "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.resblocks(x)\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n",
    "\n",
    "        scale = width ** -0.5\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n",
    "        self.ln_pre = LayerNorm(width)\n",
    "\n",
    "        self.transformer = Transformer(width, layers, heads)\n",
    "\n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv1(x)  # shape = [*, width, grid, grid]\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "        x = x + self.positional_embedding.to(x.dtype)\n",
    "        x = self.ln_pre(x)\n",
    "\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "        x = self.ln_post(x[:, 0, :])\n",
    "\n",
    "        if self.proj is not None:\n",
    "            x = x @ self.proj\n",
    "\n",
    "        return x\n",
    "\n",
    "trans = VisionTransformer(64, patch_size=32, width=256, layers=10, heads=8, output_dim=128)\n",
    "print(f\"params: {num_module_parameters(trans):,}\")\n",
    "#trans(torch.zeros(1, 3, 64, 64))\n",
    "trans#(dataset[0][0].unsqueeze(0)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401e93bc-e155-4814-bb35-2adc84b6b9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb265461-d5f6-46b7-af71-8c14f9b64020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64d672-170d-4fd9-92d1-5dba6a834751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn.Sequential(\n",
    "    nn.Conv2d(3, 5, 7),\n",
    "    nn.MaxPool2d(16),\n",
    ")(torch.rand(1, 3, 100, 100)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7995664d-d066-490d-aa94-a5935ecca389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv = Conv2dBlock([3, 5], pool_kernel_size=5)\n",
    "\n",
    "conv(torch.rand(1, 3, 100, 100)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f06b2-517c-4901-aa0c-6e4a884a5fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc66595-3724-47d0-8467-d2f4643a0df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d1b76-105d-45f0-87ce-988ec4891348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5de8d5-f43e-4ec5-beff-042f51ef4234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_model = clip.load(\"ViT-B/32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dab621f-e3fb-4739-843d-3f22218078c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_module_parameters(clip_model)\n",
    "clip_model.visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef7b66-cf0e-448e-b991-0118fe2eee55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395e77ef-9745-46ba-b368-f8bc18a5fa9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_model.visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e064d6-0500-4ded-8563-c55550e5578e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def compressed_size(img, format: str, **kwargs):\n",
    "    fp = BytesIO()\n",
    "    img.save(fp, format, **kwargs)\n",
    "    return fp.tell()\n",
    "\n",
    "img = VF.to_pil_image(dataset[114][0])\n",
    "print(compressed_size(img, \"jpeg\", quality=0))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a430d55-10d8-407b-8da4-e90f0fb74705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range(100):\n",
    "    img = VF.to_pil_image(dataset[i][0])\n",
    "    row = {}\n",
    "    for q in range(0, 20, 1):\n",
    "        row[f\"q{q}\"] = compressed_size(img, \"jpeg\", quality=q)\n",
    "    rows.append(row)\n",
    "df = pd.DataFrame(rows)\n",
    "#px.line(df)\n",
    "px.imshow(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce7861-1e30-4b79-a2b9-995b3e9d8946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range(100):\n",
    "    img = VF.to_pil_image(dataset[i][0])\n",
    "    row = {}\n",
    "    for q, o in (\n",
    "        (0, False), (1, False), (1, True), (4, False), (4, True), (9, False), (9, True),\n",
    "    ):\n",
    "        row[f\"q{q}{o}\"] = compressed_size(img, \"png\", compress_level=q, optimize=o)\n",
    "    rows.append(row)\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a25140-8be5-43dc-91aa-a6d90f83bf80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8576116a-d479-4036-8c3c-648775912e93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "px.line(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bce93e-d2a3-4863-a5cd-4f4a1328a8fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img=VF.to_pil_image(dataset[118][0])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ea1023-ceb2-4643-8ccd-1ef2a834852f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "ratioer = ImageCompressionRatio()\n",
    "for i in range(1000, 2000):\n",
    "    img = VF.to_pil_image(dataset[i][0])\n",
    "    row = ratioer.all(img)\n",
    "    row.update(ratioer.all(\n",
    "        VF.gaussian_blur(img, kernel_size=[21, 21], sigma=10),\n",
    "        suffix=\"-blur\",\n",
    "    ))\n",
    "    rows.append(row)\n",
    "df = pd.DataFrame(rows)\n",
    "display(px.line(df))\n",
    "px.imshow(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f73c84-e031-451b-8433-a683f266acb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = df.loc[df[\"png-low-blur\"] < df[\"jpeg-low\"]].copy()\n",
    "df2.loc[:, \"diff\"] = df[\"jpeg-low\"] - df[\"png-high\"]\n",
    "df2 = df2.sort_values(\"diff\")\n",
    "#df2\n",
    "#px.line(df2)\n",
    "VF.to_pil_image(make_grid([dataset[i][0] for i in df2.index]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
