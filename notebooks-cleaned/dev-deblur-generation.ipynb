{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daddfe2f-19a0-44be-baa0-52acd1b4b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from init_notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6964d46-825d-432f-960f-359818f0703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.datasets.classic import *\n",
    "ds = cifar10_dataset(train=True)\n",
    "ds2 = mnist_dataset(train=True)\n",
    "\n",
    "print([ds[i][0].shape for i in range(8)])\n",
    "VF.to_pil_image(make_grid(\n",
    "    [ds[i][0] for i in range(8*8)]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecacc81c-58aa-414a-9851-a9e224a47c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur(images, a: float):\n",
    "    ks = 21\n",
    "    sig = max(0.001, math.pow(a, 1.7) * 7)\n",
    "    images = VF.gaussian_blur(images, [ks, ks], [sig, sig]) \n",
    "    return images\n",
    "    \n",
    "images = torch.cat([ds[i][0].unsqueeze(0) for i in range(8)])\n",
    "for i in range(20):\n",
    "    display(VF.to_pil_image(make_grid(blur(images, i/19))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896bcf1-8467-40a5-b966-08c0885358fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic(images, a: float):\n",
    "    tr = VT.ElasticTransform(a*250., 3.)\n",
    "    sig = max(0.001, math.pow(a, 1.7) * 7)\n",
    "    images = tr(images)\n",
    "    return images\n",
    "    \n",
    "images = torch.cat([ds[i][0].unsqueeze(0) for i in range(8)])\n",
    "for i in range(20):\n",
    "    display(VF.to_pil_image(make_grid(elastic(images, i/19))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321aec0-ad15-4c84-a67e-ef18ee0dc608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.diffusion.sampler import *\n",
    "class DiffusionSamplerDeform(DiffusionSamplerBase):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            alpha: float = 50.,\n",
    "            sigma: float = 3.,\n",
    "            fill: float = -1.,\n",
    "    ):\n",
    "        self.alpha = alpha\n",
    "        self.sigma = sigma\n",
    "        self.fill = fill\n",
    "\n",
    "    @staticmethod\n",
    "    def get_displacement(sigma: List[float], size: List[int], batch_size, generator):\n",
    "        dx = torch.rand([1, 1] + size, generator=generator) * 2 - 1\n",
    "        if sigma[0] > 0.0:\n",
    "            kx = int(8 * sigma[0] + 1)\n",
    "            # if kernel size is even we have to make it odd\n",
    "            if kx % 2 == 0:\n",
    "                kx += 1\n",
    "            dx = VF.gaussian_blur(dx, [kx, kx], sigma)\n",
    "        dx = dx / (size[0] / batch_size)\n",
    "\n",
    "        dy = torch.rand([1, 1] + size, generator=generator) * 2 - 1\n",
    "        if sigma[1] > 0.0:\n",
    "            ky = int(8 * sigma[1] + 1)\n",
    "            # if kernel size is even we have to make it odd\n",
    "            if ky % 2 == 0:\n",
    "                ky += 1\n",
    "            dy = VF.gaussian_blur(dy, [ky, ky], sigma)\n",
    "        dy = dy / size[1] \n",
    "        return torch.concat([dx, dy], 1).permute([0, 2, 3, 1])[0]  # 1 x H x W x 2\n",
    "\n",
    "    def _add_noise(\n",
    "            self,\n",
    "            images: torch.Tensor,\n",
    "            noise_amounts: torch.Tensor,\n",
    "            generator: Optional[torch.Generator],\n",
    "    ):\n",
    "        B, C, H, W = images.shape\n",
    "        disp = self.get_displacement([self.sigma, self.sigma], [B * H, W], B, generator).to(images)\n",
    "        # disp is [B * H, W, 2]\n",
    "        disp = disp.view(B, H, W, 2) * noise_amounts[:, None, None] * self.alpha\n",
    "        \n",
    "        return torch.cat([\n",
    "            VF.elastic_transform(image.unsqueeze(0), d.unsqueeze(0), fill=self.fill)\n",
    "            for image, d in zip(images, disp)\n",
    "        ])\n",
    "\n",
    "sampler = DiffusionSamplerDeform(200, 6)\n",
    "\n",
    "images = torch.cat([ds[i][0].unsqueeze(0) for i in range(8)])\n",
    "for i in range(10):\n",
    "    noisy_images, _ = sampler.add_noise(images * 2 - 1., torch.ones(images.shape[0], 1).to(images) * i / 9)\n",
    "    display(VF.to_pil_image(make_grid(noisy_images * .5 + .5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f9bcf1-aba3-4589-bfd5-410dcfd798c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.denoise.resconv import ResConv\n",
    "\n",
    "class Module(nn.Module):\n",
    "  def __init__(self):\n",
    "      super().__init__()\n",
    "      self.module = ResConv(\n",
    "          in_channels=3,\n",
    "          num_layers=3,\n",
    "          channels=32,\n",
    "          stride=1,\n",
    "          kernel_size=[3, 7, 9],\n",
    "          padding=[1, 3, 4],\n",
    "          activation=\"gelu\",\n",
    "          activation_last_layer=None,\n",
    "      )\n",
    "\n",
    "  def forward(self, x):\n",
    "      return (x - self.module(x)).clamp(0, 1) \n",
    "\n",
    "model = Module()\n",
    "model.load_state_dict(\n",
    "    torch.load(\"../checkpoints/denoise/deblur5x10-resconv-bs:64_opt:AdamW_lr:0.0003_l:3_ks1:3_ks2:9_ch:32_stride:1_act:gelu/best.pt\")\n",
    "    [\"state_dict\"]\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1afa7b8-73b0-4a25-aa51-d0d81d94c289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b53a5-0be8-40c8-aaa2-9d44191e873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    batch_size: int, shape: Tuple[int, int, int],\n",
    "    ks: int = 5, sigma: float = 10.,\n",
    "    steps: int = 5,\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn((batch_size, shape[0], shape[1]//4, shape[2]//4)).clamp(0, 1)\n",
    "        noise *= .1 + .9 * torch.linspace(0, 1, batch_size)[:, None, None, None]\n",
    "        noise = VF.gaussian_blur(noise, [ks, ks], [sigma, sigma])\n",
    "        noise = resize(noise, 4)\n",
    "        noise = VF.gaussian_blur(noise, [ks, ks], [sigma, sigma])\n",
    "        display(VF.to_pil_image(make_grid(noise, nrow=batch_size)))\n",
    "        \n",
    "        for i in range(5):\n",
    "            denoised = model(noise)\n",
    "            display(VF.to_pil_image(make_grid(denoised, nrow=batch_size)))\n",
    "            \n",
    "            noise = (noise + denoised) / 2\n",
    "            noise = VF.gaussian_blur(noise, [ks, ks], [sigma, sigma])\n",
    "\n",
    "generate(10, (3, 48, 48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb0d4b8-1cb3-4ab6-a8ec-13c4defd04f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c231d0-b185-4539-bf49-3c4b36ca17fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb59649-9aee-4e80-aa71-a4322c2f1141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "ds = torchvision.datasets.STL10(\"~/prog/data/datasets/\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b67094-849b-4650-b730-1631afd1f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.data.shape)\n",
    "torch.Tensor(ds.data).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66637b7-fd29-4f45-abd8-19e89ec95a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7916be0f-5c14-45ae-85cd-85ea48ddf3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = TensorDataset(torch.Tensor(ds.data), torch.Tensor(ds.labels))\n",
    "print(len(ds2))\n",
    "print(ds2[0][0].shape)\n",
    "VF.to_pil_image(make_grid(\n",
    "    [ds2[i][0]/255 for i in range(8*8)]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725785bf-8534-4bad-ba9b-c8086c6000b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4835c02-424b-4259-9419-73a970d15e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5478e14d-4b4d-4a2b-862e-a3eee94bf136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            image_size: int,\n",
    "            image_channels: int,\n",
    "            patch_size: int,\n",
    "            num_layers: int,\n",
    "            num_heads: int,\n",
    "            hidden_dim: int,\n",
    "            mlp_dim: int,\n",
    "            dropout: float = 0.0,\n",
    "            attention_dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        model = torchvision.models.VisionTransformer(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            hidden_dim=hidden_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "        )\n",
    "        self.patch_size = model.patch_size\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
    "        self.proj_in = nn.Conv2d(image_channels, hidden_dim, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        self.transformer = model.encoder\n",
    "        self.proj_out = nn.ConvTranspose2d(hidden_dim, image_channels, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.proj_in(x)\n",
    "        shape = y.shape\n",
    "        y = y.flatten(-2).permute(0, 2, 1)\n",
    "\n",
    "        batch_class_token = self.class_token.expand(x.shape[0], -1, -1)\n",
    "        y = torch.cat([batch_class_token, y], dim=1)\n",
    "\n",
    "        y = self.transformer(y)\n",
    "        y = y[:, :-1, :].permute(0, 2, 1)\n",
    "        y = self.proj_out(y.view(shape))\n",
    "        return y\n",
    "\n",
    "m = ViT(32, 3, 4, 3, 4, 100, 1000)\n",
    "print(f\"params: {num_module_parameters(m):,}\")\n",
    "print(m(torch.ones(1, 3, 32, 32)).shape)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9bc996-877e-4d71-8b2c-827f7d42dc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.ones(1, 3, 32, 32)\n",
    "conv = nn.Conv2d(3, 64, 4, stride=4)\n",
    "conv2 = nn.ConvTranspose2d(64, 3, 4, stride=4)\n",
    "\n",
    "img2 = conv(img)\n",
    "print(img2.shape)\n",
    "print(conv2(img2).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
