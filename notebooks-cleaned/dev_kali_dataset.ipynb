{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ed52b-36ad-43d0-99d3-236d6bb06640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from src.datasets import *\n",
    "from src.util.image import * \n",
    "from src.algo import Space2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f37365-390e-4844-9c88-7a4858727e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9e45f85-0ba8-4fec-ba81-8493cc9e008e",
   "metadata": {},
   "source": [
    "## p = abs(p) / dot(p, p) - v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52c13ff-c8b6-49b9-867b-338f70467b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kali2d(\n",
    "        space: Space2d,\n",
    "        param: torch.Tensor,\n",
    "        iterations: int = 11,\n",
    "        out_weights: Optional[torch.Tensor] = None,\n",
    "        accumulate: str = \"submin\",  # none, mean, max, min, submin, alternate\n",
    "        exponent: float = 0,\n",
    "        sin_freq: float = 0,\n",
    "        aa: int = 0,\n",
    ") -> torch.Tensor:\n",
    "    param = param.reshape(-1, 1, 1)\n",
    "\n",
    "    def _render(space: torch.Tensor) -> torch.Tensor:\n",
    "        if accumulate == \"none\":\n",
    "            pass\n",
    "        elif accumulate in (\"min\", \"submin\"):\n",
    "            accum = torch.ones_like(space) * iterations\n",
    "        else:\n",
    "            accum = torch.zeros_like(space)\n",
    "\n",
    "        for iteration in range(iterations):\n",
    "            dot_prod = torch.sum(space * space, dim=0, keepdim=True) + 0.000001\n",
    "            space = torch.abs(space) / dot_prod\n",
    "            \n",
    "            a_space = space\n",
    "            if exponent:\n",
    "                a_space = torch.exp(-a_space * exponent)\n",
    "                \n",
    "            if sin_freq:\n",
    "                a_space = torch.sin(a_space * sin_freq)\n",
    "\n",
    "            if accumulate == \"mean\":\n",
    "                accum = accum + a_space\n",
    "\n",
    "            elif accumulate == \"max\":\n",
    "                accum = torch.max(a_space, accum)\n",
    "\n",
    "            elif accumulate == \"min\":\n",
    "                accum = torch.min(a_space, accum)\n",
    "            \n",
    "            elif accumulate == \"submin\":\n",
    "                accum = accum - torch.min(accum, a_space)\n",
    "\n",
    "            elif accumulate == \"alternate\":\n",
    "                accum = accum + (a_space if iteration % 2 == 0 else -a_space)\n",
    "                        \n",
    "            if iteration < iterations - 1:\n",
    "                space = space - param\n",
    "\n",
    "        if accumulate == \"none\":\n",
    "            output = a_space \n",
    "        elif accumulate == \"min\":\n",
    "            output = accum * iterations\n",
    "        else:\n",
    "            output = accum / iterations\n",
    "            if accumulate == \"alternate\":\n",
    "                output = output * 2\n",
    "\n",
    "        return output\n",
    "\n",
    "    if aa and aa > 1:\n",
    "        s = space.shape\n",
    "        aa_space = space.space().repeat(1, aa, aa)\n",
    "        for x in range(aa):\n",
    "            for y in range(aa):\n",
    "                if x or y:\n",
    "                    aa_space[1, y*s[-2]:(y+1)*s[-2], x*s[-1]:(x+1)*s[-1]] += (y / aa / s[-2]) * space.scale\n",
    "                    aa_space[0, y*s[-2]:(y+1)*s[-2], x*s[-1]:(x+1)*s[-1]] += (x / aa / s[-1]) * space.scale\n",
    "\n",
    "        output = _render(aa_space)\n",
    "        for x in range(0, aa):\n",
    "            for y in range(0, aa):\n",
    "                if x or y:\n",
    "                    output[:, :s[-2], :s[-1]] = output[:, :s[-2], :s[-1]] + output[:, y*s[-2]:(y+1)*s[-2], x*s[-1]:(x+1)*s[-1]]\n",
    "        output = output[:, :s[-2], :s[-1]] / (aa * aa)\n",
    "       \n",
    "    else:\n",
    "        output = _render(space.space())\n",
    "\n",
    "    if out_weights is not None:\n",
    "        a = output.permute(1, 2, 0).reshape(-1, 3)\n",
    "        output = torch.matmul(a, out_weights).reshape((output.shape[1], output.shape[2], output.shape[0])).permute(2, 0, 1)\n",
    "\n",
    "    return torch.clamp(output, 0, 1)\n",
    "\n",
    "\n",
    "\n",
    "space = Space2d((3, 300, 300), offset=torch.Tensor([0,0,0]), scale=1.)\n",
    "#VF.to_pil_image(\n",
    "img = kali2d(space, param=torch.Tensor([.5, .5, .5]))#, accumulate=\"x\", aa=2)#out_weights=torch.randn((3, 3)) / 2.)\n",
    "VF.to_pil_image(img[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb97dbd-779a-49c7-8e55-26a581220b9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = kali2d(\n",
    "    Space2d(\n",
    "        shape=(3, 128, 128), \n",
    "        offset=torch.Tensor([0.7,0,0]), \n",
    "        scale=.01,\n",
    "    ), \n",
    "    param=torch.Tensor([.75, .75, .75]),\n",
    "    iterations=21,\n",
    "    #out_weights=torch.rand((3, 3)),\n",
    "    accumulate=\"x\",\n",
    "    aa=10,\n",
    ")\n",
    "#img = VF.resize(img, [512, 512], interpolation=VF.InterpolationMode.BICUBIC)\n",
    "img = VF.resize(img, [1024, 1024], interpolation=VF.InterpolationMode.NEAREST)\n",
    "VF.to_pil_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ed29a4-3729-463d-a31d-acbc636a4cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6409f6-502d-4c00-8836-398af6f035a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6932259a-c056-47d7-bd12-b38b101a1470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae915d9-f6cd-4cff-b64a-4f22f148fed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Kali2dDataset(Dataset):\n",
    "    accumulation_choices = [\"none\", \"mean\", \"min\", \"max\"]\n",
    "    def __init__(\n",
    "        self,\n",
    "        shape: Tuple[int, int, int],\n",
    "        size: int = 1000,\n",
    "        seed: int = 23,\n",
    "        min_scale: float = 0.,\n",
    "        max_scale: float = 2.,\n",
    "        min_offset: float = -2.,\n",
    "        max_offset: float = 2.,\n",
    "        min_iterations: int = 1,\n",
    "        max_iterations: int = 37,\n",
    "        accumulation_modes: Optional[Iterable[str]] = None,\n",
    "        dtype: torch.dtype = torch.float,\n",
    "        aa: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.shape = shape\n",
    "        self._size = size\n",
    "        self.seed = seed\n",
    "        self.min_scale = min_scale\n",
    "        self.max_scale = max_scale\n",
    "        self.min_offset = min_offset\n",
    "        self.max_offset = max_offset\n",
    "        self.min_iterations = min_iterations\n",
    "        self.max_iterations = max_iterations\n",
    "        self.accumulation_modes = self.accumulation_choices if accumulation_modes is None else list(accumulation_modes)\n",
    "        self.dtype = dtype\n",
    "        self.aa = aa\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self._size\n",
    "    \n",
    "    def __getitem__(self, idx) -> torch.Tensor:\n",
    "        rng = torch.Generator().manual_seed(idx ^ self.seed)\n",
    "        \n",
    "        space = Space2d(\n",
    "            shape=self.shape,\n",
    "            offset=torch.rand(self.shape[0], dtype=self.dtype, generator=rng) * (self.min_offset - self.max_offset) + self.min_offset,\n",
    "            scale=torch.pow(torch.rand(1, dtype=self.dtype, generator=rng)[0], 3.) * (self.max_scale - self.min_scale) + self.min_scale,\n",
    "            dtype=self.dtype,\n",
    "        )\n",
    "        \n",
    "        param=torch.rand(self.shape[0], dtype=self.dtype, generator=rng) * 1.2\n",
    "        accumulate = self.accumulation_modes[torch.randint(0, len(self.accumulation_modes), (1,), generator=rng)[0]]\n",
    "        iterations=max(self.min_iterations, min(self.max_iterations, \n",
    "            int(torch.randint(self.min_iterations, self.max_iterations, (1,), generator=rng)[0]) + int(1. / space.scale)\n",
    "        ))\n",
    "        out_weights = (\n",
    "            torch.rand((self.shape[0], self.shape[0]), dtype=self.dtype, generator=rng) / math.sqrt(self.shape[0])\n",
    "            + torch.randn((self.shape[0], self.shape[0]), dtype=self.dtype, generator=rng) * .2\n",
    "        )\n",
    "        return kali2d(\n",
    "            space=space,\n",
    "            param=param,\n",
    "            iterations=iterations,\n",
    "            accumulate=accumulate,\n",
    "            out_weights=out_weights,\n",
    "            aa=self.aa,\n",
    "        )\n",
    "        \n",
    "dataset = Kali2dDataset(\n",
    "    (3, 64, 64), aa=2, size=1_000_000, \n",
    "    accumulation_modes=[\"min\", \"max\"],\n",
    "    min_iterations=17,\n",
    "    min_scale=0.01, max_scale=2.,\n",
    "    min_offset=-2., max_offset=2.\n",
    ")\n",
    "\n",
    "VF.to_pil_image(make_grid(\n",
    "    [dataset[i] for i in range(16*16)],\n",
    "    nrow=16\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84128d28-eb2f-47a6-a874-c3a12a868ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IterableImageFilterDataset(IterableDataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        dataset: Union[IterableDataset, Dataset],\n",
    "        min_mean: float = 0.,\n",
    "        max_mean: float = 0.,\n",
    "        min_std: float = 0.,\n",
    "        max_std: float = 0.,\n",
    "        min_compression_ratio: float = 0.,\n",
    "        max_compression_ratio: float = 0.,\n",
    "        min_scaled_compression_ratio: float = 0.,\n",
    "        max_scaled_compression_ratio: float = 0.,\n",
    "        scaled_compression_shape: Iterable[int] = (16, 16),\n",
    "        min_blurred_compression_ratio: float = 0.,\n",
    "        max_blurred_compression_ratio: float = 0.,\n",
    "        blurred_compression_kernel_size: Iterable[int] = (11, 11),\n",
    "        blurred_compression_sigma: float = 10.,\n",
    "        compression_format: str = \"png\",\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.min_mean = min_mean\n",
    "        self.max_mean = max_mean\n",
    "        self.min_std = min_std\n",
    "        self.max_std = max_std\n",
    "        self.min_compression_ratio = min_compression_ratio\n",
    "        self.max_compression_ratio = max_compression_ratio\n",
    "        self.min_scaled_compression_ratio = min_scaled_compression_ratio\n",
    "        self.max_scaled_compression_ratio = max_scaled_compression_ratio\n",
    "        self.scaled_compression_shape = list(scaled_compression_shape)\n",
    "        self.min_blurred_compression_ratio = min_blurred_compression_ratio\n",
    "        self.max_blurred_compression_ratio = max_blurred_compression_ratio\n",
    "        self.blurred_compression_kernel_size = blurred_compression_kernel_size\n",
    "        self.blurred_compression_sigma = blurred_compression_sigma\n",
    "        self.compression_format = compression_format\n",
    "        \n",
    "    def __iter__(self) -> Generator[torch.Tensor, None, None]:\n",
    "        for image in self._iter_dataset():\n",
    "\n",
    "            if self.min_mean or self.max_mean:\n",
    "                mean = image.mean()\n",
    "                if self.min_mean and mean < self.min_mean:\n",
    "                    continue\n",
    "                if self.max_mean and mean > self.max_mean:\n",
    "                    continue\n",
    "\n",
    "            if self.min_std or self.max_std:\n",
    "                std = image.std()\n",
    "                if self.min_std and std < self.min_std:\n",
    "                    continue\n",
    "                if self.max_std and std > self.max_std:\n",
    "                    continue\n",
    "                    \n",
    "            if self.min_compression_ratio or self.max_compression_ratio:\n",
    "                ratio = self.calc_compression_ratio(image)\n",
    "                if self.min_compression_ratio and ratio < self.min_compression_ratio:\n",
    "                    continue\n",
    "                if self.max_compression_ratio and ratio > self.max_compression_ratio:\n",
    "                    continue\n",
    "\n",
    "            if self.min_scaled_compression_ratio or self.max_scaled_compression_ratio:\n",
    "                ratio = self.calc_compression_ratio(\n",
    "                    VF.resize(image, self.scaled_compression_shape, interpolation=VF.InterpolationMode.BICUBIC)\n",
    "                )\n",
    "                if self.min_scaled_compression_ratio and ratio < self.min_scaled_compression_ratio:\n",
    "                    continue\n",
    "                if self.max_scaled_compression_ratio and ratio > self.max_scaled_compression_ratio:\n",
    "                    continue\n",
    "\n",
    "            if self.min_blurred_compression_ratio or self.max_blurred_compression_ratio:\n",
    "                ratio = self.calc_compression_ratio(\n",
    "                    VF.gaussian_blur(image, self.blurred_compression_kernel_size, self.blurred_compression_sigma)\n",
    "                )\n",
    "                if self.min_blurred_compression_ratio and ratio < self.min_blurred_compression_ratio:\n",
    "                    continue\n",
    "                if self.max_blurred_compression_ratio and ratio > self.max_blurred_compression_ratio:\n",
    "                    continue\n",
    "                #print(ratio)\n",
    "                \n",
    "            yield image\n",
    "\n",
    "    def _iter_dataset(self) -> Generator[torch.Tensor, None, None]:\n",
    "        if isinstance(self.dataset, Dataset):\n",
    "            for i in range(len(self.dataset)):\n",
    "                yield self.dataset[i]\n",
    "        else:\n",
    "            yield from self.dataset\n",
    "    \n",
    "    def calc_compression_ratio(self, image: torch.Tensor) -> float:\n",
    "        img = VF.to_pil_image(image)\n",
    "        fp = BytesIO()\n",
    "        img.save(fp, self.compression_format)\n",
    "        memory_size = math.prod(image.shape)\n",
    "        compress_size = fp.tell()\n",
    "        return compress_size / memory_size\n",
    "\n",
    "            \n",
    "ds_iter = IterableImageFilterDataset(\n",
    "    dataset,\n",
    "    #min_mean=.2,\n",
    "    max_mean=.3,\n",
    "    #min_std=.4,\n",
    "    #max_std=.3,\n",
    "    #min_compression_ratio=.5,\n",
    "    max_compression_ratio=.9,\n",
    "    #min_scaled_compression_ratio=.7,\n",
    "    #scaled_compression_shape=(16, 16),\n",
    "    min_blurred_compression_ratio=.3,\n",
    "    #min_blurred_compression_ratio=.32,\n",
    "    #blurred_compression_sigma=10.,\n",
    "    #blurred_compression_kernel_size=[21, 21],\n",
    ")\n",
    "samples = []\n",
    "total = 16 * 16\n",
    "for image in tqdm(ds_iter, total=total):\n",
    "    samples.append(image)\n",
    "    if len(samples) >= total:\n",
    "        break\n",
    "\n",
    "VF.to_pil_image(make_grid(samples, nrow=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0763beb9-5879-441a-9aa9-d71cab42d897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VF.to_pil_image(VF.gaussian_blur(dataset[55], [11, 11], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9edee6e-bf84-472a-a1bc-d8641cf03e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_image(image) -> bool:\n",
    "    if torch.all(image.reshape(3, -1).std(1) < 0.1):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def iter_images():\n",
    "    cropper = VT.RandomCrop(target_shape[-2:])\n",
    "    def _iter_crops(image, min_size: int):\n",
    "        count = 2 + max(1, min(40, (min_size - 400) // 200))\n",
    "        min_scale = max(.05, 1. - min_size / 400)\n",
    "        #print(min_size, min_scale)\n",
    "        num_yielded = 0\n",
    "        num_tried = 0\n",
    "        while num_yielded < count and num_tried < count * 5:\n",
    "            img = image\n",
    "            #image = VT.RandomAffine(degrees=30, scale=[2, 2])(image)\n",
    "            scale = min_scale + math.pow(random.random() * (1. - min_scale), 10.)\n",
    "            #if scale < random.random():\n",
    "            #    img = VT.RandomPerspective(distortion_scale=.7)(img)\n",
    "            #    crop_x = max(target_shape[-1], img.shape[-1] // 5)\n",
    "            #    crop_y = max(target_shape[-2], img.shape[-2] // 5)\n",
    "            #    img = VF.crop(img, crop_y // 2, crop_x // 2, img.shape[-2] - crop_y, img.shape[-1] - crop_x)\n",
    "            img = VF.resize(img, [\n",
    "                max(target_shape[-2], int(image.shape[-2] * scale)), \n",
    "                max(target_shape[-1], int(image.shape[-1] * scale)),\n",
    "            ])\n",
    "            if random.random() < .5:\n",
    "                center = center=[random.randrange(target_shape[-2]), random.randrange(target_shape[-2])]\n",
    "                img = VT.RandomRotation(30, center=center)(img)\n",
    "            img = cropper(img)\n",
    "            \n",
    "            num_tried += 1\n",
    "            if check_image(img):\n",
    "                yield img\n",
    "                num_yielded += 1\n",
    "            \n",
    "    for base_image in base_images:\n",
    "        image = set_image_channels(base_image, target_shape[0])\n",
    "        #yield image\n",
    "        yield image_resize_crop(image, target_shape[-2:])\n",
    "        \n",
    "        min_size = min(*image.shape[-2:])\n",
    "        if min_size >= 200:\n",
    "            yield from _iter_crops(image, min_size)\n",
    "        \n",
    "samples = []\n",
    "total = 16*16\n",
    "for i in tqdm(iter_images(), total=total):\n",
    "    i = i.clamp(0, 1)\n",
    "    if check_image(i):\n",
    "        samples.append(i)\n",
    "    if len(samples) >= total:\n",
    "        break\n",
    "VF.to_pil_image(make_grid(samples, nrow=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d225cf1-7d2a-4428-abe4-f274920989b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples[15].view(3, -1).std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c76813-e131-4e04-bd72-96e55df6be0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def store_dataset(\n",
    "        images: Iterable,\n",
    "        dtype=torch.float32,\n",
    "        #image_folder=\"~/Pictures/__diverse/\",\n",
    "        output_filename=\"../datasets/photos-32x32-std01.pt\",\n",
    "        max_megabyte=1_000,\n",
    "):\n",
    "    tensor_batch = []\n",
    "    tensor_size = 0\n",
    "    last_print_size = 0\n",
    "    for image in tqdm(images):\n",
    "        if len(image.shape) < 4:\n",
    "            image = image.unsqueeze(0)\n",
    "        tensor_batch.append(image.clamp(0, 1))\n",
    "        tensor_size += math.prod(image.shape) * 4\n",
    "\n",
    "        if tensor_size - last_print_size > 1024 * 1024 * 50:\n",
    "            last_print_size = tensor_size\n",
    "\n",
    "            print(f\"size: {tensor_size:,}\")\n",
    "\n",
    "        if tensor_size >= max_megabyte * 1024 * 1024:\n",
    "            break\n",
    "\n",
    "    tensor_batch = torch.cat(tensor_batch)\n",
    "    torch.save(tensor_batch, output_filename)\n",
    "\n",
    "store_dataset(iter_images())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75290a4b-136d-4ad4-9827-cc860d21bbb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ds = TensorDataset(torch.load(\"../datasets/diverse-32x32-std01.pt\"))\n",
    "ds = TensorDataset(torch.load(\"../datasets/photos-32x32-std01.pt\"))\n",
    "dl = DataLoader(ds, shuffle=True, batch_size=24*24)\n",
    "for batch in dl:\n",
    "    img = VF.to_pil_image(make_grid(batch[0], nrow=24))\n",
    "    break\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c1f44-9eb9-4911-bf43-682fe4db7a01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = torch.rand(16, 8)\n",
    "torch.concat([t, t]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
