{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de67573b-3acf-472f-8769-a53ba1089397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import random\n",
    "import math\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from typing import Optional, Callable, List, Tuple, Iterable, Generator, Union\n",
    "\n",
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "plotly.io.templates.default = \"plotly_dark\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, IterableDataset, RandomSampler\n",
    "import torchvision.transforms as VT\n",
    "import torchvision.transforms.functional as VF\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display\n",
    "\n",
    "from src.util.image import *\n",
    "from src.util import *\n",
    "from src.models.util import *\n",
    "from src.algo import ca1\n",
    "\n",
    "def resize(img, scale: float, mode: VF.InterpolationMode = VF.InterpolationMode.NEAREST):\n",
    "    return VF.resize(img, [max(1, int(s * scale)) for s in img.shape[-2:]], mode, antialias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a9792-8adc-4735-a609-721b2423e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.outer([1, 2, 1], [-1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75fa99-f3a4-4812-8519-14cc64d8d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralCA(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int = 16,\n",
    "        channels_hidden: int = 128,\n",
    "        default_shape: Optional[Tuple[int, int]] = None,\n",
    "        activation: Union[str, Callable] = \"tanh\",\n",
    "    ):\n",
    "        assert channels >= 4, channels\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.channels = channels\n",
    "        self.default_shape = default_shape\n",
    "        self.activation = activation_to_callable(activation)\n",
    "\n",
    "        sobel = torch.Tensor([\n",
    "            [-1,  0,  1],\n",
    "            [-2,  0,  2],\n",
    "            [-1,  0,  1]\n",
    "        ]) / 2.\n",
    "        self.sobel1 = nn.Parameter(\n",
    "            sobel.view(1, 1, 3, 3).repeat(channels, 1, 1, 1),\n",
    "            #torch.randn(channels, channels, 3, 3)\n",
    "            requires_grad=False,\n",
    "        )\n",
    "        self.sobel2 = nn.Parameter(\n",
    "            sobel.T.view(1, 1, 3, 3).repeat(channels, 1, 1, 1),\n",
    "            #torch.randn(channels, channels, 3, 3)\n",
    "            requires_grad=False,\n",
    "        )\n",
    "        self.lin1 = nn.Linear(channels * 3, channels_hidden)\n",
    "        self.lin2 = nn.Linear(channels_hidden, channels)\n",
    "\n",
    "    def forward(self, state: torch.Tensor, iterations: int = 32):\n",
    "        for it in range(iterations):\n",
    "            \n",
    "            s1 = F.conv2d(state, self.sobel1, padding=1, groups=self.channels, )\n",
    "            s2 = F.conv2d(state, self.sobel1, padding=1, groups=self.channels)\n",
    "            s = torch.concat([state, s1, s2], dim=-3)\n",
    "            \n",
    "            s = self.lin1(s.transpose(1, 3))\n",
    "            s = F.relu6(s)\n",
    "            s = self.lin2(s).transpose(1, 3)\n",
    "            \n",
    "            next_state = state + s#self.activation(s)\n",
    "            \n",
    "            #mask = ~(next_state[..., 4:5, :, :] > .1).expand(-1, self.channels, -1, -1)\n",
    "            #next_state[mask] = next_state[mask] * .1\n",
    "\n",
    "            state = next_state\n",
    "            \n",
    "        return state\n",
    "            \n",
    "    def generate(self, state: Optional[torch.Tensor] = None, shape: Optional[Tuple[int, int]] = None, **kwargs):\n",
    "        if state is not None:\n",
    "            shape = state.shape[-2:]\n",
    "        if shape is None:\n",
    "            shape = self.default_shape\n",
    "        if shape is None:\n",
    "            raise ValueError(\"Need to define `shape` or `default_shape` in constructor\")\n",
    "        if state is None:\n",
    "            state = torch.zeros(1, model.channels, *shape).to(self.sobel1)\n",
    "            state[..., :, shape[-2] // 2, shape[-1] // 2] = 1\n",
    "        image = torch.tanh(self(state, **kwargs)) * .5 + .5\n",
    "        image = image[:, :3, :, :] * image[:, 3:4, :, :]\n",
    "        return image\n",
    "        \n",
    "model = NeuralCA()\n",
    "print(f\"params: {num_module_parameters(model, trainable=True):,} / {num_module_parameters(model):,}\")\n",
    "#image = model(image, iterations=32)\n",
    "image = model.generate(shape=(100, 100), iterations=20)\n",
    "VF.to_pil_image(resize(image[0, :3, :, :], 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3133e624-2117-4075-9496-a8d0fdc8faf5",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27527f85-0480-45d8-b1c6-fd19dec63992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_image(\n",
    "        model: nn.Module, \n",
    "        image: str,\n",
    "        shape: Tuple[int, int] = (64, 64),\n",
    "        epochs: int = 1000,\n",
    "        device: str = \"auto\",\n",
    "):\n",
    "    target_image = VF.to_tensor(PIL.Image.open(Path(image).expanduser()))[:3]\n",
    "    assert target_image.shape[-3] >= 3\n",
    "    target_image = image_resize_crop(target_image, shape) \n",
    "    display(VF.to_pil_image(target_image))\n",
    "    \n",
    "    device = to_torch_device(device)\n",
    "    model.to(device)\n",
    "    target_image = target_image.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, .999)\n",
    "\n",
    "    print(f\"params: {num_module_parameters(model):,}\")\n",
    "\n",
    "    seed_image = torch.zeros(1, model.channels, *shape).to(device)\n",
    "    seed_image[..., :, seed_image.shape[-2] // 2, seed_image.shape[-1] // 2] = 1\n",
    "\n",
    "    history = {\"loss\": [], \"lr\": []}\n",
    "    images = []\n",
    "    try:\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "            image = model.generate(\n",
    "                seed_image,\n",
    "                iterations=random.randint(32, 96),\n",
    "            )\n",
    "            #image = torch.tanh(model(seed_image)) * .5 + .5\n",
    "            #image = image[:, :3, :, :] * image[:, 3:4, :, :]\n",
    "            \n",
    "            loss = F.mse_loss(image[:, :3], target_image.unsqueeze(0).expand(image.shape[0], -1, -1, -1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "    \n",
    "            history[\"loss\"].append(float(loss))\n",
    "            lr = scheduler.get_last_lr()\n",
    "            if isinstance(lr, (list, tuple)):\n",
    "                lr = lr[0]\n",
    "            history[\"lr\"].append(lr)\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "                images.append(image[0])\n",
    "\n",
    "            if len(images) >= 10:\n",
    "                display(VF.to_pil_image(make_grid(images, nrow=len(images))))\n",
    "                images.clear()\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    display(px.line(pd.DataFrame(history)))\n",
    "    \n",
    "model = NeuralCA()\n",
    "train_image(\n",
    "    model,\n",
    "    #\"~/Pictures/kali.png\",\n",
    "    \"~/Pictures/matt.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0bea4d-3e98-454d-bc7f-c7f57f0cd038",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    display(VF.to_pil_image(resize(\n",
    "        model.generate((64, 64))[0]\n",
    "    , 4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
