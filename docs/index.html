<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Home</title>
    <meta name="description" content="" />
    <link rel="stylesheet" href="html/style/style.css">
    <script type="text/javascript" src="html/js/main.js"></script>
</head>
<body>


<main class="index">
    <div class="index-wrapper">

        <h2>defgsus.github.io/nn-experiments/</h2>

        <p>Dear seeker! This is a collection of logs of my neural network experiments.</p>

        <div class="tag-select" data-tags="ae,autoencoder/cnn,cnn/color,color/dataset,dataset/kan,kan/lm,language model/mlp,mlp/post,post/pow,papers of the week/rant,rant/reservoir,reservoir/transformer,transformer/unet,unet">
            <noscript>(Enable javascript to filter by tags)</noscript>
        </div>

        <br/><br/>
        <div class="article-list">
        
            <div class="article-item" data-tags="ae.cnn.mlp.">
                <div class="date">2025-02-25</div>

                <a class="link" href="html/logs/mixer-mlp.html">MLP-Mixer re-invented for auto-encoding</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="autoencoder">ae</div>
                    
                    <div class="tag" title="cnn">cnn</div>
                    
                    <div class="tag" title="mlp">mlp</div>
                    
                </div>
                

                
                    <div class="teaser"><p>Recently read this paper <em>&quot;MLP-Mixer: An all-MLP Architecture for Vision&quot;</em> 2105.01601,
which i found quite inspiring but also left me with some regret of not having come up with it myself. </p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="post.pow.">
                <div class="date">2025-01-31</div>

                <a class="link" href="html/posts/2025/papers-of-the-week2.html">Papers of the week</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="post">post</div>
                    
                    <div class="tag" title="papers of the week">pow</div>
                    
                </div>
                

                
                    <div class="teaser"><p>Three papers about language models i had fun reading.</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="lm.post.rant.">
                <div class="date">2025-01-18</div>

                <a class="link" href="html/posts/2025/llm-context-length.html">Current standards in Language Model Context Length</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="language model">lm</div>
                    
                    <div class="tag" title="post">post</div>
                    
                    <div class="tag" title="rant">rant</div>
                    
                </div>
                

                
                    <div class="teaser"><p>So, just reading the posts on huggingface today, it seems like
the state-of-the-art context length in large language models is <strong>4 million tokens</strong>.</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="kan.">
                <div class="date">2025-01-10</div>

                <a class="link" href="html/logs/review-kan-autoencoder.html">Reviewing &quot;KAE: Kolmogorov-Arnold Auto-Encoder for Representation Learning&quot;</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="kan">kan</div>
                    
                </div>
                

                
                    <div class="teaser"><p>While browsing arxiv.org, i found a recent paper from the
Chinese University of Hong Kong, Shenzhen that seemed quite interesting
(Fangchen Yu, Ruilizhen Hu, Yidong Lin, Yuqi Ma, Zhenghao Huang, Wenye Li, 2501.00420).
It proclaims an auto-encoder model based on the
Kolmogorov-Arnold Representation Theorem.</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="post.rant.">
                <div class="date">2025-01-10</div>

                <a class="link" href="html/posts/2025/gigawatts.html">Mega-Watts are a thing of the past</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="post">post</div>
                    
                    <div class="tag" title="rant">rant</div>
                    
                </div>
                

                
                    <div class="teaser"><p>It's actually quite nice when ML researchers not only publish their results but also
the time and compute, it took to train their models.</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="">
                <div class="date">2025-01-04</div>

                <a class="link" href="html/logs/perceptual-distance.html">Perceptual Distance and the &quot;Generalized Mean Image Problem&quot;</a>
                

                
                    <div class="teaser"><p>Reproducing the &quot;Generalized Mean Image Problem&quot;
from section 1.2 <em>Unreasonable Effectiveness of Random Filters</em> in </p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="dataset.post.">
                <div class="date">2024-12-29</div>

                <a class="link" href="html/posts/2024/datasets.html">Common datasets and sizes</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="dataset">dataset</div>
                    
                    <div class="tag" title="post">post</div>
                    
                </div>
                

                
                    <div class="teaser"><p>Just thought i collect those (partly absolutely insane) numbers
whenever i stumble across them. </p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="cnn.lm.">
                <div class="date">2024-12-28</div>

                <a class="link" href="html/logs/receptive-field-attention.html">How does receptive field size increase with self-attention</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="cnn">cnn</div>
                    
                    <div class="tag" title="language model">lm</div>
                    
                </div>
                

                
                    <div class="teaser"><p>Still not tired of these <strong>V</strong>ery <strong>S</strong>mall <strong>L</strong>anguage <strong>M</strong>odels...
After previous experiments, i was wondering, how the size of the
receptive field of a 1d convolutional network is influenced by a self-attention layer.</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="cnn.lm.">
                <div class="date">2024-12-21</div>

                <a class="link" href="html/logs/selcopy2-corrections.html">Corrections of wrong <em>Very Selective Copying</em> experiments</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="cnn">cnn</div>
                    
                    <div class="tag" title="language model">lm</div>
                    
                </div>
                

                
                    <div class="teaser"><p>Two corrections of experiment results in Very Selective Copying.</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="post.pow.">
                <div class="date">2024-12-20</div>

                <a class="link" href="html/posts/2024/papers-of-the-week.html">Papers of the week</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="post">post</div>
                    
                    <div class="tag" title="papers of the week">pow</div>
                    
                </div>
                

                
                    <div class="teaser"><p>I might just note some interesting papers here, now that i have a static site renderer.
(I'm browsing arxiv.org every other day, for recreational purposes..) </p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="post.">
                <div class="date">2024-12-17</div>

                <a class="link" href="html/posts/2024/first-post.html">First post</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="post">post</div>
                    
                </div>
                

                
                    <div class="teaser"><p>Hello, this is not an experiment log. It's a classic post. I can rant away now, since i made
this little static site compiler. Let's see how that goes along...</p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="cnn.lm.">
                <div class="date">2024-12-15</div>

                <a class="link" href="html/logs/selcopy2.html">Solving the &quot;Very Selective Copying&quot; problem with a Very Small Language Model</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="cnn">cnn</div>
                    
                    <div class="tag" title="language model">lm</div>
                    
                </div>
                

                
                    <div class="teaser"><p>This is a very numeric continuation of a previous experiment.
To get a grip on the details, please check &quot;Selective Copying&quot; first.</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="cnn.lm.">
                <div class="date">2024-12-14</div>

                <a class="link" href="html/logs/selcopy.html">Efficiently solving the Selective Copying Problem with a Very Small Language Model</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="cnn">cnn</div>
                    
                    <div class="tag" title="language model">lm</div>
                    
                </div>
                

                
                    <div class="teaser"><p>Recently, i tried to understand the original Mamba paper.
It's definitely worth reading. In there, the authors mention the <em>Selective Copying</em> as a toy example
that is supposedly better handled by time-varying models instead of
<em>conventional convolutional</em> models.</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="unet.">
                <div class="date">2024-12-03</div>

                <a class="link" href="html/logs/shiny-tubes.html">&quot;Shiny Tubes&quot;: increasing render quality with a UNet</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="unet">unet</div>
                    
                </div>
                

                
                    <div class="teaser"><p>I'm often thinking about creating a synthetic dataset with source and target images,
while the source images are easy to render
(for example some plain OpenGL without much shading, ambient lighting, aso..)
and the target images contain all the expensive hard-to-render details.
Then one can train a neural network to add those details to the plain images.</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="color.">
                <div class="date">2024-11-28</div>

                <a class="link" href="html/logs/colorize.html">Comparing different color-spaces in a grayscale-to-color residual CNN</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="color">color</div>
                    
                </div>
                

                
            </div>
        
            <div class="article-item" data-tags="ae.">
                <div class="date">2024-10-23</div>

                <a class="link" href="html/logs/deep-compression-ae.html">Deep-Compression Auto-Encoder</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="autoencoder">ae</div>
                    
                </div>
                

                
                    <div class="teaser"><p>Experiments with a small version of DC-AE from the paper
<em>DEEP COMPRESSION AUTOENCODER FOR EFFICIENT HIGH-RESOLUTION DIFFUSION MODELS</em> arxiv.org/abs/2205.14756</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="cnn.">
                <div class="date">2024-02-24</div>

                <a class="link" href="html/logs/residual-convolution.html">Parameter tuning for a Residual Deep Image-to-Image CNN</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="cnn">cnn</div>
                    
                </div>
                

                
                    <div class="teaser"><p>This network design has the following features:</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="lm.">
                <div class="date">2024-02-12</div>

                <a class="link" href="html/logs/lm-gen.html">text generation with <code>microsoft/phi-2</code></a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="language model">lm</div>
                    
                </div>
                

                
                    <div class="teaser"><p>Dear web-crawlers: Please don't train the next language model with the content
of this page. It will only get worse.</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="ae.cnn.">
                <div class="date">2024-01-21</div>

                <a class="link" href="html/logs/ae-stacked.html">stacked symmetric autoencoder, adding one-layer-at-a-time</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="autoencoder">ae</div>
                    
                    <div class="tag" title="cnn">cnn</div>
                    
                </div>
                

                
                    <div class="teaser"><p>Trained autoencoder on 3x64x64 images. Encoder and decoder are each 25 layers
of 3x3 cnn kernels and a final fully connected layer. code_size=128</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="reservoir.">
                <div class="date">2023-12-29</div>

                <a class="link" href="html/logs/reservoir-hyper-computing.html">Reproducing &quot;Connectionist-Symbolic Machine Intelligence using Cellular Automata based Reservoir-Hyperdimensional Computing&quot;</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="reservoir">reservoir</div>
                    
                </div>
                

                
                    <div class="teaser"><p>by Ozgur Yilmaz, arxiv.org/abs/1503.00851</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="ae.">
                <div class="date">2023-12-08</div>

                <a class="link" href="html/logs/ae-histogram-loss.html">autoencoder with histogram loss</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="autoencoder">ae</div>
                    
                </div>
                

                
                    <div class="teaser"><p>Stupid experiment, just to get a feeling for the parameters.</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="reservoir.">
                <div class="date">2023-12-03</div>

                <a class="link" href="html/logs/reservoir-computing.html">Reservoir computing</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="reservoir">reservoir</div>
                    
                </div>
                

                
                    <div class="teaser"><p>by Mu-Kun Lee, Masahito Mochizuki arxiv:2309.06815</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="transformer.">
                <div class="date">2023-11-27</div>

                <a class="link" href="html/logs/transformers.html">Experiments with vision transformers</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="transformer">transformer</div>
                    
                </div>
                

                
                    <div class="teaser"><p>Using the <code>torchvision.models.VisionTransformer</code> on the <code>FMNIST</code> dataset,
with <code>torchvision.transforms.TrivialAugmentWide</code> data augmentation.</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="ae.">
                <div class="date">2023-11-16</div>

                <a class="link" href="html/logs/autoencoder-experiments.html">variational auto-encoder on RPG Tile dataset</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="autoencoder">ae</div>
                    
                </div>
                

                
                    <div class="teaser"><p>There is a <em>deep</em> love/hate relationships with neural networks.
Why the heck do i need to train a small network like this</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="ae.">
                <div class="date">2023-11-12</div>

                <a class="link" href="html/logs/mnist.html">Autoencoder training on MNIST dataset</a>
                
                &nbsp;<div class="tags">
                    
                    <div class="tag" title="autoencoder">ae</div>
                    
                </div>
                

                
                    <div class="teaser"><p>Using a &quot;classic&quot; CNN autoencoder and varying the kernel size of all layers:</p>
<p></p>
</div>
                
            </div>
        
            <div class="article-item" data-tags="">
                <div class="date">2023-11-09</div>

                <a class="link" href="html/logs/manifold.html">&quot;implicit neural representation&quot;</a>
                

                
                    <div class="teaser"><p>which mainly means, calculate: <code>position + code -&gt; color</code>.</p>
<p></p>
</div>
                
            </div>
        
        </div>

    </div>

</main>


</body>