<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>autoencoder with histogram loss</title>
    <meta name="description" content="" />
    <link rel="stylesheet" href="../../html/style/style-6680f4a6.css">
    <script type="text/javascript" src="../../html/js/main-63fab8db.js"></script>
</head>
<body>


<main class="article">
    <div class="article-left">
        <h3><a href="../../index.html">&lt;&lt; nn-experiments</a></h3>
        <ul>
            
            
            <li class="indent-1"><a href="#autoencoder-with-histogram-loss" title="autoencoder with histogram loss">autoencoder with histogram loss</a></li>
            
            
        </ul>
    </div>

    <div class="article-mid">

        <div class="show-when-small">
            <a href="../../index.html">&lt;&lt; nn-experiments</a></h3>
        </div>

        <h1 id="autoencoder-with-histogram-loss">autoencoder with histogram loss <a href="#autoencoder-with-histogram-loss" class="heading-linker">‚Üê</a></h1>
<p>Stupid experiment, just to get a feeling for the parameters.</p>
<p>Basically a simple autoencoder but the loss only considers the histogram
using the <em>soft histogram</em> mentioned by Tony-Y in the
<a href="https://discuss.pytorch.org/t/differentiable-torch-histc/25865/4" target="_blank">pytorch forum</a>.</p>
<div style="overflow: scroll;"><div class="highlight"><pre><span></span><span class="nt">matrix</span><span class="p">:</span>
<span class="w">  </span><span class="nt">bins</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">100</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">200</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">50</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">sigma</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">100</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">200</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">50</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">norm</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">False</span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">loss</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;&#39;l1&#39;&quot;</span><span class="p p-Indicator">]</span>

<span class="nt">experiment_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ae/hist/hl-${matrix_slug}</span>

<span class="nt">trainer</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">experiments.ae.trainer.TrainAutoencoderSpecial</span>

<span class="nt">train_set</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">  </span><span class="no">rpg_tile_dataset_3x32x32(SHAPE, validation=False)</span>

<span class="nt">validation_set</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">  </span><span class="no">rpg_tile_dataset_3x32x32(SHAPE, validation=True)</span>

<span class="nt">batch_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">64</span>
<span class="nt">learnrate</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.0003</span>
<span class="nt">optimizer</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">AdamW</span>
<span class="nt">scheduler</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CosineAnnealingLR</span>
<span class="nt">loss_function</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">  </span><span class="no">HistogramLoss(${bins}, 0., 1., sigma=${sigma}, loss=${loss}, normalize=${norm})</span>
<span class="nt">max_inputs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">500_000</span>

<span class="nt">globals</span><span class="p">:</span>
<span class="w">  </span><span class="nt">SHAPE</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">(3, 32, 32)</span>
<span class="w">  </span><span class="nt">CODE_SIZE</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">128</span>

<span class="nt">model</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">  </span><span class="no">encoder = EncoderConv2d(SHAPE, code_size=CODE_SIZE, channels=(24, 32, 48), kernel_size=5)</span>

<span class="w">  </span><span class="no">encoded_shape = encoder.convolution.get_output_shape(SHAPE)</span>
<span class="w">  </span><span class="no">decoder = nn.Sequential(</span>
<span class="w">      </span><span class="no">nn.Linear(CODE_SIZE, math.prod(encoded_shape)),</span>
<span class="w">      </span><span class="no">Reshape(encoded_shape),</span>
<span class="w">      </span><span class="no">encoder.convolution.create_transposed(act_last_layer=False),</span>
<span class="w">  </span><span class="no">)</span>

<span class="w">  </span><span class="no">EncoderDecoder(encoder, decoder)</span>
</pre></div>
</div><p><div style="overflow: scroll;"><img src="../../logs/img/ae-histogramloss.png" alt="loss plots" /></div></p>
<p>Normalizing the histograms before calculating the difference did not
converge well. And reproduction look terrible as could be expected: </p>
<div style="overflow: scroll;"><table>
<thead>
<tr>
<th>green</th>
<th>yellow</th>
</tr>
</thead>
<tbody>
<tr>
<td><div style="overflow: scroll;"><img src="../../logs/img/ae-histogramloss-repro-green.png" alt="repro" /></div></td>
<td><div style="overflow: scroll;"><img src="../../logs/img/ae-histogramloss-repro-yellow.png" alt="repro" /></div></td>
</tr>
</tbody></table></div><div style="overflow: scroll;"><table>
<thead>
<tr>
<th>purple</th>
<th>gray</th>
</tr>
</thead>
<tbody>
<tr>
<td><div style="overflow: scroll;"><img src="../../logs/img/ae-histogramloss-repro-purple.png" alt="repro" /></div></td>
<td><div style="overflow: scroll;"><img src="../../logs/img/ae-histogramloss-repro-gray.png" alt="repro" /></div></td>
</tr>
</tbody></table></div>

        <!-- article footer -->
        <div class="flex article-footer">
            <div>
                 <a target="_blank" href="https://github.com/defgsus/nn-experiments/issues">Leave a comment</a>
            </div>

            <div class="flex-grow"></div>

            <div>
                Edit on <a target="_blank" href="https://github.com/defgsus/nn-experiments/blob/master/docs/logs/2023-12-08-ae-histogram-loss.md">github</a>
            </div>
        </div>

        <div class="flex article-footer">
            <div>
                
                    <a href="../../html/logs/reservoir-computing.html">
                        &lt;&lt; Reservoir computing
                    </a>
                
            </div>

            <div class="flex-grow"></div>

            <div>
                
                <a href="../../html/logs/reservoir-hyper-computing.html">
                    Reproducing &quot;Connectionist-Symbolic Machine Intelligence using Cellular Automata based Reservoir-Hyperdimensional Computing&quot; &gt;&gt;
                </a>
                
            </div>
        </div>
    </div>

</main>


</body>