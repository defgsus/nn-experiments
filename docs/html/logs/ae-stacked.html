<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>stacked symmetric autoencoder, adding one-layer-at-a-time</title>
    <meta name="description" content="" />
    <link rel="stylesheet" href="../../html/style/style.css">
    
</head>
<body>


<main class="article">
    <div class="article-left">
        <h3><a href="../../index.html">&lt;&lt; nn-experiments</a></h3>
        <ul>
            
            
            <li class="indent-1"><a href="#stacked-symmetric-autoencoder-adding-one-layer-at-a-time" title="stacked symmetric autoencoder, adding one-layer-at-a-time">stacked symmetric autoencoder, adding one-layer-at-a-time</a></li>
            
            
        </ul>
    </div>

    <div class="article-mid">

        <div class="show-when-small">
            <a href="../../index.html">&lt;&lt; nn-experiments</a></h3>
        </div>

        <h1 id="stacked-symmetric-autoencoder-adding-one-layer-at-a-time">stacked symmetric autoencoder, adding one-layer-at-a-time <a href="#stacked-symmetric-autoencoder-adding-one-layer-at-a-time" class="heading-linker">‚Üê</a></h1>
<p>Trained autoencoder on 3x64x64 images. Encoder and decoder are each 25 layers
of 3x3 cnn kernels and a final fully connected layer. code_size=128</p>
<p>In training, every N input steps the number of used layers is increased. First,
the autoencoder only uses first encoder and last decoder layer. That way,
it's possible to train the whole 25 layers after a time. It's just not good:</p>
<p><div style="overflow: scroll;"><img src="../../logs/img/ae-stacked-64.png" alt="loss plots" /></div></p>
<ul>
<li><strong>white</strong>: reference baseline cnn (using space-to-depth (or pixel-unshuffle))</li>
<li><strong>cyan</strong>: 32-chan 25 layer cnn, next layer activated every 47,000 steps (1 epoch)</li>
<li><strong>yellow</strong>: 32-chan 25 layer cnn, next layer activated every 8,000 steps</li>
<li><strong>purple</strong>: 128-chan 25 layer cnn, next layer activated every 8,000 steps</li>
</ul>
<p>Also compared symmetric vs. non-symmetric. Symmetric means the convolutions
and fully connected layer use <strong>the same</strong> weights for encoding and decoding.
symmetric is half the number of parameters for the autoencoder and performs only
slightly below non-symmetric. The biases are not shared between encoder and decoder.</p>


        <!-- article footer -->
        <div class="flex article-footer">
            <div>
                 <a target="_blank" href="https://github.com/defgsus/nn-experiments/issues">Leave a comment</a>
            </div>

            <div class="flex-grow"></div>

            <div>
                Edit on <a target="_blank" href="https://github.com/defgsus/nn-experiments/blob/master/docs/logs/2024-01-21-ae-stacked.md">github</a>
            </div>
        </div>

        <div class="flex article-footer">
            <div>
                
                    <a href="../../html/logs/reservoir-hyper-computing.html">
                        &lt;&lt; Reproducing &quot;Connectionist-Symbolic Machine Intelligence using Cellular Automata based Reservoir-Hyperdimensional Computing&quot;
                    </a>
                
            </div>

            <div class="flex-grow"></div>

            <div>
                
                <a href="../../html/logs/lm-gen.html">
                    text generation with <code>microsoft/phi-2</code> &gt;&gt;
                </a>
                
            </div>
        </div>
    </div>

</main>


</body>