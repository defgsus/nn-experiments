<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Solving the &quot;Very Selective Copying&quot; problem with a Very Small Language Model</title>
    <meta name="description" content="" />
    <link rel="stylesheet" href="../../html/style/style.css">
    
</head>
<body>


<main class="article">
    <div class="article-left">
        <h3><a href="../../index.html">&lt;&lt; nn-experiments</a></h3>
        <ul>
            
            
            <li class="indent-1"><a href="#solving-the-quotvery-selective-copyingquot-problem-with-a-very-small-language-model" title="Solving the &quot;Very Selective Copying&quot; problem with a Very Small Language Model">Solving the &quot;Very Selective Copying&quot; problem with a Very Small Language Model</a></li>
            
            
            
            <li class="indent-3"><a href="#preliminary-tests" title="Preliminary tests">Preliminary tests</a></li>
            
            
            
            
            
            
            
            <li class="indent-3"><a href="#number-of-operations-versus-number-of-layers" title="Number of operations versus number of layers">Number of operations versus number of layers</a></li>
            
            
            
            
            
            
            
            
            
            <li class="indent-3"><a href="#quick-comparison-with-mamba-and-lstm" title="Quick comparison with Mamba and LSTM">Quick comparison with Mamba and LSTM</a></li>
            
            
            
            <li class="indent-3"><a href="#attention-please" title="Attention, please!">Attention, please!</a></li>
            
            
            
            
            
            
            
            
            
            <li class="indent-3"><a href="#compare-attention-activation-function" title="Compare attention activation function">Compare attention activation function</a></li>
            
            
            
            <li class="indent-3"><a href="#compare-position-of-self-attention-in-3-layers-network" title="Compare position of self-attention in 3-layers network">Compare position of self-attention in 3-layers network</a></li>
            
            
            
            <li class="indent-3"><a href="#compare-position-of-self-attention-in-5-layers-network" title="Compare position of self-attention in 5-layers network">Compare position of self-attention in 5-layers network</a></li>
            
            
            
            <li class="indent-3"><a href="#6-layers" title="6-layers">6-layers</a></li>
            
            
        </ul>
    </div>

    <div class="article-mid">

        <div class="show-when-small">
            <a href="../../index.html">&lt;&lt; nn-experiments</a></h3>
        </div>

        <h1 id="solving-the-quotvery-selective-copyingquot-problem-with-a-very-small-language-model">Solving the &quot;Very Selective Copying&quot; problem with a Very Small Language Model <a href="#solving-the-quotvery-selective-copyingquot-problem-with-a-very-small-language-model" class="heading-linker">←</a></h1>
<p>This is a very numeric continuation of a previous experiment.
To get a grip on the details, please check <a href="../../html/logs/selcopy.html">&quot;Selective Copying&quot;</a> first.</p>
<p>Basically, it's a synthetic question-answer dataset that requires some <em>&quot;computational&quot;</em> skill.
See, if you can find out the rule by yourself:</p>
<div style="overflow: scroll;"><pre><code>NA: 2&gt;1: AN   
DQ: -1: Q    
EJWHB: 3&gt;5: EJBHW
ULHP: 3&gt;4, 2&gt;1: LUPH 
YNP: 3&gt;1, -3, 2&gt;1: NP   
EJACQ: 1&gt;2, -1, +3: EAJCQ
YESBR: 3&gt;5, 5&gt;1, 1&gt;5, 3&gt;4: YEBRS
UXMP: -2, -3, 1&gt;2, +2: MPU  
</code></pre>
</div><p><strong>Spoiler</strong>: First few letters are the program input, the numbers and operators are the program
and answer, after the second colon, is the result of the program. </p>
<ul>
<li><code>x&gt;y</code> means: exchange <code>x</code>th item with <code>y</code>th item. </li>
<li><code>-x</code> means: remove <code>x</code>th item from memory and put it onto the stack. </li>
<li><code>+x</code> means: pop last item from stack and insert at position <code>x</code></li>
</ul>
<p>I argue, that this dataset</p>
<ul>
<li>requires a large enough <em>receptive field</em></li>
<li>requires some cognitional ability to build up a history and to use it subsequently. </li>
<li>should actually be called <em>Selective Swapping</em> </li>
</ul>
<h3 id="preliminary-tests">Preliminary tests <a href="#preliminary-tests" class="heading-linker">←</a></h3>
<p>Just to get a feeling for the dataset, i took the best model from the
<a href="../../html/logs/selcopy.html">&quot;Selective Copying&quot;</a> experiment with a few parameter variations
and ran it on questions that have an input length of 2 to 5 items and 1 to 5 operations:</p>
<p><div style="overflow: scroll;"><img src="../../logs/img/selcopy2/selcopy2_mask-error_l6-12-18.png" alt="error curves" /></div></p>
<p>Training now takes 4 million steps, and could even be a bit longer. But generally, the loss curves
seem to converge at about there. </p>
<div style="overflow: scroll;"><table>
<thead>
<tr>
<th align="right">nitem</th>
<th align="left">len</th>
<th align="left">nops</th>
<th align="right">l</th>
<th align="right">ch</th>
<th align="right">ks</th>
<th align="left">dil</th>
<th align="right">validation loss</th>
<th align="right">validation_mask_error%</th>
<th align="right">model params</th>
<th align="right">train time (minutes)</th>
<th align="right">throughput</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">26</td>
<td align="left">2,5</td>
<td align="left">1,5</td>
<td align="right">6</td>
<td align="right">64</td>
<td align="right">9</td>
<td align="left">1,2,3,4,5,1</td>
<td align="right">0.0525558</td>
<td align="right">17.5876</td>
<td align="right">237,952</td>
<td align="right">15.30</td>
<td align="right">4,358/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="left">2,5</td>
<td align="left">1,5</td>
<td align="right">6</td>
<td align="right">128</td>
<td align="right">9</td>
<td align="left">1,2,3,4,5,1</td>
<td align="right">0.0365154</td>
<td align="right">11.8352</td>
<td align="right">918,272</td>
<td align="right">10.76</td>
<td align="right">6,195/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="left">2,5</td>
<td align="left">1,5</td>
<td align="right">12</td>
<td align="right">64</td>
<td align="right">9</td>
<td align="left">1,1,2,2,3,3,4,4,5,5,1,1</td>
<td align="right">0.0076271</td>
<td align="right">2.0063</td>
<td align="right">459,520</td>
<td align="right">27.05</td>
<td align="right">2,464/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="left">2,5</td>
<td align="left">1,5</td>
<td align="right">18</td>
<td align="right">64</td>
<td align="right">9</td>
<td align="left">1,1,1,2,2,2,3,3,3,4,4,4,5,5,5,1,1,1</td>
<td align="right">0.0028902</td>
<td align="right">0.7583</td>
<td align="right">681,088</td>
<td align="right">39.80</td>
<td align="right">1,675/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="left">2,5</td>
<td align="left">1,5</td>
<td align="right">12</td>
<td align="right">128</td>
<td align="right">9</td>
<td align="left">1,1,2,2,3,3,4,4,5,5,1,1</td>
<td align="right">0.0028549</td>
<td align="right">0.7205</td>
<td align="right">1,803,776</td>
<td align="right">18.81</td>
<td align="right">3,543/s</td>
</tr>
</tbody></table></div><p>I'm using dilation all the time, because 1.) it's required for the receptive field of the
6-layer network and 2.) because it runs faster. For the 12 and 18-layer networks i just expanded the
dilation settings without much thinking about it. (Actually i did think quite a bit
about it but decided to evaluate good 12 or 18-layer dilation settings another time)</p>
<p>Here are examples from the validation set from the worst and the best performing network:</p>
<h4 id="6-layer32-chan-validation-mask-error-176">6-layer/32-chan: validation mask error 17.6% <a href="#6-layer32-chan-validation-mask-error-176" class="heading-linker">←</a></h4>
<p><div style="overflow: scroll;"><img src="../../logs/img/selcopy2/selcopy2_validation-example_nops1-5_l6-ch64.png" alt="validation example output" /></div></p>
<p>It is noticeable that long programs seem to be the problem for this network.</p>
<h4 id="12-layer128-chan-validation-mask-error-07">12-layer/128-chan: validation mask error 0.7% <a href="#12-layer128-chan-validation-mask-error-07" class="heading-linker">←</a></h4>
<p><div style="overflow: scroll;"><img src="../../logs/img/selcopy2/selcopy2_validation-example_nops1-5_l12-ch128.png" alt="validation example output" /></div></p>
<p>Wow! Everything correct, even the long ones!</p>
<h3 id="number-of-operations-versus-number-of-layers">Number of operations versus number of layers <a href="#number-of-operations-versus-number-of-layers" class="heading-linker">←</a></h3>
<p>The above example is a bit messy because it has a variable number of operations and
we don't know how many answers are fully correct, only how many characters.
So first, we add a new evaluation metric <code>validation_sample_error%</code>. While the mask error gives
the percentage of wrong characters within the mask area, the sample error gives the percentage
of wrong answers, even if only one character is wrong.</p>
<p>Further, to better evaluate the relationship between number of operations (length of the &quot;program&quot;)
and the number of layers in the network, i set a fixed number of operations.</p>
<p>I also dropped the stack operations (<code>+</code>/<code>-</code>) for now and made it all a bit tighter because
i'm not interested in the influence of the receptive field in this experiment.
Example data with 5 operations looks like this:</p>
<div style="overflow: scroll;"><pre><code>NPFKT:5&gt;4 3&gt;1 1&gt;4 4&gt;2 1&gt;3:NFTPK
UKLRM:5&gt;1 4&gt;2 1&gt;4 2&gt;4 3&gt;2:KLMRU
LCEWI:3&gt;5 3&gt;2 4&gt;2 3&gt;1 3&gt;5:CWEIL
LRUPX:1&gt;4 3&gt;2 5&gt;4 1&gt;3 2&gt;1:URPXL
UQJOR:5&gt;4 2&gt;5 5&gt;4 4&gt;3 2&gt;5:URQJO
BYGMJ:5&gt;4 4&gt;3 2&gt;3 5&gt;2 5&gt;3:BMJGY
PBNFM:3&gt;5 5&gt;1 1&gt;3 2&gt;3 5&gt;3:MNPFB
TNEOL:1&gt;3 4&gt;1 4&gt;1 4&gt;3 5&gt;3:ENLTO
ALEVT:1&gt;2 2&gt;1 2&gt;1 4&gt;2 4&gt;3:LVAET
UIZNQ:3&gt;4 2&gt;3 5&gt;3 4&gt;1 1&gt;5:INQUZ
</code></pre>
</div><p>Running the networks on 2, 3, 4 and 5 operations per question:</p>
<h4 id="6-layer">6-layer <a href="#6-layer" class="heading-linker">←</a></h4>
<p><div style="overflow: scroll;"><img src="../../logs/img/selcopy2/selcopy2_error-curves_l6-nops-2-5.png" alt="error curves" /></div></p>
<h4 id="12-layer">12-layer <a href="#12-layer" class="heading-linker">←</a></h4>
<p><div style="overflow: scroll;"><img src="../../logs/img/selcopy2/selcopy2_error-curves_l12-nops-2-5.png" alt="error curves" /></div></p>
<h4 id="18-layer">18-layer <a href="#18-layer" class="heading-linker">←</a></h4>
<p><div style="overflow: scroll;"><img src="../../logs/img/selcopy2/selcopy2_error-curves_l18-nops-2-5.png" alt="error curves" /></div></p>
<p>And here is it all together in a table. The dilation settings are left out for readability.
They are <code>1,2,3,4,5,1</code> for the 6-layer, <code>1,2,3,4,5,1,2,3,4,5,1,1</code> for the 12-layer
and <code>1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,1,1,1</code> for the 18-layer network.</p>
<div style="overflow: scroll;"><table>
<thead>
<tr>
<th align="right">nitem</th>
<th align="right">len</th>
<th align="right">nops</th>
<th align="right">l</th>
<th align="right">ch</th>
<th align="right">ks</th>
<th align="right">validation loss</th>
<th align="right">validation_mask_error%</th>
<th align="right">validation_sample_error%</th>
<th align="right">model params</th>
<th align="right">train time (minutes)</th>
<th align="right">throughput</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">26</td>
<td align="right">5</td>
<td align="right"><strong>5</strong></td>
<td align="right">6</td>
<td align="right">64</td>
<td align="right">9</td>
<td align="right">0.23663</td>
<td align="right">71.9705</td>
<td align="right">99.9104</td>
<td align="right">237,952</td>
<td align="right">58.04</td>
<td align="right">1,145/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="right">5</td>
<td align="right"><strong>5</strong></td>
<td align="right">12</td>
<td align="right">64</td>
<td align="right">9</td>
<td align="right">0.208893</td>
<td align="right">63.6883</td>
<td align="right">99.4327</td>
<td align="right">459,520</td>
<td align="right">40.0</td>
<td align="right">1,660/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="right">5</td>
<td align="right"><strong>5</strong></td>
<td align="right">18</td>
<td align="right">64</td>
<td align="right">9</td>
<td align="right">0.163338</td>
<td align="right">50.2667</td>
<td align="right">98.547</td>
<td align="right">681,088</td>
<td align="right">99.0</td>
<td align="right">670/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="right">6</td>
<td align="right">64</td>
<td align="right">9</td>
<td align="right">0.185987</td>
<td align="right">48.7739</td>
<td align="right">98.5967</td>
<td align="right">237,952</td>
<td align="right">15.1</td>
<td align="right">4,413/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="right">12</td>
<td align="right">64</td>
<td align="right">9</td>
<td align="right">0.0415355</td>
<td align="right">8.8953</td>
<td align="right">37.8682</td>
<td align="right">459,520</td>
<td align="right">26.03</td>
<td align="right">2,561/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="right">5</td>
<td align="right">3</td>
<td align="right">6</td>
<td align="right">64</td>
<td align="right">9</td>
<td align="right">0.00458625</td>
<td align="right">0.559315</td>
<td align="right">2.75677</td>
<td align="right">237,952</td>
<td align="right">7.32</td>
<td align="right">9,109/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="right">18</td>
<td align="right">64</td>
<td align="right">9</td>
<td align="right">0.000329348</td>
<td align="right">0.0378185</td>
<td align="right">0.189092</td>
<td align="right">681,088</td>
<td align="right">37.13</td>
<td align="right">1,795/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="right">5</td>
<td align="right">3</td>
<td align="right">18</td>
<td align="right">64</td>
<td align="right">9</td>
<td align="right">1.86865e-06</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">681,088</td>
<td align="right">5.07</td>
<td align="right">13,139/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="right">5</td>
<td align="right">3</td>
<td align="right">12</td>
<td align="right">64</td>
<td align="right">9</td>
<td align="right">3.1804e-06</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">459,520</td>
<td align="right">11.5</td>
<td align="right">5,796/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="right">5</td>
<td align="right">2</td>
<td align="right">18</td>
<td align="right">64</td>
<td align="right">9</td>
<td align="right">4.39884e-07</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">681,088</td>
<td align="right">15.3</td>
<td align="right">4,357/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="right">5</td>
<td align="right">2</td>
<td align="right">6</td>
<td align="right">64</td>
<td align="right">9</td>
<td align="right">5.3105e-07</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">237,952</td>
<td align="right">7.15</td>
<td align="right">9,319/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="right">5</td>
<td align="right">2</td>
<td align="right">12</td>
<td align="right">64</td>
<td align="right">9</td>
<td align="right">1.0319e-06</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">459,520</td>
<td align="right">11.3</td>
<td align="right">5,900/s</td>
</tr>
</tbody></table></div><p>Something is obviously very strange. None of the networks can handle 5 operations.
But some of them just did in the last experiment??</p>
<p>What i really like about this neural net research is that one often gets along with intuition
only. We simply don't know enough until all tests are made, which probably never happens.
However, in this case, i would argue that a 5 operations sequence is
just too long for the network to learn the task in reasonable time. Put yourself into
the eyes of the model. The training algorithm constantly tells you:</p>
<blockquote>
<p>If you see this: <code>PHMTW:5&gt;3 2&gt;3 5&gt;4 5&gt;3 1&gt;3:?????</code>
you should output that: <code>PHMTW:5&gt;3 2&gt;3 5&gt;4 5&gt;3 1&gt;3:TWPMH</code></p>
</blockquote>
<p>on and on ... for millions of examples. Now, for a human, maybe a programmer even, this should
be solvable pretty quick. Once you have seen 10 examples, you can be sure what the
algorithm is. But for a neural network trained with <em>stochastic gradient descend</em> it is basically
a <em>brute-force</em> approach. It's like: Hey network, you gotta lot of parameters, that's fine,
now if you see this X, and you should output that Y, there is a chance that inreasing this one
parameter while decreasing that other one would help you to output the correct answer, the
next time you see the same question. And on and on...</p>
<p>Once again, reasoning from intuition, the network can probably learn the task much easier when it has
some easier examples in the dataset. Schools and such also don't teach children algebra by
starting with: <em>What is the answer to 7 + 4 + 9 + 2 + 6 = ??</em></p>
<p>Following, the training set contains questions with 2 to 5 operations, e.g.:</p>
<div style="overflow: scroll;"><pre><code>HGOKX:3&gt;1 5&gt;4:OGHXK
CXZMN:5&gt;4 2&gt;5:CMZNX
RMYTC:2&gt;1 4&gt;5 4&gt;2:MCYRT
DNAHL:1&gt;3 3&gt;4 1&gt;2 4&gt;1 3&gt;2:DHANL
HSBQA:4&gt;5 4&gt;5:HSBQA
DTFKS:4&gt;5 3&gt;2 4&gt;5 5&gt;1:SFTKD
UXGJN:4&gt;2 1&gt;2 1&gt;4 3&gt;1 3&gt;2:GXUJN
</code></pre>
</div><p>and the validation set still holds the same 5-operations-only questions for which all networks have
failed previously.</p>
<p>Here's the loss and error curves of the 18-layer network trained with 5 and 2-5 operations:</p>
<p><div style="overflow: scroll;"><img src="../../logs/img/selcopy2/selcopy2_error-curves_l18-nops-2-5-vnops5.png" alt="error curves" /></div></p>
<p>The bright one is the brighter one. Indeed, the 18-layer gets down to 2.5% sample error. It is
able to solve the task (to a degree), it could just not learn it from the previous dataset. One could argue,
though, that it would eventually learn the task just from the 5-operations examples but it would
probably take 10 to 100 times more computation / CO2-emissions / floodings / draughts / you-name-it.</p>
<div style="overflow: scroll;"><table>
<thead>
<tr>
<th align="right">nitem</th>
<th align="right">len</th>
<th align="left">nops</th>
<th align="right">val-nops</th>
<th align="right">l</th>
<th align="right">ch</th>
<th align="right">ks</th>
<th align="right">validation loss</th>
<th align="right">validation_mask_error%</th>
<th align="right">validation_sample_error%</th>
<th align="left">model params</th>
<th align="right">train time (minutes)</th>
<th align="left">throughput</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">26</td>
<td align="right">5</td>
<td align="left">2,5</td>
<td align="right">5</td>
<td align="right">6</td>
<td align="right">64</td>
<td align="right">9</td>
<td align="right">0.208661</td>
<td align="right">60.0816</td>
<td align="right">99.5123</td>
<td align="left">237,952</td>
<td align="right">16.55</td>
<td align="left">4,027/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="right">5</td>
<td align="left">2,5</td>
<td align="right">5</td>
<td align="right">12</td>
<td align="right">64</td>
<td align="right">9</td>
<td align="right">0.0322987</td>
<td align="right">6.74761</td>
<td align="right">27.3885</td>
<td align="left">459,520</td>
<td align="right">27.59</td>
<td align="left">2,416/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="right">5</td>
<td align="left">2,5</td>
<td align="right">5</td>
<td align="right">18</td>
<td align="right">64</td>
<td align="right">9</td>
<td align="right">0.00329394</td>
<td align="right">0.565287</td>
<td align="right">2.5577</td>
<td align="left">681,088</td>
<td align="right">41.25</td>
<td align="left">1,615/s</td>
</tr>
</tbody></table></div><p>We can see from the table, that the 12 and especially the 6-layer networks are struggling.
Looking at the plots of the 6-layer networks trained with 5 and 2-5 operations, we can
see that the mask error decreases by a good amount but actual sample error stays roughly
the same. It learned to put some letters at the right place but still fails
for almost every validation sample:</p>
<p><div style="overflow: scroll;"><img src="../../logs/img/selcopy2/selcopy2_error-curves_l6-nops-2-5-vnops5.png" alt="error curves" /></div></p>
<ul>
<li>Quick takeaway: <strong>Put also some easy examples in the training set!</strong></li>
</ul>
<p>The curves suggest, however, that training has not yet converged.
There are sure a few more per-mille to squeeze out.</p>
<p>This is a good point of origin for further experimentation.
Can we get the 6-layer network to solve the <em>5-operations Very Selective Copying</em> problem,</p>
<ul>
<li>without adding so many modules that it actually resembles a 12-layer network</li>
<li>without making it slower to execute than the 12-layer network</li>
<li><em><strong>bonus</strong></em>: by keeping the number of model parameters equal or even lower</li>
</ul>
<p>In other words, is there a trick, maybe to pass data around in a different way,
that strongly increases the computational performance?</p>
<h3 id="quick-comparison-with-mamba-and-lstm">Quick comparison with Mamba and LSTM <a href="#quick-comparison-with-mamba-and-lstm" class="heading-linker">←</a></h3>
<p>Just for another set of baselines, i tried the <a href="https://github.com/state-spaces/mamba" target="_blank">state-spaces/Mamba</a>
(yet the <a href="https://github.com/johnma2006/mamba-minimal" target="_blank">slow version</a> because the fast doesn't run on my system)
and the all-beloved LSTM (the <a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html" target="_blank">pytorch implementation</a>). </p>
<p><div style="overflow: scroll;"><img src="../../logs/img/selcopy2/selcopy2_error-curves_l6-nops-2-5-mamba-and-lstm.png" alt="error curves" /></div></p>
<p>The yellow/brownish is the 6-layers model from above, green is Mamba and blue the LSTM.
All of them have 6 layers</p>
<div style="overflow: scroll;"><table>
<thead>
<tr>
<th align="right">nitem</th>
<th align="right">len</th>
<th align="left">nops</th>
<th align="right">val-nops</th>
<th align="right">l</th>
<th align="left">model</th>
<th align="right">validation loss</th>
<th align="right">validation_mask_error%</th>
<th align="right">validation_sample_error%</th>
<th align="right">model params</th>
<th align="right">train time (minutes)</th>
<th align="right">throughput</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">26</td>
<td align="right">5</td>
<td align="left">2,5</td>
<td align="right">5</td>
<td align="right">6</td>
<td align="left">LSTM hidden_size=64</td>
<td align="right">0.264829</td>
<td align="right">79.9602</td>
<td align="right">100</td>
<td align="right">216,064</td>
<td align="right">6.72</td>
<td align="right">9,926/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="right">5</td>
<td align="left">2,5</td>
<td align="right">5</td>
<td align="right">6</td>
<td align="left">Conv1d channels=64 (from above)</td>
<td align="right">0.208661</td>
<td align="right">60.0816</td>
<td align="right">99.5123</td>
<td align="right">237,952</td>
<td align="right">16.55</td>
<td align="right">4,027/s</td>
</tr>
<tr>
<td align="right">26</td>
<td align="right">5</td>
<td align="left">2,5</td>
<td align="right">5</td>
<td align="right">6</td>
<td align="left">MAMBA d_model=32 d_state=16</td>
<td align="right">0.199433</td>
<td align="right">55.8081</td>
<td align="right">99.1839</td>
<td align="right">67,936</td>
<td align="right">126.56</td>
<td align="right">526/s</td>
</tr>
</tbody></table></div><ul>
<li>None of these especially-crafted models reached a significant performance gain. </li>
<li>To be fair: The Mamba model is <em>very</em> small compared to the Conv1d, just 28% of parameters.
Though it manages to consume 7.8x more computation time. The first tiny drop in sample error
occurred after 1 hour of training and i do not have the patience today to train an
equally-sized Mamba for comparison. (The dynamics of the curves suggest, that it would not change much)</li>
<li>The LSTM basically archived nothing (though equal-sized). It's about as bad as the 6-layer
Conv1d with only 5-operations questions in the training set. At least, it's blazinlgy fast!</li>
<li>Disclaimer: I do not know any good practices about using or training LSTMs or Mambas and
surely made some grave mistake..</li>
</ul>
<p>Back to the residual CNN. To find some interesting method, quick,
i switched to 3 operations and 3-layer networks. The above condition holds as well. The 3-layer
network is not able to solve the 3-operations task in the validation set, but it only takes about
2 minutes to find out:</p>
<p><div style="overflow: scroll;"><img src="../../logs/img/selcopy2/selcopy2_error-curves_l3-vnops3.png" alt="error curves" /></div></p>
<p>The validation sample error stays roughly at 100%.</p>
<p>I tried a couple of things, like adding cross-residual connections, e.g. from layer 1 to layer 3,
which did not noticeably improve performance. Also tried <em>learnable</em> weights, so that each layer
can use as input a weighted mix of all previous layers. Was not much help either, also not for the
6-layer networks.</p>
<h3 id="attention-please">Attention, please! <a href="#attention-please" class="heading-linker">←</a></h3>
<p><strong>UPDATE</strong>: After revision of the used experiment files, i just painfully realized that
some of the below findings are wrong.
In short, i forgot to put the <strong>QKV</strong> and the <strong>attention activation</strong> variables to the network
constructor and as such, the tables comparing these values show something completely different.
There is obviously so much deviation between similar runs, that the results fooled me to make
conclusions about best parameters, although there was nothing to compare...</p>
<p>I'm leaving the manuscript below as it is, as a reminder how stupid the
search for explanations was, without a scientific basis.
However, the corrections are linked at the appropriate places.</p>
<hr />
<p>Eventually i remembered that, in an age-old lecture by Geoff Hinton, he argued that for this
particular case he showed, the network can not succeed without a cell-wise multiplication somewhere.
Then there is this relatively recent Schlag, Irie &amp; Schmidhuber paper
(<a href="https://arxiv.org/abs/2102.11174" target="_blank">2102.11174</a>)
&quot;Linear Transformers Are Secretly Fast Weight Programmers&quot;, which states:
<em>Fast Weight Programmers (FWPs, 1991) program their fast weight memories through sequences of
outer products between self-invented key and value patterns.</em> I just like the <em>self-invented</em> notion.</p>
<p>To make it short, i plugged a <code>torch.nn.MultiheadAttention</code> module into the last layer, such that
the 1d-convolution, which normally outputs 64 channels, now outputs 128 channels, which are split
into 64 <em>query</em> and 64 <em>key</em> channels and the attention-<em>values</em> are the input of the layer.
The pseudo-code looks like:</p>
<div style="overflow: scroll;"><pre><code class="language-python">qk = Conv1d(64, 128)(inp)
q, k = split(qk, 64)
outp = attention(q=q, k=k, v=inp)
</code></pre>
</div><p>Not shown is, that q, k and v (which are each of shape <code>[B, C, L]</code>) are transposed before the
attention to move the channels (<code>C</code>) to the last dimension (<code>[B, L, C]</code>), and make it a
channel-wise attention.</p>
<p>The pytorch <code>MultiheadAttention</code> has a predefined fixed length, because it adds bias and
a couple of other learnable parameters, so it has to be used channel-wise, since we don't
want to fix the sequence length that the network processes.</p>
<p>But wait, the <em>Fast Weight Programmers</em>-paper, said <em>self-invented <strong>keys and values</strong></em> not
self-invented keys and queries. So, for completeness, let's test all the combinations</p>
<h4 id="emself-inventedem-keys-and-values"><em>self-invented</em> keys and values <a href="#emself-inventedem-keys-and-values" class="heading-linker">←</a></h4>
<div style="overflow: scroll;"><pre><code class="language-python">kv = Conv1d(64, 128)(inp)
k, v = split(kv, 64)
outp = attention(q=inp, k=k, v=v)
</code></pre>
</div><h4 id="emself-inventedem-queries-and-values"><em>self-invented</em> queries and values <a href="#emself-inventedem-queries-and-values" class="heading-linker">←</a></h4>
<div style="overflow: scroll;"><pre><code class="language-python">qv = Conv1d(64, 128)(inp)
q, v = split(qv, 64)
outp = attention(q=q, k=inp, v=v)
</code></pre>
</div><h4 id="emself-inventedem-queries-keys-and-values"><em>self-invented</em> queries, keys and values <a href="#emself-inventedem-queries-keys-and-values" class="heading-linker">←</a></h4>
<div style="overflow: scroll;"><pre><code class="language-python">qkv = Conv1d(64, 192)(inp)
q, k, v = split(qkv, 64)
outp = attention(q=q, k=k, v=v)
</code></pre>
</div><p>Unfortunately, the validation sample error varies quite a lot for each run of the same network:</p>
<p><div style="overflow: scroll;"><img src="../../logs/img/selcopy2/selcopy2_error-curves_l3-vnops3-5trials.png" alt="different error curves in 5 trials of the same network" /></div></p>
<p>So i'm doing 5 runs each and report the average. That's 80 runs for this messy plot and the table below!</p>
<p><div style="overflow: scroll;"><img src="../../logs/img/selcopy2/selcopy2_error-curves_l3-vnops3-attention-invention.png" alt="many different error curves" /></div></p>
<p>Each run has the <strong>same validation set</strong> but different training sets and initial network weights.
(reproduce with <a href="https://github.com/defgsus/nn-experiments/blob/613064ad85c1baf0032febe159db800537728440/experiments/textmask/qa-program/convtext-qa-program-3ops-attn-invent.yml" target="_blank">convtext-qa-program-3ops-attn-invent.yml @ 613064ad</a>)</p>
<p><strong>UPDATE: Please check the <a href="../../html/logs/selcopy2-corrections.html#compare-attention-invention">correction</a>!</strong></p>
<div style="overflow: scroll;"><table>
<thead>
<tr>
<th align="left">qkv</th>
<th align="left">attn</th>
<th align="right">validation loss</th>
<th align="right">validation_mask_error%</th>
<th align="right">validation_sample_error%</th>
<th align="right">validation_sample_error% (std)</th>
<th align="right">model params</th>
<th align="right">train time (minutes)</th>
<th align="right">throughput</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"></td>
<td align="left">0,0,0</td>
<td align="right">0.316488</td>
<td align="right">64.9682</td>
<td align="right">99.5322</td>
<td align="right"></td>
<td align="right">176,320</td>
<td align="right">2.24</td>
<td align="right">7,434/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,1</td>
<td align="right">0.323012</td>
<td align="right">63.3965</td>
<td align="right">99.4765</td>
<td align="right">0.8504</td>
<td align="right">246,272</td>
<td align="right">1.77</td>
<td align="right">9,437/s</td>
</tr>
<tr>
<td align="left">QKV</td>
<td align="left">0,0,1</td>
<td align="right">0.287839</td>
<td align="right">56.5426</td>
<td align="right">93.1568</td>
<td align="right">14.4636</td>
<td align="right">246,272</td>
<td align="right">1.86</td>
<td align="right">8,950/s</td>
</tr>
<tr>
<td align="left">KV</td>
<td align="left">0,0,1</td>
<td align="right">0.219660</td>
<td align="right">41.3674</td>
<td align="right">86.0450</td>
<td align="right">16.6931</td>
<td align="right">246,272</td>
<td align="right">1.88</td>
<td align="right">8,884/s</td>
</tr>
<tr>
<td align="left">QV</td>
<td align="left">0,0,1</td>
<td align="right">0.197362</td>
<td align="right">38.2166</td>
<td align="right">75.5036</td>
<td align="right">29.8328</td>
<td align="right">246,272</td>
<td align="right">2.1</td>
<td align="right">7,963/s</td>
</tr>
<tr>
<td align="left">QV</td>
<td align="left">0,0,4</td>
<td align="right">0.151719</td>
<td align="right">29.0418</td>
<td align="right">61.0768</td>
<td align="right">47.2315</td>
<td align="right">246,272</td>
<td align="right">1.93</td>
<td align="right">8,661/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,4</td>
<td align="right">0.048829</td>
<td align="right">8.2794</td>
<td align="right">32.6791</td>
<td align="right">32.2507</td>
<td align="right">246,272</td>
<td align="right">1.93</td>
<td align="right">8,653/s</td>
</tr>
<tr>
<td align="left">QKV</td>
<td align="left">0,0,4</td>
<td align="right">0.051348</td>
<td align="right">8.4331</td>
<td align="right">31.2838</td>
<td align="right">42.0001</td>
<td align="right">246,272</td>
<td align="right">1.88</td>
<td align="right">8,863/s</td>
</tr>
<tr>
<td align="left">KV</td>
<td align="left">0,0,4</td>
<td align="right">0.048607</td>
<td align="right">8.0497</td>
<td align="right">30.4976</td>
<td align="right">36.3580</td>
<td align="right">246,272</td>
<td align="right">1.87</td>
<td align="right">8,905/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,8</td>
<td align="right">0.050850</td>
<td align="right">8.9243</td>
<td align="right">25.5235</td>
<td align="right">40.7858</td>
<td align="right">246,272</td>
<td align="right">1.9</td>
<td align="right">8,771/s</td>
</tr>
<tr>
<td align="left">QV</td>
<td align="left">0,0,8</td>
<td align="right">0.034589</td>
<td align="right">6.0991</td>
<td align="right">18.8754</td>
<td align="right">38.8027</td>
<td align="right">246,272</td>
<td align="right">1.96</td>
<td align="right">8,490/s</td>
</tr>
<tr>
<td align="left">QKV</td>
<td align="left">0,0,8</td>
<td align="right">0.024016</td>
<td align="right">3.6755</td>
<td align="right">15.4618</td>
<td align="right">27.0721</td>
<td align="right">246,272</td>
<td align="right">1.92</td>
<td align="right">8,698/s</td>
</tr>
<tr>
<td align="left">KV</td>
<td align="left">0,0,8</td>
<td align="right">0.002678</td>
<td align="right">0.2691</td>
<td align="right">1.3395</td>
<td align="right">1.0593</td>
<td align="right">246,272</td>
<td align="right">2.03</td>
<td align="right">8,228/s</td>
</tr>
<tr>
<td align="left">QV</td>
<td align="left">0,0,True</td>
<td align="right">0.002325</td>
<td align="right">0.2683</td>
<td align="right">1.2997</td>
<td align="right">0.5423</td>
<td align="right">229,632</td>
<td align="right">1.77</td>
<td align="right">9,449/s</td>
</tr>
<tr>
<td align="left">KV</td>
<td align="left">0,0,True</td>
<td align="right">0.002089</td>
<td align="right">0.2316</td>
<td align="right">1.1504</td>
<td align="right">0.4276</td>
<td align="right">229,632</td>
<td align="right">1.67</td>
<td align="right">9,973/s</td>
</tr>
<tr>
<td align="left">QKV</td>
<td align="left">0,0,True</td>
<td align="right">0.001974</td>
<td align="right">0.2157</td>
<td align="right">1.0648</td>
<td align="right">0.3161</td>
<td align="right">229,632</td>
<td align="right">1.84</td>
<td align="right">9,104/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,True</td>
<td align="right">0.001686</td>
<td align="right">0.1755</td>
<td align="right">0.8638</td>
<td align="right">0.4294</td>
<td align="right">229,632</td>
<td align="right">1.62</td>
<td align="right">10,301/s</td>
</tr>
</tbody></table></div><ul>
<li>
<p><code>qkv</code> is one of the <em>self-invented</em> methods above. e.g. <code>QK</code> means, <code>Q</code>uery and <code>K</code>ey come from convolution, <code>V</code>alue is unprocessed.</p>
</li>
<li>
<p><code>attn: 0,0,0</code> is no attention</p>
</li>
<li>
<p><code>attn: 0,0,X</code> is multi-head attention in final layer with <code>X</code> heads</p>
</li>
<li>
<p><code>attn: 0,0,True</code> is a self-built attention, loosely following <strong>equation 6</strong> in
<em>Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention</em> (<a href="https://arxiv.org/abs/2006.16236" target="_blank">2006.16236</a>]).
In contrast to the torch <code>MultiheadAttention</code>, The self-made function does not have any
parameters and works on sequences and channels of any length.</p>
<div style="overflow: scroll;"><pre><code>def attn(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -&gt; torch.Tensor:
    q = F.elu(q) + 1.
    k = F.elu(k) + 1.
    return q @ (k.permute(0, 2, 1) @ v) / (v.shape[-1] * v.shape[-2])
</code></pre>
</div></li>
<li>
<p><code>(std)</code> is the standard variation given for the validation sample error</p>
</li>
</ul>
<p>So,</p>
<ul>
<li>the <strong>average validation sample error</strong> is smallest for the <strong>self-built, self-invented QK</strong> attention,
followed by all the other self-built variants. </li>
<li>
Best number-of-heads for pytorch attention seems to be 8, <ul>
<li>and the only performing <em>self-invented</em> combination seems to be <strong>K</strong>ey/<strong>V</strong>alue,
as stated in the FWP paper..</li>
</ul>
</li>
<li>The absolutely-prooven-to-not-working model is pytorch attention with QK and heads=1.</li>
<li>Generally, the pytorch attention module is very unstable in this experiment.
Look at the error deviations! Occasionally, one of 5 runs produced a far better
validation sample error than 0.8%. </li>
</ul>
<p>Admittedly, there is something about this so-called <em>Attention</em> that works pretty well.
Thinking in terms of the <em>receptive field</em>, these two dot-products in the attention formula
bring every point in the sequence in contact with every other point, in a way that makes some
sense. How exactly it makes sense, i can not tell. That's why i run these tests.</p>
<h3 id="compare-attention-activation-function">Compare attention activation function <a href="#compare-attention-activation-function" class="heading-linker">←</a></h3>
<p>The above activation function in the self-built attention module, <code>elu(x) + 1</code>, is suggested
by the authors (<a href="https://arxiv.org/abs/2006.16236" target="_blank">2006.16236</a>), mainly because it needs
to be &gt;= 0 and the elu gives a smooth gradient (unlike relu). </p>
<p>The <em>Fast Weight Programmers</em>-paper (<a href="https://arxiv.org/abs/2102.11174" target="_blank">2102.11174</a>) suggests to
replace this function with a more sparsity &amp; orthogonality - aimed
<em>Deterministic Parameter-Free Projection</em> (DPFP). Comparing the two activations
for the 3-operators, 3-layers, 1-million-steps setup, averaged over 5 runs each, gives:</p>
<p><strong>UPDATE: Check the <a href="../../html/logs/selcopy2-corrections.html#compare-attention-activation-function">correction</a>!</strong></p>
<div style="overflow: scroll;"><table>
<thead>
<tr>
<th align="left">qkv</th>
<th align="left">attn</th>
<th align="left">attnact</th>
<th align="right">validation loss</th>
<th align="right">validation_mask_error%</th>
<th align="right">validation_sample_error%</th>
<th align="right">validation_sample_error% (std)</th>
<th align="right">model params</th>
<th align="right">train time (minutes)</th>
<th align="right">throughput</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">QK</td>
<td align="left">0,0,True</td>
<td align="left">dpfp</td>
<td align="right">0.00149588</td>
<td align="right">0.164013</td>
<td align="right">0.79816</td>
<td align="right">0.538314</td>
<td align="right">229,632</td>
<td align="right">1.56</td>
<td align="right">10,723/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,True</td>
<td align="left">elu+1</td>
<td align="right">0.00110781</td>
<td align="right">0.091162</td>
<td align="right">0.44785</td>
<td align="right">0.211001</td>
<td align="right">229,632</td>
<td align="right">1.52</td>
<td align="right">10,981/s</td>
</tr>
</tbody></table></div><p>Well, there is certainly a more powerful activation to be found for this particular task..</p>
<h3 id="compare-position-of-self-attention-in-3-layers-network">Compare position of self-attention in 3-layers network <a href="#compare-position-of-self-attention-in-3-layers-network" class="heading-linker">←</a></h3>
<p>Placing the best (most consistent) attention method from above into <em>all the layers</em>:</p>
<div style="overflow: scroll;"><table>
<thead>
<tr>
<th align="left">qkv</th>
<th align="left">attn</th>
<th align="right">validation loss</th>
<th align="right">validation_mask_error%</th>
<th align="right">validation_sample_error%</th>
<th align="right">validation_sample_error% (std)</th>
<th align="right">model params</th>
<th align="right">train time (minutes)</th>
<th align="right">throughput</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">QK</td>
<td align="left">T,0,0</td>
<td align="right">0.292424</td>
<td align="right">54.8229</td>
<td align="right">99.1043</td>
<td align="right">0.0962</td>
<td align="right">229,632</td>
<td align="right">2.88</td>
<td align="right">5,788/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,T,0</td>
<td align="right">0.196146</td>
<td align="right">39.6819</td>
<td align="right">95.0159</td>
<td align="right">0.6247</td>
<td align="right">282,944</td>
<td align="right">2.93</td>
<td align="right">5,681/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,T,T</td>
<td align="right">0.088230</td>
<td align="right">17.0322</td>
<td align="right">57.3487</td>
<td align="right">29.2807</td>
<td align="right">336,256</td>
<td align="right">2.02</td>
<td align="right">8,247/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,T,0</td>
<td align="right">0.053625</td>
<td align="right">8.9275</td>
<td align="right">38.6146</td>
<td align="right">8.3909</td>
<td align="right">229,632</td>
<td align="right">2.77</td>
<td align="right">6,011/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,T</td>
<td align="right">0.001912</td>
<td align="right">0.2161</td>
<td align="right">1.0648</td>
<td align="right">0.5693</td>
<td align="right">229,632</td>
<td align="right">1.54</td>
<td align="right">10,827/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,T,T</td>
<td align="right">0.001125</td>
<td align="right">0.1361</td>
<td align="right">0.6926</td>
<td align="right">1.2740</td>
<td align="right">282,944</td>
<td align="right">1.80</td>
<td align="right">9,275/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,0,T</td>
<td align="right">0.000185</td>
<td align="right">0.0079</td>
<td align="right">0.0437</td>
<td align="right">0.0341</td>
<td align="right">282,944</td>
<td align="right">1.71</td>
<td align="right">9,744/s</td>
</tr>
</tbody></table></div><p>Well, how can we explain that?
It's very unfortunate that there is no obvious rule popping out, where to put the attention layers,
except that there should be one in the final layer.</p>
<p>Or do you instantly know how to setup a 5-layer network for the 5-operations task, now? </p>
<h3 id="compare-position-of-self-attention-in-5-layers-network">Compare position of self-attention in 5-layers network <a href="#compare-position-of-self-attention-in-5-layers-network" class="heading-linker">←</a></h3>
<p>For efficiency, keep the above dataset (and the 1 million steps of training),
and use a 5-layer network with <code>dilation=[2, 3, 4, 5, 1]</code> to try all the attention-permutations.</p>
<div style="overflow: scroll;"><table>
<thead>
<tr>
<th align="left">qkv</th>
<th align="left">attn</th>
<th align="right">validation loss</th>
<th align="right">validation_mask_error%</th>
<th align="right">validation_sample_error%</th>
<th align="right">model params</th>
<th align="right">train time (minutes)</th>
<th align="right">throughput</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">QK</td>
<td align="left">0,0,0,0,0</td>
<td align="right">0.226301</td>
<td align="right">50.4797</td>
<td align="right">98.7162</td>
<td align="right">282,944</td>
<td align="right">2.92</td>
<td align="right">5,705/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,T,T,T,T</td>
<td align="right">0.168785</td>
<td align="right">32.6393</td>
<td align="right">91.9686</td>
<td align="right">549,504</td>
<td align="right">2.88</td>
<td align="right">5,783/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,0,0,0,0</td>
<td align="right">0.160457</td>
<td align="right">29.9224</td>
<td align="right">87.0521</td>
<td align="right">336,256</td>
<td align="right">3.16</td>
<td align="right">5,276/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,T,T,T,0</td>
<td align="right">0.143025</td>
<td align="right">25.5971</td>
<td align="right">83.1111</td>
<td align="right">496,192</td>
<td align="right">3.67</td>
<td align="right">4,545/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,T,T,0,0</td>
<td align="right">0.131017</td>
<td align="right">23.9471</td>
<td align="right">80.4638</td>
<td align="right">442,880</td>
<td align="right">3.47</td>
<td align="right">4,804/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,T,0,0,0</td>
<td align="right">0.119354</td>
<td align="right">22.0004</td>
<td align="right">75.7564</td>
<td align="right">389,568</td>
<td align="right">3.3</td>
<td align="right">5,053/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,0,0,T,0</td>
<td align="right">0.103017</td>
<td align="right">18.4912</td>
<td align="right">69.4068</td>
<td align="right">389,568</td>
<td align="right">3.38</td>
<td align="right">4,927/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,T,0,T,0</td>
<td align="right">0.0981063</td>
<td align="right">17.9598</td>
<td align="right">66.9984</td>
<td align="right">442,880</td>
<td align="right">3.49</td>
<td align="right">4,769/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,0,T,0,0</td>
<td align="right">0.0658464</td>
<td align="right">11.9805</td>
<td align="right">50.3881</td>
<td align="right">389,568</td>
<td align="right">3.36</td>
<td align="right">4,956/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,0,T,T,T</td>
<td align="right">0.0689705</td>
<td align="right">12.0979</td>
<td align="right">49.4128</td>
<td align="right">496,192</td>
<td align="right">2.63</td>
<td align="right">6,327/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,T,T,0,T</td>
<td align="right">0.0544455</td>
<td align="right">9.24164</td>
<td align="right">40.2170</td>
<td align="right">496,192</td>
<td align="right">2.63</td>
<td align="right">6,334/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,T,0,0,0</td>
<td align="right">0.0165344</td>
<td align="right">2.36266</td>
<td align="right">11.2162</td>
<td align="right">336,256</td>
<td align="right">3.17</td>
<td align="right">5,251/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,0,T,T,0</td>
<td align="right">0.0127326</td>
<td align="right">1.93471</td>
<td align="right">9.40486</td>
<td align="right">442,880</td>
<td align="right">3.57</td>
<td align="right">4,674/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,T,T,T,T</td>
<td align="right">0.00333168</td>
<td align="right">0.427946</td>
<td align="right">2.15963</td>
<td align="right">496,192</td>
<td align="right">2.56</td>
<td align="right">6,512/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,T,0,T,0</td>
<td align="right">0.00353792</td>
<td align="right">0.376194</td>
<td align="right">1.88097</td>
<td align="right">389,568</td>
<td align="right">3.36</td>
<td align="right">4,957/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,T,T,0,0</td>
<td align="right">0.00345656</td>
<td align="right">0.350318</td>
<td align="right">1.75159</td>
<td align="right">389,568</td>
<td align="right">3.42</td>
<td align="right">4,866/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,T,0,T,T</td>
<td align="right">0.00226949</td>
<td align="right">0.248806</td>
<td align="right">1.29379</td>
<td align="right">496,192</td>
<td align="right">2.59</td>
<td align="right">6,447/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,T,0,0</td>
<td align="right">0.00188953</td>
<td align="right">0.181131</td>
<td align="right">0.915605</td>
<td align="right">336,256</td>
<td align="right">3.24</td>
<td align="right">5,138/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,T,T,T,0</td>
<td align="right">0.00184384</td>
<td align="right">0.173169</td>
<td align="right">0.885748</td>
<td align="right">442,880</td>
<td align="right">3.53</td>
<td align="right">4,723/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,T,T,0</td>
<td align="right">0.00132653</td>
<td align="right">0.095541</td>
<td align="right">0.477707</td>
<td align="right">389,568</td>
<td align="right">3.23</td>
<td align="right">5,153/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,0,0,T,T</td>
<td align="right">0.000828459</td>
<td align="right">0.083598</td>
<td align="right">0.417994</td>
<td align="right">442,880</td>
<td align="right">2.43</td>
<td align="right">6,846/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,T,0,T,T</td>
<td align="right">0.000377713</td>
<td align="right">0.035828</td>
<td align="right">0.189092</td>
<td align="right">442,880</td>
<td align="right">2.46</td>
<td align="right">6,767/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,0,T,0</td>
<td align="right">0.000290391</td>
<td align="right">0.0199045</td>
<td align="right">0.0995223</td>
<td align="right">336,256</td>
<td align="right">3.11</td>
<td align="right">5,354/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,T,T,T</td>
<td align="right">0.000282769</td>
<td align="right">0.0199045</td>
<td align="right">0.0995223</td>
<td align="right">442,880</td>
<td align="right">2.51</td>
<td align="right">6,644/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,T,0,0,T</td>
<td align="right">0.000122952</td>
<td align="right">0.0159236</td>
<td align="right">0.0796178</td>
<td align="right">442,880</td>
<td align="right">2.41</td>
<td align="right">6,913/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,T,0,T</td>
<td align="right">0.000144108</td>
<td align="right">0.0119427</td>
<td align="right">0.0696656</td>
<td align="right">389,568</td>
<td align="right">2.31</td>
<td align="right">7,203/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,0,T,0,T</td>
<td align="right">0.000167425</td>
<td align="right">0.00796178</td>
<td align="right">0.0398089</td>
<td align="right">442,880</td>
<td align="right">2.5</td>
<td align="right">6,664/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,0,0,T</td>
<td align="right">6.164e-05</td>
<td align="right">0.00398089</td>
<td align="right">0.0199045</td>
<td align="right">336,256</td>
<td align="right">2.04</td>
<td align="right">8,179/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,T,0,0,T</td>
<td align="right">6.42146e-05</td>
<td align="right">0.00398089</td>
<td align="right">0.0199045</td>
<td align="right">389,568</td>
<td align="right">2.26</td>
<td align="right">7,376/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,T,T,0,T</td>
<td align="right">0.00010805</td>
<td align="right">0.00398089</td>
<td align="right">0.0199045</td>
<td align="right">442,880</td>
<td align="right">2.5</td>
<td align="right">6,675/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,0,0,0,T</td>
<td align="right">0.000154807</td>
<td align="right">0.00398089</td>
<td align="right">0.0199045</td>
<td align="right">389,568</td>
<td align="right">2.33</td>
<td align="right">7,141/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,0,T,T</td>
<td align="right">2.67664e-05</td>
<td align="right">0</td>
<td align="right">0.00995223</td>
<td align="right">389,568</td>
<td align="right">2.24</td>
<td align="right">7,453/s</td>
</tr>
</tbody></table></div><p>Okay okay, there is some slight pattern for arrangement of the self-attention modules emerging.</p>
<p>Now pick the best ones and train and test against 5-operations questions (at 4 million training steps):</p>
<div style="overflow: scroll;"><table>
<thead>
<tr>
<th align="left">qkv</th>
<th align="left">attn</th>
<th align="right">validation loss</th>
<th align="right">validation_mask_error%</th>
<th align="right">validation_sample_error%</th>
<th align="right">model params</th>
<th align="right">train time (minutes)</th>
<th align="right">throughput</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">QK</td>
<td align="left">T,0,T,0,T</td>
<td align="right">0.107469</td>
<td align="right">29.7174</td>
<td align="right">87.2711</td>
<td align="right">442,880</td>
<td align="right">10.17</td>
<td align="right">6,553/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,T,T,0,T</td>
<td align="right">0.0795734</td>
<td align="right">21.7874</td>
<td align="right">74.3432</td>
<td align="right">442,880</td>
<td align="right">10.03</td>
<td align="right">6,648/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">T,0,0,0,T</td>
<td align="right">0.0692764</td>
<td align="right">18.6744</td>
<td align="right">68.2524</td>
<td align="right">389,568</td>
<td align="right">9.57</td>
<td align="right">6,965/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,T,0,0,T</td>
<td align="right">0.0635093</td>
<td align="right">16.5983</td>
<td align="right">61.6939</td>
<td align="right">389,568</td>
<td align="right">9.63</td>
<td align="right">6,926/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,T,0,T</td>
<td align="right">0.0525193</td>
<td align="right">14.9960</td>
<td align="right">58.0812</td>
<td align="right">389,568</td>
<td align="right">9.37</td>
<td align="right">7,117/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,0,0,T</td>
<td align="right">0.0072723</td>
<td align="right">1.72572</td>
<td align="right">8.1110</td>
<td align="right">336,256</td>
<td align="right">8.09</td>
<td align="right">8,245/s</td>
</tr>
</tbody></table></div><p>It's still not completely sufficient. Let's switch back to 6-layer networks, again copying the
best attention-settings, although, there is only <em>one</em> performing okay-ish,
the others are worse than 50% error. </p>
<h3 id="6-layers">6-layers <a href="#6-layers" class="heading-linker">←</a></h3>
<div style="overflow: scroll;"><table>
<thead>
<tr>
<th align="left">qkv</th>
<th align="left">attn</th>
<th align="right">validation loss</th>
<th align="right">validation_mask_error%</th>
<th align="right">validation_sample_error%</th>
<th align="right">model params</th>
<th align="right">train time (minutes)</th>
<th align="right">throughput</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">QK</td>
<td align="left">T,0,0,0,0,T</td>
<td align="right">0.0529652</td>
<td align="right">14.0844</td>
<td align="right">54.9562</td>
<td align="right">442,880</td>
<td align="right">10.39</td>
<td align="right">6,419/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,T,0,0,0,T</td>
<td align="right">0.0196075</td>
<td align="right">4.79498</td>
<td align="right">21.666</td>
<td align="right">442,880</td>
<td align="right">14.6</td>
<td align="right">4,566/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,0,T,0,T</td>
<td align="right">0.0143111</td>
<td align="right">3.58081</td>
<td align="right">16.5705</td>
<td align="right">442,880</td>
<td align="right">14.5</td>
<td align="right">4,599/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,T,0,0,T</td>
<td align="right">0.0136107</td>
<td align="right">3.27826</td>
<td align="right">15.2767</td>
<td align="right">442,880</td>
<td align="right">14.49</td>
<td align="right">4,601/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,0,0,T,T</td>
<td align="right">0.00143896</td>
<td align="right">0.254777</td>
<td align="right">1.27389</td>
<td align="right">442,880</td>
<td align="right">14.59</td>
<td align="right">4,569/s</td>
</tr>
<tr>
<td align="left">QK</td>
<td align="left">0,0,0,0,0,T</td>
<td align="right">6.6917e-05</td>
<td align="right">0.00995223</td>
<td align="right">0.0597134</td>
<td align="right">389,568</td>
<td align="right">13.82</td>
<td align="right">4,823/s</td>
</tr>
</tbody></table></div><p>Well, i'm a bit tired of this topic right now. </p>
<p>Let's wrap the findings up, quickly: </p>
<ul>
<li>a 6-layer network can solve the 5-operations task, if we add self-attention to the last layer.</li>
<li>The number of model parameters increased from 237,952 to 389,568 (because i still used
the 13-kernels from the 3-layers experiments and the one 64-128 convolution for the <em>self-invented</em> QK) </li>
<li>The training time increased from 14 to 17 minutes. </li>
<li>Validation sample error decreased from 99% to 0.06%</li>
<li>Is this a passable building block for a <em>Very Small Language Model</em>?
I might have said yes but right now i say: Don't know, have to test it...</li>
</ul>
<p>A little graphical disorder at the end of the day:</p>
<p><div style="overflow: scroll;"><img src="../../logs/img/selcopy2/selcopy2_error-curves_messy-end-of-day.png" alt="error curves" /></div></p>


        <!-- article footer -->
        <div class="flex article-footer">
            <div>
                 <a target="_blank" href="https://github.com/defgsus/nn-experiments/issues">Leave a comment</a>
            </div>

            <div class="flex-grow"></div>

            <div>
                Edit on <a target="_blank" href="https://github.com/defgsus/nn-experiments/blob/master/docs/logs/2024-12-15-selcopy2.md">github</a>
            </div>
        </div>

        <div class="flex article-footer">
            <div>
                
                    <a href="../../html/logs/selcopy.html">
                        &lt;&lt; Efficiently solving the Selective Copying Problem with a Very Small Language Model
                    </a>
                
            </div>

            <div class="flex-grow"></div>

            <div>
                
                <a href="../../html/posts/2024/first-post.html">
                    First post &gt;&gt;
                </a>
                
            </div>
        </div>
    </div>

</main>


</body>