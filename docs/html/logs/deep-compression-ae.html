<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Deep-Compression Auto-Encoder</title>
    <meta name="description" content="" />
    <link rel="stylesheet" href="../../html/style/style.css">
    
</head>
<body>


<main class="article">
    <div class="article-left">
        <h3><a href="../../index.html">&lt;&lt; nn-experiments</a></h3>
        <ul>
            
            
            <li class="indent-1"><a href="#deep-compression-auto-encoder" title="Deep-Compression Auto-Encoder">Deep-Compression Auto-Encoder</a></li>
            
            
            
            
            
            
            
            
            
            
            
            
        </ul>
    </div>

    <div class="article-mid">

        <div class="show-when-small">
            <a href="../../index.html">&lt;&lt; nn-experiments</a></h3>
        </div>

        <h1 id="deep-compression-auto-encoder">Deep-Compression Auto-Encoder</h1>
<p>Experiments with a small version of DC-AE from the paper
<em>DEEP COMPRESSION AUTOENCODER FOR EFFICIENT HIGH-RESOLUTION DIFFUSION MODELS</em> <a href="https://arxiv.org/abs/2205.14756">arxiv.org/abs/2205.14756</a></p>
<p>Code can be found here <a href="https://github.com/mit-han-lab/efficientvit">https://github.com/mit-han-lab/efficientvit</a>, and i copied the DC-AE parts and ported
the type-hints back to python &lt;3.10 <a href="https://github.com/defgsus/nn-experiments/blob/master/src/models/efficientvit">here</a>.</p>
<p>Using the good ol' 47,579 RPG dataset, for one million 3x32x32 images.</p>
<p><div style="overflow: scroll;"><img src="../../logs/img/dc-ae-rpg47k-1M.png" alt="loss plots" /></div></p>
<p>(Not all runs are finished because they seemed to converge at the same validation error)</p>
<p>The <strong>baseline (gray)</strong> is the usual stacked symmetric CNN AE with 24, 32, 48 channels, kernel-size 5 and a linear
layer for latent code size of 128, which results in a <strong>compression ratio of 24</strong>.</p>
<p>The DC-AE model configurations i tried are:</p>
<h4 id="dcae-03-yellow-params-81m-ratio-96">dcae-03 (yellow) params: 81M, ratio 96</h4>
<pre><code>&quot;latent_channels=32 &quot;
&quot;encoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU] &quot;
&quot;encoder.width_list=[64,128,256,256,512,512] encoder.depth_list=[0,4,8,2,2,2] &quot;
&quot;decoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU] &quot;
&quot;decoder.width_list=[64,128,256,256,512,512] decoder.depth_list=[0,5,10,2,2,2] &quot;
&quot;decoder.norm=[bn2d,bn2d,bn2d,trms2d,trms2d,trms2d] decoder.act=[relu,relu,relu,silu,silu,silu]&quot;
</code></pre>
<h4 id="dcae-04-purple-params-81m-ratio-48">dcae-04 (purple) params: 81M, ratio 48</h4>
<p>lowered compression ratio (via <code>latent_channels</code>)</p>
<pre><code>&quot;latent_channels=64 &quot;
&quot;encoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU] &quot;
&quot;encoder.width_list=[64,128,256,256,512,512] encoder.depth_list=[0,4,8,2,2,2] &quot;
&quot;decoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU] &quot;
&quot;decoder.width_list=[64,128,256,256,512,512] decoder.depth_list=[0,5,10,2,2,2] &quot;
&quot;decoder.norm=[bn2d,bn2d,bn2d,trms2d,trms2d,trms2d] decoder.act=[relu,relu,relu,silu,silu,silu]&quot;
</code></pre>
<h4 id="dcae-05-green-params-110m-ratio-48">dcae-05 (green) params: 110M, ratio: 48</h4>
<p>increased width</p>
<pre><code>&quot;latent_channels=64 &quot;
&quot;encoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU] &quot;
&quot;encoder.width_list=[128,256,256,512,512,512] encoder.depth_list=[0,4,8,2,2,2] &quot;
&quot;decoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU] &quot;
&quot;decoder.width_list=[128,256,256,512,512,512] decoder.depth_list=[0,5,10,2,2,2] &quot;
&quot;decoder.norm=[bn2d,bn2d,bn2d,trms2d,trms2d,trms2d] decoder.act=[relu,relu,relu,silu,silu,silu]&quot;
</code></pre>
<h4 id="dcae-06-orange-params-142m-ratio-48">dcae-06 (orange): params: 142M, ratio: 48</h4>
<p>increased width</p>
<pre><code>&quot;latent_channels=64 &quot;
&quot;encoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU] &quot;
&quot;encoder.width_list=[64,128,256,256,512,1024] encoder.depth_list=[0,4,8,2,2,2] &quot;
&quot;decoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU] &quot;
&quot;decoder.width_list=[64,128,256,256,512,1024] decoder.depth_list=[0,5,10,2,2,2] &quot;
&quot;decoder.norm=[bn2d,bn2d,bn2d,trms2d,trms2d,trms2d] decoder.act=[relu,relu,relu,silu,silu,silu]&quot;
</code></pre>
<h4 id="dcae-07-pink-params-106m-ratio-48">dcae-07 (pink): params: 106M, ratio: 48</h4>
<p>Sames as dcae-04 but increased depth.</p>
<pre><code>&quot;latent_channels=64 &quot;
&quot;encoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU] &quot;
&quot;encoder.width_list=[64,128,256,256,512,512] encoder.depth_list=[0,4,8,2,3,4] &quot;
&quot;decoder.block_type=[ResBlock,ResBlock,ResBlock,EViT_GLU,EViT_GLU,EViT_GLU] &quot;
&quot;decoder.width_list=[64,128,256,256,512,512] decoder.depth_list=[0,5,10,2,3,4] &quot;  
&quot;decoder.norm=[bn2d,bn2d,bn2d,trms2d,trms2d,trms2d] decoder.act=[relu,relu,relu,silu,silu,silu]&quot;
</code></pre>
<p>The original model in the repo has 323M parameters which i could not train on my GPU,
so i lowered the width of the layers to make it fit. Also increased the learning rate from
originally <code>9.375e-7</code> to <code>9.375e-5</code>. Batch size was 64.</p>
<p>The latent shape for 32x32 images is actually <code>[&lt;latent_channels&gt;, 1, 1]</code>. The first try with
compression ratio 96 was a bit too much. With ratio 48 the results are better than the baseline (with 24)
so this is promising. However, none of the changes (dcae-05 to dcae-07) did enhance the performance.</p>
<p>Here's the reconstruction of the validation set for <strong>baseline</strong> and <strong>dcae-05</strong>:</p>
<div style="overflow: scroll;"><table>
<thead>
<tr>
<th>baseline</th>
<th>dcae-05</th>
</tr>
</thead>
<tbody>
<tr>
<td><div style="overflow: scroll;"><img src="../../logs/img/dc-ae-rpg47k-1M-recon-baseline.png" alt="reconstruction plot" /></div></td>
<td><div style="overflow: scroll;"><img src="../../logs/img/dc-ae-rpg47k-1M-recon-05.png" alt="reconstruction plot" /></div></td>
</tr>
</tbody></table></div>

        <!-- article footer -->
        <div class="flex article-footer">
            <div>
                 <a href="https://github.com/defgsus/nn-experiments/issues">Leave a comment</a>
            </div>

            <div class="flex-grow"></div>

            <div>
                Edit on <a href="https://github.com/defgsus/nn-experiments/blob/master/docs/">github</a>
            </div>
        </div>

        <div class="flex article-footer">
            <div>
                
                    <a href="../../html/logs/residual-convolution.html">
                        &lt;&lt; Parameter tuning for a Residual Deep Image-to-Image CNN
                    </a>
                
            </div>

            <div class="flex-grow"></div>

            <div>
                
                <a href="../../html/logs/colorize.html">
                    Comparing different color-spaces in a grayscale-to-color residual CNN $gt;$gt;
                </a>
                
            </div>
        </div>
    </div>

</main>


</body>