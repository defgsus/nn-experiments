<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>papers-of-the-week2</title>
    <meta name="description" content="" />
    <link rel="stylesheet" href="../../../html/style/style.css">
    <script type="text/javascript" src="../../../html/js/main.js"></script>
</head>
<body>


<main class="article">
    <div class="article-left">
        <h3><a href="../../../index.html">&lt;&lt; nn-experiments</a></h3>
        <ul>
            
            
            <li class="indent-2"><a href="#how-large-language-models-llms-extrapolate-from-guided-missiles-to-guided-prompts" title="How Large Language Models (LLMs) Extrapolate: From Guided Missiles to Guided Prompts">How Large Language Models (LLMs) Extrapolate: From Guided Missiles to Guided Prompts</a></li>
            
            
            
            <li class="indent-2"><a href="#on-the-dangers-of-stochastic-parrots-can-language-models-be-too-big" title="On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</a></li>
            
            
            
            <li class="indent-2"><a href="#uncovering-deceptive-tendencies-in-language-models-a-simulated-company-ai-assistant" title="Uncovering Deceptive Tendencies in Language Models: A Simulated Company AI Assistant">Uncovering Deceptive Tendencies in Language Models: A Simulated Company AI Assistant</a></li>
            
            
        </ul>
    </div>

    <div class="article-mid">

        <div class="show-when-small">
            <a href="../../../index.html">&lt;&lt; nn-experiments</a></h3>
        </div>

        <p>Three papers about language models i had fun reading.</p>
<h2 id="how-large-language-models-llms-extrapolate-from-guided-missiles-to-guided-prompts">How Large Language Models (LLMs) Extrapolate: From Guided Missiles to Guided Prompts <a href="#how-large-language-models-llms-extrapolate-from-guided-missiles-to-guided-prompts" class="heading-linker">←</a></h2>
<p><em>Xuenan Cao</em></p>
<p><a href="https://arxiv.org/pdf/2501.10361" target="_blank">arxiv:2501.10361</a></p>
<p>I don't really know what the message of this paper is supposed to be, except that, what
today's next-token-prediction language models do, is extrapolation. So nobody should wonder
when they give information that is essentially wrong. It <em>could</em> have been true in a parallel
universe. </p>
<p>It's more a historical essay about Norbert Wiener, Andrey Kolmogorov and the hot and cold war
and contains many nice figures of speech, like,
<em>A transmission apparatus tosses out a word like a jet ejecting a bomb</em>.</p>
<h2 id="on-the-dangers-of-stochastic-parrots-can-language-models-be-too-big">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? <a href="#on-the-dangers-of-stochastic-parrots-can-language-models-be-too-big" class="heading-linker">←</a></h2>
<p><em>Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, Shmargaret Shmitchell</em></p>
<p><a href="https://dl.acm.org/doi/10.1145/3442188.3445922" target="_blank">DOI:10.1145/3442188.3445922</a></p>
<p>One of the papers, cited in the above <em>Extrapolation</em> essay. At the recent pace in LLM engineering,
a paper from 2021 is already <em>ancient</em>. Nevertheless, it's an interesting read with many valid
critiques that are still not really addressed. E.g., the idea that diversity and unbiased
comprehension arises when feeding in the whole of the internet (excluding texts with <em>bad</em> words). </p>
<h2 id="uncovering-deceptive-tendencies-in-language-models-a-simulated-company-ai-assistant">Uncovering Deceptive Tendencies in Language Models: A Simulated Company AI Assistant <a href="#uncovering-deceptive-tendencies-in-language-models-a-simulated-company-ai-assistant" class="heading-linker">←</a></h2>
<p><em>Olli Järviniemi, Evan Hubinger</em></p>
<p><a href="https://arxiv.org/pdf/2405.0157" target="_blank">arxiv:2405.0157</a></p>
<p>What if, an LLM works for a company and colleagues address it with all sorts of
(maybe not entirely ethical) tasks during the day.
It also has access to company chat and through that, is informed
that an evaluator will soon appear to see if the model abides AI regulations.
Will it talk honestly to the evaluator?</p>
<p>It's an interesting story the authors have created, a bit like a text adventure,
and i'm honestly impressed what amount of text an LLM like Claude 3 Opus can handle.</p>


        <!-- article footer -->
        <div class="flex article-footer">
            <div>
                 <a target="_blank" href="https://github.com/defgsus/nn-experiments/issues">Leave a comment</a>
            </div>

            <div class="flex-grow"></div>

            <div>
                Edit on <a target="_blank" href="https://github.com/defgsus/nn-experiments/blob/master/docs/posts/2025/2025-01-31-papers-of-the-week2.md">github</a>
            </div>
        </div>

        <div class="flex article-footer">
            <div>
                
                    <a href="../../../html/posts/2025/llm-context-length.html">
                        &lt;&lt; Current standards in Language Model Context Length
                    </a>
                
            </div>

            <div class="flex-grow"></div>

            <div>
                
            </div>
        </div>
    </div>

</main>


</body>